{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Background**\n",
        "\n",
        "An autoencoder is a neural network that takes an input, encodes it into a lower-dimensional latent space through \"encoding\" layers, and then attempts to reconstruct the original input using \"decoding\" layers. Autoencoders are often used for dimensionality reduction.\n",
        "\n",
        "# **Task:**\n",
        "\n",
        "Train a 64x16x64 autoencoder on the digits dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "STP4ommpjqPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HVAadAdxq3hU",
        "outputId": "ed48c9fe-3a66-44e4-ebe0-a2cf281fd3f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32d000dc-598e-4a1a-afe4-da37af2716ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-32d000dc-598e-4a1a-afe4-da37af2716ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nn.py to nn.py\n"
          ]
        }
      ],
      "source": [
        "# uploading nn.py\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nn import NeuralNetwork"
      ],
      "metadata": {
        "id": "YB90mUPSpVTt",
        "collapsed": true
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading digits datasets using `sklearn.datasets.load_digits()`"
      ],
      "metadata": {
        "id": "UUlVf7NljpRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits = load_digits()\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target"
      ],
      "metadata": {
        "id": "z9h6nRR1jc0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into training and validation"
      ],
      "metadata": {
        "id": "1picdx4-kPjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "np.random.seed(888)\n",
        "\n",
        "indices = np.arange(n_samples)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_size = int(0.8 * n_samples)\n",
        "X_train, X_test = data[indices[:train_size]], data[indices[train_size:]]\n",
        "y_train, y_test = digits.target[indices[:train_size]], digits.target[indices[train_size:]]"
      ],
      "metadata": {
        "id": "rkvODtFcjlBQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate instance of `NeuralNetwork` class\n"
      ],
      "metadata": {
        "id": "A_puPJN2kSxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_architecture = [\n",
        "    {'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},  # Input -> Hidden layer (64 -> 16)\n",
        "    {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}  # Hidden -> Output layer (16 -> 64)\n",
        "]\n",
        "\n",
        "learning_rate = 1e-3\n",
        "random_seed = 8\n",
        "batch_size = 50\n",
        "epochs = 1000\n",
        "loss_function = 'mean_squared_error'\n",
        "\n",
        "network = NeuralNetwork(nn_architecture, learning_rate, random_seed, batch_size, epochs, loss_function)"
      ],
      "metadata": {
        "id": "otv_CAw3xAYZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing a digit\n",
        "plt.imshow(X_train[0].reshape(8, 8), cmap=\"gray\")\n",
        "plt.show()\n",
        "print(X_train[0].reshape(8, 8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "G6VrfJY2rN7o",
        "outputId": "767efb1c-3594-4bc5-9fbd-41f4e34a2f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  4. 15. 16. 13.  1.  0.  0.]\n",
            " [ 0.  9. 14. 10. 16.  6.  0.  0.]\n",
            " [ 0.  1.  1.  6. 16.  2.  0.  0.]\n",
            " [ 0.  0.  0.  8. 14.  1.  0.  0.]\n",
            " [ 0.  0.  5. 16.  5.  0.  0.  0.]\n",
            " [ 0.  0. 13. 14.  0.  0.  0.  0.]\n",
            " [ 0.  5. 16.  9.  8.  8. 10.  0.]\n",
            " [ 0.  4. 15. 16. 16. 16.  9.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of inputs and labels\n",
        "print(\"Shape of training inputs is:\", X_train.shape)\n",
        "print(\"Shape of training set labels is:\", y_train.shape)\n",
        "\n",
        "print(\"Shape of test inputs is:\", X_test.shape)\n",
        "print(\"Shape of test set labels is:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EV-f4d1rSQD",
        "outputId": "20e83641-ce3d-4f4a-b3f0-ed6911ad7237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training inputs is: (1437, 64)\n",
            "Shape of training set labels is: (1437,)\n",
            "Shape of test inputs is: (360, 64)\n",
            "Shape of test set labels is: (360,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train your autoencoder on the training data"
      ],
      "metadata": {
        "id": "KdqjdSuulAp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "nn_architecture = [\n",
        "    {'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},  # Input -> Hidden layer (64 -> 16)\n",
        "    {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}  # Hidden -> Output layer (16 -> 64)\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store the validation losses\n",
        "validation_loss = []\n",
        "\n",
        "# List of hyperparameters to test\n",
        "lr_test = [1e-2, 1e-3, 1e-4]\n",
        "batch_size_test = [200, 300, 400, 500]\n",
        "hyperparameters = [(x, y) for x in lr_test for y in batch_size_test]\n",
        "loss_function = 'mean_squared_error'\n",
        "epochs = 400\n",
        "\n",
        "# Loop through all hyperparameter combinations\n",
        "for var in hyperparameters:\n",
        "    print('lr, batch_size:', var)  # Corrected print statement to use var\n",
        "\n",
        "    # Create the neural network with the current hyperparameters\n",
        "    network = NeuralNetwork(\n",
        "        nn_architecture,\n",
        "        lr=var[0],\n",
        "        seed=random_seed,\n",
        "        batch_size=var[1],\n",
        "        epochs=epochs,\n",
        "        loss_function=loss_function\n",
        "    )\n",
        "\n",
        "    # Train the network\n",
        "    per_epoch_loss_train, per_epoch_loss_val = network.fit(X_train.T, X_train.T, X_test.T, X_test.T)\n",
        "\n",
        "    # Store the average of the last 10 epochs of validation loss\n",
        "    validation_loss.append(np.mean(per_epoch_loss_val[-10:]))\n",
        "\n",
        "# Print the results of validation losses for each combination of hyperparameters\n",
        "print(\"Validation loss list:\", validation_loss)"
      ],
      "metadata": {
        "id": "PSWgePjNz5jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0424e02-f189-488a-a86c-8e19625a9556",
        "collapsed": true
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr, batch_size: (0.01, 200)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 914.3434050937504\n",
            "Epoch 2  \tTraining Loss: 914.8765254955363\tValidation Loss: 0.9401842090383703\n",
            "Epoch 3  \tTraining Loss: 0.9370181172349683\tValidation Loss: 0.938417920087931\n",
            "Epoch 4  \tTraining Loss: 0.9352944862056255\tValidation Loss: 0.936747075070756\n",
            "Epoch 5  \tTraining Loss: 0.933665246573885\tValidation Loss: 0.9351675675169631\n",
            "Epoch 6  \tTraining Loss: 0.9321261771003404\tValidation Loss: 0.9336743741716047\n",
            "Epoch 7  \tTraining Loss: 0.9306722873049843\tValidation Loss: 0.932262748898413\n",
            "Epoch 8  \tTraining Loss: 0.9292988629063693\tValidation Loss: 0.930928207367703\n",
            "Epoch 9  \tTraining Loss: 0.9280014505353674\tValidation Loss: 0.9296665125910237\n",
            "Epoch 10  \tTraining Loss: 0.9267758432949464\tValidation Loss: 0.9284736612557136\n",
            "Epoch 11  \tTraining Loss: 0.9256180671191448\tValidation Loss: 0.9273458708151057\n",
            "Epoch 12  \tTraining Loss: 0.9245243678870094\tValidation Loss: 0.9262795672925838\n",
            "Epoch 13  \tTraining Loss: 0.9234911992497139\tValidation Loss: 0.9252713737599956\n",
            "Epoch 14  \tTraining Loss: 0.9225152111313887\tValidation Loss: 0.9243180994531209\n",
            "Epoch 15  \tTraining Loss: 0.9215932388663703\tValidation Loss: 0.9234167294889514\n",
            "Epoch 16  \tTraining Loss: 0.9207222929376548\tValidation Loss: 0.9225644151514933\n",
            "Epoch 17  \tTraining Loss: 0.9198995492832761\tValidation Loss: 0.9217584647146432\n",
            "Epoch 18  \tTraining Loss: 0.9191223401391766\tValidation Loss: 0.920996334772427\n",
            "Epoch 19  \tTraining Loss: 0.9183881453888855\tValidation Loss: 0.9202756220485412\n",
            "Epoch 20  \tTraining Loss: 0.917694584391944\tValidation Loss: 0.9195940556586788\n",
            "Epoch 21  \tTraining Loss: 0.9170394082645927\tValidation Loss: 0.918949489800599\n",
            "Epoch 22  \tTraining Loss: 0.9164204925876811\tValidation Loss: 0.9183398968482793\n",
            "Epoch 23  \tTraining Loss: 0.9158358305181611\tValidation Loss: 0.9177633608278012\n",
            "Epoch 24  \tTraining Loss: 0.9152835262821043\tValidation Loss: 0.917218071253747\n",
            "Epoch 25  \tTraining Loss: 0.9147617890290651\tValidation Loss: 0.9167023173061593\n",
            "Epoch 26  \tTraining Loss: 0.9142689270216375\tValidation Loss: 0.9162144823298196\n",
            "Epoch 27  \tTraining Loss: 0.913803342150826\tValidation Loss: 0.9157530386373933\n",
            "Epoch 28  \tTraining Loss: 0.9133635247550738\tValidation Loss: 0.9153165425996779\n",
            "Epoch 29  \tTraining Loss: 0.9129480487251275\tValidation Loss: 0.9149036300071866\n",
            "Epoch 30  \tTraining Loss: 0.9125555668798249\tValidation Loss: 0.9145130116880591\n",
            "Epoch 31  \tTraining Loss: 0.9121848065978083\tValidation Loss: 0.914143469368121\n",
            "Epoch 32  \tTraining Loss: 0.9118345656910037\tValidation Loss: 0.9137933444379073\n",
            "Epoch 33  \tTraining Loss: 0.911503201974437\tValidation Loss: 0.911566019941909\n",
            "Epoch 34  \tTraining Loss: 0.9093003221542041\tValidation Loss: 0.909461300535763\n",
            "Epoch 35  \tTraining Loss: 0.9072193610344994\tValidation Loss: 0.9074724193138101\n",
            "Epoch 36  \tTraining Loss: 0.9052535710026187\tValidation Loss: 0.9055929833596824\n",
            "Epoch 37  \tTraining Loss: 0.9033965778934165\tValidation Loss: 0.9038169530633082\n",
            "Epoch 38  \tTraining Loss: 0.9016423603208193\tValidation Loss: 0.9021386225815272\n",
            "Epoch 39  \tTraining Loss: 0.8999852301532403\tValidation Loss: 0.9005526013797065\n",
            "Epoch 40  \tTraining Loss: 0.8984198140695785\tValidation Loss: 0.8990537967945418\n",
            "Epoch 41  \tTraining Loss: 0.8969410361360065\tValidation Loss: 0.8976373975615347\n",
            "Epoch 42  \tTraining Loss: 0.8955441013470419\tValidation Loss: 0.8962988582537708\n",
            "Epoch 43  \tTraining Loss: 0.8942244800775405\tValidation Loss: 0.8950338845815689\n",
            "Epoch 44  \tTraining Loss: 0.8929778933951952\tValidation Loss: 0.8938384195053729\n",
            "Epoch 45  \tTraining Loss: 0.8918002991859131\tValidation Loss: 0.8927086301168793\n",
            "Epoch 46  \tTraining Loss: 0.890687879047086\tValidation Loss: 0.8916408952459005\n",
            "Epoch 47  \tTraining Loss: 0.8896370259062526\tValidation Loss: 0.8906317937528014\n",
            "Epoch 48  \tTraining Loss: 0.888644331381172\tValidation Loss: 0.889678088680907\n",
            "Epoch 49  \tTraining Loss: 0.8877065731272301\tValidation Loss: 0.8887767313528164\n",
            "Epoch 50  \tTraining Loss: 0.8868207152704439\tValidation Loss: 0.8879248367355169\n",
            "Epoch 51  \tTraining Loss: 0.885983885353116\tValidation Loss: 0.8871196791267131\n",
            "Epoch 52  \tTraining Loss: 0.8851933698948844\tValidation Loss: 0.8863586833462141\n",
            "Epoch 53  \tTraining Loss: 0.8844466055940425\tValidation Loss: 0.885639416414571\n",
            "Epoch 54  \tTraining Loss: 0.883741171015827\tValidation Loss: 0.8849595796919971\n",
            "Epoch 55  \tTraining Loss: 0.8830747787407209\tValidation Loss: 0.8843170014521096\n",
            "Epoch 56  \tTraining Loss: 0.8824452679473164\tValidation Loss: 0.8837096298664311\n",
            "Epoch 57  \tTraining Loss: 0.8818505974056793\tValidation Loss: 0.8831355263769239\n",
            "Epoch 58  \tTraining Loss: 0.8812888388585035\tValidation Loss: 0.8825928594350911\n",
            "Epoch 59  \tTraining Loss: 0.880758170768586\tValidation Loss: 0.8820798985873566\n",
            "Epoch 60  \tTraining Loss: 0.8802568724123494\tValidation Loss: 0.8815950088875736\n",
            "Epoch 61  \tTraining Loss: 0.8797833183002653\tValidation Loss: 0.8811366456185561\n",
            "Epoch 62  \tTraining Loss: 0.8793359728749386\tValidation Loss: 0.8807033479480376\n",
            "Epoch 63  \tTraining Loss: 0.8789133823364135\tValidation Loss: 0.8802937381556916\n",
            "Epoch 64  \tTraining Loss: 0.8785141798915508\tValidation Loss: 0.8799065058128028\n",
            "Epoch 65  \tTraining Loss: 0.8781370710827007\tValidation Loss: 0.8795404274825914\n",
            "Epoch 66  \tTraining Loss: 0.8777808330947722\tValidation Loss: 0.8791943404570497\n",
            "Epoch 67  \tTraining Loss: 0.8774443107900305\tValidation Loss: 0.8788671457582398\n",
            "Epoch 68  \tTraining Loss: 0.8771264129623552\tValidation Loss: 0.8785578050200656\n",
            "Epoch 69  \tTraining Loss: 0.8768261087988208\tValidation Loss: 0.878265337139443\n",
            "Epoch 70  \tTraining Loss: 0.8765424245370911\tValidation Loss: 0.8779888151126521\n",
            "Epoch 71  \tTraining Loss: 0.8762744403078347\tValidation Loss: 0.8777273630466351\n",
            "Epoch 72  \tTraining Loss: 0.8760212871515632\tValidation Loss: 0.8774801533340951\n",
            "Epoch 73  \tTraining Loss: 0.8757821441998691\tValidation Loss: 0.8772464039885273\n",
            "Epoch 74  \tTraining Loss: 0.8755562360163047\tValidation Loss: 0.877025376123817\n",
            "Epoch 75  \tTraining Loss: 0.8753428300808822\tValidation Loss: 0.8768163715732666\n",
            "Epoch 76  \tTraining Loss: 0.8751412344144599\tValidation Loss: 0.8766187306406569\n",
            "Epoch 77  \tTraining Loss: 0.8749507953349023\tValidation Loss: 0.8764318299756421\n",
            "Epoch 78  \tTraining Loss: 0.8747708953374208\tValidation Loss: 0.8762550805665961\n",
            "Epoch 79  \tTraining Loss: 0.8746009510922323\tValidation Loss: 0.8760879258444138\n",
            "Epoch 80  \tTraining Loss: 0.8744404115530333\tValidation Loss: 0.8759298398911299\n",
            "Epoch 81  \tTraining Loss: 0.8742887561701636\tValidation Loss: 0.8757803257475612\n",
            "Epoch 82  \tTraining Loss: 0.8741454932026594\tValidation Loss: 0.8756389138144851\n",
            "Epoch 83  \tTraining Loss: 0.8740101581237288\tValidation Loss: 0.8755051603422598\n",
            "Epoch 84  \tTraining Loss: 0.8738823121149157\tValidation Loss: 0.875378646004783\n",
            "Epoch 85  \tTraining Loss: 0.8737615373978068\tValidation Loss: 0.8752589667165434\n",
            "Epoch 86  \tTraining Loss: 0.8736474339674868\tValidation Loss: 0.8751457556735798\n",
            "Epoch 87  \tTraining Loss: 0.8735396430035869\tValidation Loss: 0.8750386579567807\n",
            "Epoch 88  \tTraining Loss: 0.8734378126900895\tValidation Loss: 0.8749373372054307\n",
            "Epoch 89  \tTraining Loss: 0.8733416124485939\tValidation Loss: 0.8748414794316053\n",
            "Epoch 90  \tTraining Loss: 0.8732507334811991\tValidation Loss: 0.8747507857641987\n",
            "Epoch 91  \tTraining Loss: 0.8731648810012875\tValidation Loss: 0.8746649735992229\n",
            "Epoch 92  \tTraining Loss: 0.873083776419218\tValidation Loss: 0.8745837760251892\n",
            "Epoch 93  \tTraining Loss: 0.8730071566389803\tValidation Loss: 0.8745069407770735\n",
            "Epoch 94  \tTraining Loss: 0.8729347731087451\tValidation Loss: 0.8744342294277142\n",
            "Epoch 95  \tTraining Loss: 0.8728663740411461\tValidation Loss: 0.8743653880677815\n",
            "Epoch 96  \tTraining Loss: 0.8728017228620663\tValidation Loss: 0.874300125980458\n",
            "Epoch 97  \tTraining Loss: 0.8727406394694224\tValidation Loss: 0.8742383434107381\n",
            "Epoch 98  \tTraining Loss: 0.8726829252626221\tValidation Loss: 0.8741798493442678\n",
            "Epoch 99  \tTraining Loss: 0.8726283918618286\tValidation Loss: 0.874124433466712\n",
            "Epoch 100  \tTraining Loss: 0.872576809113281\tValidation Loss: 0.8740718866966265\n",
            "Epoch 101  \tTraining Loss: 0.872527945126609\tValidation Loss: 0.8740218295861287\n",
            "Epoch 102  \tTraining Loss: 0.8724813220977836\tValidation Loss: 0.8739741504330286\n",
            "Epoch 103  \tTraining Loss: 0.8724370263081974\tValidation Loss: 0.873928731920704\n",
            "Epoch 104  \tTraining Loss: 0.8723947207913\tValidation Loss: 0.8738853314464625\n",
            "Epoch 105  \tTraining Loss: 0.8723542459320842\tValidation Loss: 0.8738431511857976\n",
            "Epoch 106  \tTraining Loss: 0.8723151006776522\tValidation Loss: 0.8738010591811399\n",
            "Epoch 107  \tTraining Loss: 0.872275990979351\tValidation Loss: 0.8737541053418192\n",
            "Epoch 108  \tTraining Loss: 0.8722317818131606\tValidation Loss: 0.8736707967040981\n",
            "Epoch 109  \tTraining Loss: 0.8721630324308288\tValidation Loss: 0.8734565747003962\n",
            "Epoch 110  \tTraining Loss: 0.871967453111955\tValidation Loss: 0.8709846453900089\n",
            "Epoch 111  \tTraining Loss: 0.869248671635285\tValidation Loss: 0.931646166942967\n",
            "Epoch 112  \tTraining Loss: 0.9277306720464344\tValidation Loss: 0.8736853314115463\n",
            "Epoch 113  \tTraining Loss: 0.8721890675324174\tValidation Loss: 0.8736561404785168\n",
            "Epoch 114  \tTraining Loss: 0.8721620329529269\tValidation Loss: 0.8736285062977989\n",
            "Epoch 115  \tTraining Loss: 0.8721364946046536\tValidation Loss: 0.8736023443617326\n",
            "Epoch 116  \tTraining Loss: 0.8721123696785471\tValidation Loss: 0.8735775747934061\n",
            "Epoch 117  \tTraining Loss: 0.8720895799486313\tValidation Loss: 0.8735541220916668\n",
            "Epoch 118  \tTraining Loss: 0.8720680515183539\tValidation Loss: 0.8735319148902094\n",
            "Epoch 119  \tTraining Loss: 0.8720477145809734\tValidation Loss: 0.8735108857299595\n",
            "Epoch 120  \tTraining Loss: 0.8720285031932089\tValidation Loss: 0.8734909708440229\n",
            "Epoch 121  \tTraining Loss: 0.8720103550614167\tValidation Loss: 0.8734721099545011\n",
            "Epoch 122  \tTraining Loss: 0.8719932113396001\tValidation Loss: 0.8734542460805212\n",
            "Epoch 123  \tTraining Loss: 0.8719770164385988\tValidation Loss: 0.873437325356858\n",
            "Epoch 124  \tTraining Loss: 0.8719617178458405\tValidation Loss: 0.8734212968625625\n",
            "Epoch 125  \tTraining Loss: 0.8719472659550651\tValidation Loss: 0.8734061124590482\n",
            "Epoch 126  \tTraining Loss: 0.8719336139054756\tValidation Loss: 0.873391726637106\n",
            "Epoch 127  \tTraining Loss: 0.8719207174297907\tValidation Loss: 0.8733780963723594\n",
            "Epoch 128  \tTraining Loss: 0.8719085347107045\tValidation Loss: 0.873365180988694\n",
            "Epoch 129  \tTraining Loss: 0.8718970262452949\tValidation Loss: 0.873352942029214\n",
            "Epoch 130  \tTraining Loss: 0.871886154716931\tValidation Loss: 0.8733413431343198\n",
            "Epoch 131  \tTraining Loss: 0.871875884874276\tValidation Loss: 0.8733303499265049\n",
            "Epoch 132  \tTraining Loss: 0.8718661834169801\tValidation Loss: 0.8733199299015056\n",
            "Epoch 133  \tTraining Loss: 0.8718570188877052\tValidation Loss: 0.873310052325451\n",
            "Epoch 134  \tTraining Loss: 0.8718483615701215\tValidation Loss: 0.8733006881376816\n",
            "Epoch 135  \tTraining Loss: 0.8718401833925529\tValidation Loss: 0.8732918098589241\n",
            "Epoch 136  \tTraining Loss: 0.871832457836953\tValidation Loss: 0.873283391504528\n",
            "Epoch 137  \tTraining Loss: 0.8718251598529173\tValidation Loss: 0.8732754085024802\n",
            "Epoch 138  \tTraining Loss: 0.8718182657764604\tValidation Loss: 0.8732678376159378\n",
            "Epoch 139  \tTraining Loss: 0.871811753253281\tValidation Loss: 0.8732606568700286\n",
            "Epoch 140  \tTraining Loss: 0.8718056011662786\tValidation Loss: 0.8732538454826831\n",
            "Epoch 141  \tTraining Loss: 0.871799789567081\tValidation Loss: 0.8732473837992747\n",
            "Epoch 142  \tTraining Loss: 0.8717942996113601\tValidation Loss: 0.8732412532308601\n",
            "Epoch 143  \tTraining Loss: 0.8717891134977293\tValidation Loss: 0.8732354361958193\n",
            "Epoch 144  \tTraining Loss: 0.8717842144100205\tValidation Loss: 0.8732299160647083\n",
            "Epoch 145  \tTraining Loss: 0.8717795864627589\tValidation Loss: 0.8732246771081463\n",
            "Epoch 146  \tTraining Loss: 0.8717752146496518\tValidation Loss: 0.8732197044475718\n",
            "Epoch 147  \tTraining Loss: 0.8717710847949315\tValidation Loss: 0.8732149840087072\n",
            "Epoch 148  \tTraining Loss: 0.87176718350739\tValidation Loss: 0.873210502477584\n",
            "Epoch 149  \tTraining Loss: 0.871763498136956\tValidation Loss: 0.873206247258986\n",
            "Epoch 150  \tTraining Loss: 0.8717600167336776\tValidation Loss: 0.8732022064371786\n",
            "Epoch 151  \tTraining Loss: 0.871756728008975\tValidation Loss: 0.8731983687387965\n",
            "Epoch 152  \tTraining Loss: 0.8717536212990348\tValidation Loss: 0.8731947234977723\n",
            "Epoch 153  \tTraining Loss: 0.8717506865302335\tValidation Loss: 0.8731912606221915\n",
            "Epoch 154  \tTraining Loss: 0.871747914186474\tValidation Loss: 0.8731879705629706\n",
            "Epoch 155  \tTraining Loss: 0.8717452952783277\tValidation Loss: 0.8731848442842535\n",
            "Epoch 156  \tTraining Loss: 0.8717428213138865\tValidation Loss: 0.873181873235437\n",
            "Epoch 157  \tTraining Loss: 0.8717404842712282\tValidation Loss: 0.8731790493247297\n",
            "Epoch 158  \tTraining Loss: 0.8717382765724049\tValidation Loss: 0.8731763648941641\n",
            "Epoch 159  \tTraining Loss: 0.87173619105887\tValidation Loss: 0.8731738126959799\n",
            "Epoch 160  \tTraining Loss: 0.8717342209682681\tValidation Loss: 0.8731713858703041\n",
            "Epoch 161  \tTraining Loss: 0.8717323599125066\tValidation Loss: 0.8731690779240538\n",
            "Epoch 162  \tTraining Loss: 0.8717306018570424\tValidation Loss: 0.8731668827109985\n",
            "Epoch 163  \tTraining Loss: 0.8717289411013149\tValidation Loss: 0.8731647944129138\n",
            "Epoch 164  \tTraining Loss: 0.8717273722602615\tValidation Loss: 0.8731628075217694\n",
            "Epoch 165  \tTraining Loss: 0.8717258902468569\tValidation Loss: 0.8731609168228933\n",
            "Epoch 166  \tTraining Loss: 0.8717244902556158\tValidation Loss: 0.873159117379057\n",
            "Epoch 167  \tTraining Loss: 0.8717231677470153\tValidation Loss: 0.8731574045154356\n",
            "Epoch 168  \tTraining Loss: 0.8717219184327715\tValidation Loss: 0.8731557738053877\n",
            "Epoch 169  \tTraining Loss: 0.8717207382619361\tValidation Loss: 0.8731542210570169\n",
            "Epoch 170  \tTraining Loss: 0.8717196234077613\tValidation Loss: 0.8731527423004662\n",
            "Epoch 171  \tTraining Loss: 0.8717185702552901\tValidation Loss: 0.8731513337759101\n",
            "Epoch 172  \tTraining Loss: 0.8717175753896365\tValidation Loss: 0.8731499919222014\n",
            "Epoch 173  \tTraining Loss: 0.8717166355849114\tValidation Loss: 0.8731487133661391\n",
            "Epoch 174  \tTraining Loss: 0.8717157477937628\tValidation Loss: 0.8731474949123242\n",
            "Epoch 175  \tTraining Loss: 0.8717149091374948\tValidation Loss: 0.8731463335335677\n",
            "Epoch 176  \tTraining Loss: 0.8717141168967326\tValidation Loss: 0.8731452263618238\n",
            "Epoch 177  \tTraining Loss: 0.8717133685026064\tValidation Loss: 0.8731441706796179\n",
            "Epoch 178  \tTraining Loss: 0.8717126615284199\tValidation Loss: 0.8731431639119426\n",
            "Epoch 179  \tTraining Loss: 0.8717119936817826\tValidation Loss: 0.8731422036185945\n",
            "Epoch 180  \tTraining Loss: 0.8717113627971778\tValidation Loss: 0.873141287486931\n",
            "Epoch 181  \tTraining Loss: 0.8717107668289372\tValidation Loss: 0.8731404133250196\n",
            "Epoch 182  \tTraining Loss: 0.8717102038446123\tValidation Loss: 0.8731395790551627\n",
            "Epoch 183  \tTraining Loss: 0.8717096720187051\tValidation Loss: 0.8731387827077728\n",
            "Epoch 184  \tTraining Loss: 0.8717091696267504\tValidation Loss: 0.8731380224155831\n",
            "Epoch 185  \tTraining Loss: 0.8717086950397234\tValidation Loss: 0.8731372964081721\n",
            "Epoch 186  \tTraining Loss: 0.8717082467187576\tValidation Loss: 0.8731366030067854\n",
            "Epoch 187  \tTraining Loss: 0.8717078232101557\tValidation Loss: 0.8731359406194408\n",
            "Epoch 188  \tTraining Loss: 0.8717074231406752\tValidation Loss: 0.8731353077362971\n",
            "Epoch 189  \tTraining Loss: 0.871707045213076\tValidation Loss: 0.873134702925276\n",
            "Epoch 190  \tTraining Loss: 0.8717066882019133\tValidation Loss: 0.8731341248279211\n",
            "Epoch 191  \tTraining Loss: 0.8717063509495657\tValidation Loss: 0.8731335721554813\n",
            "Epoch 192  \tTraining Loss: 0.871706032362479\tValidation Loss: 0.8731330436852068\n",
            "Epoch 193  \tTraining Loss: 0.8717057314076232\tValidation Loss: 0.873132538256846\n",
            "Epoch 194  \tTraining Loss: 0.871705447109141\tValidation Loss: 0.8731320547693309\n",
            "Epoch 195  \tTraining Loss: 0.8717051785451834\tValidation Loss: 0.873131592177644\n",
            "Epoch 196  \tTraining Loss: 0.8717049248449221\tValidation Loss: 0.8731311494898525\n",
            "Epoch 197  \tTraining Loss: 0.8717046851857245\tValidation Loss: 0.8731307257643035\n",
            "Epoch 198  \tTraining Loss: 0.8717044587904865\tValidation Loss: 0.873130320106972\n",
            "Epoch 199  \tTraining Loss: 0.8717042449251133\tValidation Loss: 0.8731299316689497\n",
            "Epoch 200  \tTraining Loss: 0.8717040428961382\tValidation Loss: 0.873129559644071\n",
            "Epoch 201  \tTraining Loss: 0.8717038520484748\tValidation Loss: 0.8731292032666667\n",
            "Epoch 202  \tTraining Loss: 0.8717036717632924\tValidation Loss: 0.8731288618094377\n",
            "Epoch 203  \tTraining Loss: 0.8717035014560097\tValidation Loss: 0.8731285345814443\n",
            "Epoch 204  \tTraining Loss: 0.8717033405743984\tValidation Loss: 0.8731282209262036\n",
            "Epoch 205  \tTraining Loss: 0.8717031885967944\tValidation Loss: 0.8731279202198867\n",
            "Epoch 206  \tTraining Loss: 0.8717030450304045\tValidation Loss: 0.8731276318696177\n",
            "Epoch 207  \tTraining Loss: 0.8717029094097093\tValidation Loss: 0.8731273553118585\n",
            "Epoch 208  \tTraining Loss: 0.8717027812949537\tValidation Loss: 0.8731270900108836\n",
            "Epoch 209  \tTraining Loss: 0.8717026602707204\tValidation Loss: 0.8731268354573353\n",
            "Epoch 210  \tTraining Loss: 0.8717025459445844\tValidation Loss: 0.8731265911668568\n",
            "Epoch 211  \tTraining Loss: 0.8717024379458386\tValidation Loss: 0.8731263566787975\n",
            "Epoch 212  \tTraining Loss: 0.8717023359242928\tValidation Loss: 0.8731261315549889\n",
            "Epoch 213  \tTraining Loss: 0.8717022395491386\tValidation Loss: 0.8731259153785838\n",
            "Epoch 214  \tTraining Loss: 0.8717021485078762\tValidation Loss: 0.8731257077529591\n",
            "Epoch 215  \tTraining Loss: 0.8717020625053005\tValidation Loss: 0.8731255083006767\n",
            "Epoch 216  \tTraining Loss: 0.8717019812625448\tValidation Loss: 0.8731253166624984\n",
            "Epoch 217  \tTraining Loss: 0.8717019045161768\tValidation Loss: 0.873125132496455\n",
            "Epoch 218  \tTraining Loss: 0.8717018320173433\tValidation Loss: 0.8731249554769633\n",
            "Epoch 219  \tTraining Loss: 0.8717017635309646\tValidation Loss: 0.8731247852939907\n",
            "Epoch 220  \tTraining Loss: 0.8717016988349703\tValidation Loss: 0.8731246216522641\n",
            "Epoch 221  \tTraining Loss: 0.8717016377195815\tValidation Loss: 0.87312446427052\n",
            "Epoch 222  \tTraining Loss: 0.8717015799866293\tValidation Loss: 0.8731243128807948\n",
            "Epoch 223  \tTraining Loss: 0.871701525448913\tValidation Loss: 0.8731241672277521\n",
            "Epoch 224  \tTraining Loss: 0.8717014739295911\tValidation Loss: 0.8731240270680464\n",
            "Epoch 225  \tTraining Loss: 0.871701425261611\tValidation Loss: 0.8731238921697181\n",
            "Epoch 226  \tTraining Loss: 0.8717013792871641\tValidation Loss: 0.8731237623116235\n",
            "Epoch 227  \tTraining Loss: 0.8717013358571775\tValidation Loss: 0.8731236372828919\n",
            "Epoch 228  \tTraining Loss: 0.8717012948308271\tValidation Loss: 0.8731235168824122\n",
            "Epoch 229  \tTraining Loss: 0.8717012560750839\tValidation Loss: 0.8731234009183468\n",
            "Epoch 230  \tTraining Loss: 0.8717012194642807\tValidation Loss: 0.8731232892076706\n",
            "Epoch 231  \tTraining Loss: 0.8717011848797057\tValidation Loss: 0.8731231815757332\n",
            "Epoch 232  \tTraining Loss: 0.8717011522092171\tValidation Loss: 0.8731230778558453\n",
            "Epoch 233  \tTraining Loss: 0.8717011213468795\tValidation Loss: 0.873122977888886\n",
            "Epoch 234  \tTraining Loss: 0.8717010921926209\tValidation Loss: 0.8731228815229299\n",
            "Epoch 235  \tTraining Loss: 0.8717010646519074\tValidation Loss: 0.873122788612894\n",
            "Epoch 236  \tTraining Loss: 0.8717010386354369\tValidation Loss: 0.8731226990202041\n",
            "Epoch 237  \tTraining Loss: 0.8717010140588504\tValidation Loss: 0.8731226126124759\n",
            "Epoch 238  \tTraining Loss: 0.8717009908424574\tValidation Loss: 0.8731225292632148\n",
            "Epoch 239  \tTraining Loss: 0.8717009689109776\tValidation Loss: 0.8731224488515298\n",
            "Epoch 240  \tTraining Loss: 0.8717009481932976\tValidation Loss: 0.8731223712618629\n",
            "Epoch 241  \tTraining Loss: 0.8717009286222395\tValidation Loss: 0.8731222963837316\n",
            "Epoch 242  \tTraining Loss: 0.8717009101343433\tValidation Loss: 0.8731222241114859\n",
            "Epoch 243  \tTraining Loss: 0.8717008926696612\tValidation Loss: 0.8731221543440759\n",
            "Epoch 244  \tTraining Loss: 0.8717008761715636\tValidation Loss: 0.8731220869848328\n",
            "Epoch 245  \tTraining Loss: 0.8717008605865547\tValidation Loss: 0.8731220219412601\n",
            "Epoch 246  \tTraining Loss: 0.8717008458640997\tValidation Loss: 0.873121959124836\n",
            "Epoch 247  \tTraining Loss: 0.87170083195646\tValidation Loss: 0.8731218984508252\n",
            "Epoch 248  \tTraining Loss: 0.8717008188185399\tValidation Loss: 0.8731218398381011\n",
            "Epoch 249  \tTraining Loss: 0.8717008064077393\tValidation Loss: 0.8731217832089756\n",
            "Epoch 250  \tTraining Loss: 0.8717007946838152\tValidation Loss: 0.8731217284890395\n",
            "Epoch 251  \tTraining Loss: 0.8717007836087532\tValidation Loss: 0.8731216756070085\n",
            "Epoch 252  \tTraining Loss: 0.8717007731466413\tValidation Loss: 0.8731216244945786\n",
            "Epoch 253  \tTraining Loss: 0.8717007632635558\tValidation Loss: 0.8731215750862884\n",
            "Epoch 254  \tTraining Loss: 0.8717007539274508\tValidation Loss: 0.8731215273193882\n",
            "Epoch 255  \tTraining Loss: 0.871700745108053\tValidation Loss: 0.8731214811337146\n",
            "Epoch 256  \tTraining Loss: 0.871700736776766\tValidation Loss: 0.8731214364715735\n",
            "Epoch 257  \tTraining Loss: 0.8717007289065747\tValidation Loss: 0.8731213932776267\n",
            "Epoch 258  \tTraining Loss: 0.8717007214719598\tValidation Loss: 0.8731213514987852\n",
            "Epoch 259  \tTraining Loss: 0.8717007144488143\tValidation Loss: 0.873121311084108\n",
            "Epoch 260  \tTraining Loss: 0.8717007078143653\tValidation Loss: 0.8731212719847042\n",
            "Epoch 261  \tTraining Loss: 0.8717007015471006\tValidation Loss: 0.8731212341536431\n",
            "Epoch 262  \tTraining Loss: 0.8717006956266983\tValidation Loss: 0.8731211975458647\n",
            "Epoch 263  \tTraining Loss: 0.8717006900339611\tValidation Loss: 0.8731211621180976\n",
            "Epoch 264  \tTraining Loss: 0.8717006847507544\tValidation Loss: 0.8731211278287794\n",
            "Epoch 265  \tTraining Loss: 0.8717006797599473\tValidation Loss: 0.8731210946379819\n",
            "Epoch 266  \tTraining Loss: 0.8717006750453568\tValidation Loss: 0.8731210625073386\n",
            "Epoch 267  \tTraining Loss: 0.8717006705916959\tValidation Loss: 0.8731210313999771\n",
            "Epoch 268  \tTraining Loss: 0.8717006663845233\tValidation Loss: 0.8731210012804534\n",
            "Epoch 269  \tTraining Loss: 0.8717006624101967\tValidation Loss: 0.8731209721146911\n",
            "Epoch 270  \tTraining Loss: 0.8717006586558296\tValidation Loss: 0.8731209438699209\n",
            "Epoch 271  \tTraining Loss: 0.8717006551092487\tValidation Loss: 0.873120916514626\n",
            "Epoch 272  \tTraining Loss: 0.8717006517589534\tValidation Loss: 0.8731208900184877\n",
            "Epoch 273  \tTraining Loss: 0.8717006485940803\tValidation Loss: 0.873120864352334\n",
            "Epoch 274  \tTraining Loss: 0.8717006456043674\tValidation Loss: 0.8731208394880929\n",
            "Epoch 275  \tTraining Loss: 0.8717006427801207\tValidation Loss: 0.8731208153987432\n",
            "Epoch 276  \tTraining Loss: 0.871700640112182\tValidation Loss: 0.8731207920582729\n",
            "Epoch 277  \tTraining Loss: 0.8717006375919009\tValidation Loss: 0.8731207694416355\n",
            "Epoch 278  \tTraining Loss: 0.8717006352111046\tValidation Loss: 0.8731207475247109\n",
            "Epoch 279  \tTraining Loss: 0.8717006329620738\tValidation Loss: 0.8731207262842661\n",
            "Epoch 280  \tTraining Loss: 0.8717006308375159\tValidation Loss: 0.8731207056979197\n",
            "Epoch 281  \tTraining Loss: 0.8717006288305418\tValidation Loss: 0.8731206857441053\n",
            "Epoch 282  \tTraining Loss: 0.8717006269346437\tValidation Loss: 0.8731206664020407\n",
            "Epoch 283  \tTraining Loss: 0.8717006251436744\tValidation Loss: 0.8731206476516934\n",
            "Epoch 284  \tTraining Loss: 0.8717006234518266\tValidation Loss: 0.8731206294737525\n",
            "Epoch 285  \tTraining Loss: 0.8717006218536141\tValidation Loss: 0.8731206118495981\n",
            "Epoch 286  \tTraining Loss: 0.8717006203438548\tValidation Loss: 0.8731205947612747\n",
            "Epoch 287  \tTraining Loss: 0.8717006189176533\tValidation Loss: 0.8731205781914639\n",
            "Epoch 288  \tTraining Loss: 0.871700617570385\tValidation Loss: 0.87312056212346\n",
            "Epoch 289  \tTraining Loss: 0.8717006162976814\tValidation Loss: 0.8731205465411449\n",
            "Epoch 290  \tTraining Loss: 0.8717006150954159\tValidation Loss: 0.8731205314289655\n",
            "Epoch 291  \tTraining Loss: 0.8717006139596898\tValidation Loss: 0.8731205167719117\n",
            "Epoch 292  \tTraining Loss: 0.8717006128868202\tValidation Loss: 0.8731205025554954\n",
            "Epoch 293  \tTraining Loss: 0.8717006118733291\tValidation Loss: 0.8731204887657295\n",
            "Epoch 294  \tTraining Loss: 0.8717006109159297\tValidation Loss: 0.8731204753891092\n",
            "Epoch 295  \tTraining Loss: 0.8717006100115172\tValidation Loss: 0.8731204624125938\n",
            "Epoch 296  \tTraining Loss: 0.8717006091571601\tValidation Loss: 0.8731204498235884\n",
            "Epoch 297  \tTraining Loss: 0.8717006083500872\tValidation Loss: 0.873120437609927\n",
            "Epoch 298  \tTraining Loss: 0.871700607587682\tValidation Loss: 0.8731204257598565\n",
            "Epoch 299  \tTraining Loss: 0.8717006068674718\tValidation Loss: 0.8731204142620215\n",
            "Epoch 300  \tTraining Loss: 0.871700606187122\tValidation Loss: 0.873120403105448\n",
            "Epoch 301  \tTraining Loss: 0.8717006055444261\tValidation Loss: 0.8731203922795316\n",
            "Epoch 302  \tTraining Loss: 0.8717006049373004\tValidation Loss: 0.8731203817740215\n",
            "Epoch 303  \tTraining Loss: 0.871700604363776\tValidation Loss: 0.8731203715790081\n",
            "Epoch 304  \tTraining Loss: 0.8717006038219932\tValidation Loss: 0.8731203616849114\n",
            "Epoch 305  \tTraining Loss: 0.8717006033101957\tValidation Loss: 0.8731203520824674\n",
            "Epoch 306  \tTraining Loss: 0.8717006028267237\tValidation Loss: 0.8731203427627179\n",
            "Epoch 307  \tTraining Loss: 0.8717006023700093\tValidation Loss: 0.873120333716999\n",
            "Epoch 308  \tTraining Loss: 0.8717006019385719\tValidation Loss: 0.8731203249369299\n",
            "Epoch 309  \tTraining Loss: 0.8717006015310124\tValidation Loss: 0.8731203164144036\n",
            "Epoch 310  \tTraining Loss: 0.8717006011460092\tValidation Loss: 0.8731203081415763\n",
            "Epoch 311  \tTraining Loss: 0.8717006007823143\tValidation Loss: 0.8731203001108585\n",
            "Epoch 312  \tTraining Loss: 0.8717006004387482\tValidation Loss: 0.8731202923149065\n",
            "Epoch 313  \tTraining Loss: 0.8717006001141966\tValidation Loss: 0.8731202847466121\n",
            "Epoch 314  \tTraining Loss: 0.8717005998076073\tValidation Loss: 0.8731202773990955\n",
            "Epoch 315  \tTraining Loss: 0.8717005995179861\tValidation Loss: 0.873120270265698\n",
            "Epoch 316  \tTraining Loss: 0.8717005992443941\tValidation Loss: 0.8731202633399726\n",
            "Epoch 317  \tTraining Loss: 0.8717005989859443\tValidation Loss: 0.8731202566156775\n",
            "Epoch 318  \tTraining Loss: 0.8717005987417983\tValidation Loss: 0.8731202500867692\n",
            "Epoch 319  \tTraining Loss: 0.8717005985111645\tValidation Loss: 0.8731202437473955\n",
            "Epoch 320  \tTraining Loss: 0.8717005982932953\tValidation Loss: 0.8731202375918882\n",
            "Epoch 321  \tTraining Loss: 0.8717005980874838\tValidation Loss: 0.8731202316147579\n",
            "Epoch 322  \tTraining Loss: 0.8717005978930632\tValidation Loss: 0.8731202258106883\n",
            "Epoch 323  \tTraining Loss: 0.8717005977094029\tValidation Loss: 0.8731202201745276\n",
            "Epoch 324  \tTraining Loss: 0.8717005975359069\tValidation Loss: 0.8731202147012868\n",
            "Epoch 325  \tTraining Loss: 0.8717005973720132\tValidation Loss: 0.8731202093861307\n",
            "Epoch 326  \tTraining Loss: 0.8717005972171904\tValidation Loss: 0.8731202042243753\n",
            "Epoch 327  \tTraining Loss: 0.8717005970709365\tValidation Loss: 0.8731201992114817\n",
            "Epoch 328  \tTraining Loss: 0.8717005969327767\tValidation Loss: 0.8731201943430505\n",
            "Epoch 329  \tTraining Loss: 0.8717005968022634\tValidation Loss: 0.8731201896148191\n",
            "Epoch 330  \tTraining Loss: 0.8717005966789731\tValidation Loss: 0.8731201850226553\n",
            "Epoch 331  \tTraining Loss: 0.8717005965625066\tValidation Loss: 0.8731201805625539\n",
            "Epoch 332  \tTraining Loss: 0.8717005964524861\tValidation Loss: 0.8731201762306332\n",
            "Epoch 333  \tTraining Loss: 0.8717005963485545\tValidation Loss: 0.8731201720231286\n",
            "Epoch 334  \tTraining Loss: 0.8717005962503752\tValidation Loss: 0.8731201679363919\n",
            "Epoch 335  \tTraining Loss: 0.8717005961576293\tValidation Loss: 0.8731201639668853\n",
            "Epoch 336  \tTraining Loss: 0.8717005960700167\tValidation Loss: 0.8731201601111784\n",
            "Epoch 337  \tTraining Loss: 0.8717005959872528\tValidation Loss: 0.8731201563659449\n",
            "Epoch 338  \tTraining Loss: 0.8717005959090697\tValidation Loss: 0.8731201527279595\n",
            "Epoch 339  \tTraining Loss: 0.8717005958352135\tValidation Loss: 0.873120149194094\n",
            "Epoch 340  \tTraining Loss: 0.8717005957654449\tValidation Loss: 0.8731201457613145\n",
            "Epoch 341  \tTraining Loss: 0.871700595699538\tValidation Loss: 0.8731201424266787\n",
            "Epoch 342  \tTraining Loss: 0.871700595637278\tValidation Loss: 0.8731201391873324\n",
            "Epoch 343  \tTraining Loss: 0.8717005955784644\tValidation Loss: 0.8731201360405074\n",
            "Epoch 344  \tTraining Loss: 0.8717005955229056\tValidation Loss: 0.8731201329835181\n",
            "Epoch 345  \tTraining Loss: 0.8717005954704218\tValidation Loss: 0.8731201300137589\n",
            "Epoch 346  \tTraining Loss: 0.8717005954208427\tValidation Loss: 0.8731201271287029\n",
            "Epoch 347  \tTraining Loss: 0.8717005953740077\tValidation Loss: 0.8731201243258977\n",
            "Epoch 348  \tTraining Loss: 0.8717005953297644\tValidation Loss: 0.8731201216029638\n",
            "Epoch 349  \tTraining Loss: 0.8717005952879701\tValidation Loss: 0.873120118957593\n",
            "Epoch 350  \tTraining Loss: 0.8717005952484886\tValidation Loss: 0.8731201163875454\n",
            "Epoch 351  \tTraining Loss: 0.8717005952111926\tValidation Loss: 0.8731201138906473\n",
            "Epoch 352  \tTraining Loss: 0.8717005951759604\tValidation Loss: 0.873120111464789\n",
            "Epoch 353  \tTraining Loss: 0.8717005951426784\tValidation Loss: 0.8731201091079238\n",
            "Epoch 354  \tTraining Loss: 0.8717005951112382\tValidation Loss: 0.873120106818065\n",
            "Epoch 355  \tTraining Loss: 0.871700595081538\tValidation Loss: 0.8731201045932845\n",
            "Epoch 356  \tTraining Loss: 0.8717005950534819\tValidation Loss: 0.8731201024317108\n",
            "Epoch 357  \tTraining Loss: 0.8717005950269782\tValidation Loss: 0.8731201003315278\n",
            "Epoch 358  \tTraining Loss: 0.8717005950019414\tValidation Loss: 0.8731200982909724\n",
            "Epoch 359  \tTraining Loss: 0.8717005949782906\tValidation Loss: 0.8731200963083331\n",
            "Epoch 360  \tTraining Loss: 0.8717005949559483\tValidation Loss: 0.8731200943819489\n",
            "Epoch 361  \tTraining Loss: 0.8717005949348428\tValidation Loss: 0.8731200925102071\n",
            "Epoch 362  \tTraining Loss: 0.8717005949149053\tValidation Loss: 0.8731200906915423\n",
            "Epoch 363  \tTraining Loss: 0.8717005948960713\tValidation Loss: 0.8731200889244346\n",
            "Epoch 364  \tTraining Loss: 0.8717005948782797\tValidation Loss: 0.8731200872074085\n",
            "Epoch 365  \tTraining Loss: 0.8717005948614727\tValidation Loss: 0.8731200855390318\n",
            "Epoch 366  \tTraining Loss: 0.8717005948455959\tValidation Loss: 0.8731200839179133\n",
            "Epoch 367  \tTraining Loss: 0.8717005948305977\tValidation Loss: 0.8731200823427031\n",
            "Epoch 368  \tTraining Loss: 0.8717005948164298\tValidation Loss: 0.8731200808120898\n",
            "Epoch 369  \tTraining Loss: 0.871700594803046\tValidation Loss: 0.8731200793248004\n",
            "Epoch 370  \tTraining Loss: 0.8717005947904027\tValidation Loss: 0.8731200778795988\n",
            "Epoch 371  \tTraining Loss: 0.8717005947784593\tValidation Loss: 0.8731200764752847\n",
            "Epoch 372  \tTraining Loss: 0.8717005947671771\tValidation Loss: 0.8731200751106923\n",
            "Epoch 373  \tTraining Loss: 0.8717005947565192\tValidation Loss: 0.8731200737846898\n",
            "Epoch 374  \tTraining Loss: 0.8717005947464509\tValidation Loss: 0.8731200724961781\n",
            "Epoch 375  \tTraining Loss: 0.87170059473694\tValidation Loss: 0.8731200712440896\n",
            "Epoch 376  \tTraining Loss: 0.8717005947279555\tValidation Loss: 0.8731200700273882\n",
            "Epoch 377  \tTraining Loss: 0.8717005947194681\tValidation Loss: 0.8731200688450665\n",
            "Epoch 378  \tTraining Loss: 0.8717005947114507\tValidation Loss: 0.873120067696147\n",
            "Epoch 379  \tTraining Loss: 0.8717005947038768\tValidation Loss: 0.8731200665796804\n",
            "Epoch 380  \tTraining Loss: 0.8717005946967223\tValidation Loss: 0.873120065494744\n",
            "Epoch 381  \tTraining Loss: 0.8717005946899635\tValidation Loss: 0.8731200644404421\n",
            "Epoch 382  \tTraining Loss: 0.871700594683579\tValidation Loss: 0.873120063415905\n",
            "Epoch 383  \tTraining Loss: 0.8717005946775478\tValidation Loss: 0.8731200624202873\n",
            "Epoch 384  \tTraining Loss: 0.8717005946718503\tValidation Loss: 0.8731200614527688\n",
            "Epoch 385  \tTraining Loss: 0.8717005946664681\tValidation Loss: 0.8731200605125514\n",
            "Epoch 386  \tTraining Loss: 0.8717005946613838\tValidation Loss: 0.8731200595988617\n",
            "Epoch 387  \tTraining Loss: 0.8717005946565812\tValidation Loss: 0.8731200587109467\n",
            "Epoch 388  \tTraining Loss: 0.8717005946520442\tValidation Loss: 0.8731200578480762\n",
            "Epoch 389  \tTraining Loss: 0.8717005946477583\tValidation Loss: 0.8731200570095401\n",
            "Epoch 390  \tTraining Loss: 0.8717005946437093\tValidation Loss: 0.8731200561946493\n",
            "Epoch 391  \tTraining Loss: 0.8717005946398847\tValidation Loss: 0.8731200554027339\n",
            "Epoch 392  \tTraining Loss: 0.8717005946362715\tValidation Loss: 0.8731200546331431\n",
            "Epoch 393  \tTraining Loss: 0.8717005946328586\tValidation Loss: 0.8731200538852452\n",
            "Epoch 394  \tTraining Loss: 0.8717005946296347\tValidation Loss: 0.8731200531584259\n",
            "Epoch 395  \tTraining Loss: 0.8717005946265889\tValidation Loss: 0.8731200524520891\n",
            "Epoch 396  \tTraining Loss: 0.8717005946237119\tValidation Loss: 0.8731200517656551\n",
            "Epoch 397  \tTraining Loss: 0.8717005946209939\tValidation Loss: 0.8731200510985608\n",
            "Epoch 398  \tTraining Loss: 0.8717005946184264\tValidation Loss: 0.8731200504502595\n",
            "Epoch 399  \tTraining Loss: 0.8717005946160011\tValidation Loss: 0.8731200498202201\n",
            "Epoch 400  \tTraining Loss: 0.8717005946137097\tValidation Loss: 0.8731200492079257\n",
            "lr, batch_size: (0.01, 300)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 914.3434050937504\n",
            "Epoch 2  \tTraining Loss: 914.8765254955363\tValidation Loss: 0.9401842090383703\n",
            "Epoch 3  \tTraining Loss: 0.9370181172349683\tValidation Loss: 0.938417920087931\n",
            "Epoch 4  \tTraining Loss: 0.9352944862056255\tValidation Loss: 0.936747075070756\n",
            "Epoch 5  \tTraining Loss: 0.933665246573885\tValidation Loss: 0.9351675675169631\n",
            "Epoch 6  \tTraining Loss: 0.9321261771003404\tValidation Loss: 0.9336743741716047\n",
            "Epoch 7  \tTraining Loss: 0.9306722873049843\tValidation Loss: 0.932262748898413\n",
            "Epoch 8  \tTraining Loss: 0.9292988629063693\tValidation Loss: 0.930928207367703\n",
            "Epoch 9  \tTraining Loss: 0.9280014505353674\tValidation Loss: 0.9296665125910237\n",
            "Epoch 10  \tTraining Loss: 0.9267758432949464\tValidation Loss: 0.9284736612557136\n",
            "Epoch 11  \tTraining Loss: 0.9256180671191448\tValidation Loss: 0.9273458708151057\n",
            "Epoch 12  \tTraining Loss: 0.9245243678870094\tValidation Loss: 0.9262795672925838\n",
            "Epoch 13  \tTraining Loss: 0.9234911992497139\tValidation Loss: 0.9252713737599956\n",
            "Epoch 14  \tTraining Loss: 0.9225152111313887\tValidation Loss: 0.9243180994531209\n",
            "Epoch 15  \tTraining Loss: 0.9215932388663703\tValidation Loss: 0.9234167294889514\n",
            "Epoch 16  \tTraining Loss: 0.9207222929376548\tValidation Loss: 0.9225644151514933\n",
            "Epoch 17  \tTraining Loss: 0.9198995492832761\tValidation Loss: 0.9217584647146432\n",
            "Epoch 18  \tTraining Loss: 0.9191223401391766\tValidation Loss: 0.920996334772427\n",
            "Epoch 19  \tTraining Loss: 0.9183881453888855\tValidation Loss: 0.9202756220485412\n",
            "Epoch 20  \tTraining Loss: 0.917694584391944\tValidation Loss: 0.9195940556586788\n",
            "Epoch 21  \tTraining Loss: 0.9170394082645927\tValidation Loss: 0.918949489800599\n",
            "Epoch 22  \tTraining Loss: 0.9164204925876811\tValidation Loss: 0.9183398968482793\n",
            "Epoch 23  \tTraining Loss: 0.9158358305181611\tValidation Loss: 0.9177633608278012\n",
            "Epoch 24  \tTraining Loss: 0.9152835262821043\tValidation Loss: 0.917218071253747\n",
            "Epoch 25  \tTraining Loss: 0.9147617890290651\tValidation Loss: 0.9167023173061593\n",
            "Epoch 26  \tTraining Loss: 0.9142689270216375\tValidation Loss: 0.9162144823298196\n",
            "Epoch 27  \tTraining Loss: 0.913803342150826\tValidation Loss: 0.9157530386373933\n",
            "Epoch 28  \tTraining Loss: 0.9133635247550738\tValidation Loss: 0.9153165425996779\n",
            "Epoch 29  \tTraining Loss: 0.9129480487251275\tValidation Loss: 0.9149036300071866\n",
            "Epoch 30  \tTraining Loss: 0.9125555668798249\tValidation Loss: 0.9145130116880591\n",
            "Epoch 31  \tTraining Loss: 0.9121848065978083\tValidation Loss: 0.914143469368121\n",
            "Epoch 32  \tTraining Loss: 0.9118345656910037\tValidation Loss: 0.9137933444379073\n",
            "Epoch 33  \tTraining Loss: 0.911503201974437\tValidation Loss: 0.911566019941909\n",
            "Epoch 34  \tTraining Loss: 0.9093003221542041\tValidation Loss: 0.909461300535763\n",
            "Epoch 35  \tTraining Loss: 0.9072193610344994\tValidation Loss: 0.9074724193138101\n",
            "Epoch 36  \tTraining Loss: 0.9052535710026187\tValidation Loss: 0.9055929833596824\n",
            "Epoch 37  \tTraining Loss: 0.9033965778934165\tValidation Loss: 0.9038169530633082\n",
            "Epoch 38  \tTraining Loss: 0.9016423603208193\tValidation Loss: 0.9021386225815272\n",
            "Epoch 39  \tTraining Loss: 0.8999852301532403\tValidation Loss: 0.9005526013797065\n",
            "Epoch 40  \tTraining Loss: 0.8984198140695785\tValidation Loss: 0.8990537967945418\n",
            "Epoch 41  \tTraining Loss: 0.8969410361360065\tValidation Loss: 0.8976373975615347\n",
            "Epoch 42  \tTraining Loss: 0.8955441013470419\tValidation Loss: 0.8962988582537708\n",
            "Epoch 43  \tTraining Loss: 0.8942244800775405\tValidation Loss: 0.8950338845815689\n",
            "Epoch 44  \tTraining Loss: 0.8929778933951952\tValidation Loss: 0.8938384195053729\n",
            "Epoch 45  \tTraining Loss: 0.8918002991859131\tValidation Loss: 0.8927086301168793\n",
            "Epoch 46  \tTraining Loss: 0.890687879047086\tValidation Loss: 0.8916408952459005\n",
            "Epoch 47  \tTraining Loss: 0.8896370259062526\tValidation Loss: 0.8906317937528014\n",
            "Epoch 48  \tTraining Loss: 0.888644331381172\tValidation Loss: 0.889678088680907\n",
            "Epoch 49  \tTraining Loss: 0.8877065731272301\tValidation Loss: 0.8887767313528164\n",
            "Epoch 50  \tTraining Loss: 0.8868207152704439\tValidation Loss: 0.8879248367355169\n",
            "Epoch 51  \tTraining Loss: 0.885983885353116\tValidation Loss: 0.8871196791267131\n",
            "Epoch 52  \tTraining Loss: 0.8851933698948844\tValidation Loss: 0.8863586833462141\n",
            "Epoch 53  \tTraining Loss: 0.8844466055940425\tValidation Loss: 0.885639416414571\n",
            "Epoch 54  \tTraining Loss: 0.883741171015827\tValidation Loss: 0.8849595796919971\n",
            "Epoch 55  \tTraining Loss: 0.8830747787407209\tValidation Loss: 0.8843170014521096\n",
            "Epoch 56  \tTraining Loss: 0.8824452679473164\tValidation Loss: 0.8837096298664311\n",
            "Epoch 57  \tTraining Loss: 0.8818505974056793\tValidation Loss: 0.8831355263769239\n",
            "Epoch 58  \tTraining Loss: 0.8812888388585035\tValidation Loss: 0.8825928594350911\n",
            "Epoch 59  \tTraining Loss: 0.880758170768586\tValidation Loss: 0.8820798985873566\n",
            "Epoch 60  \tTraining Loss: 0.8802568724123494\tValidation Loss: 0.8815950088875736\n",
            "Epoch 61  \tTraining Loss: 0.8797833183002653\tValidation Loss: 0.8811366456185561\n",
            "Epoch 62  \tTraining Loss: 0.8793359728749386\tValidation Loss: 0.8807033479480376\n",
            "Epoch 63  \tTraining Loss: 0.8789133823364135\tValidation Loss: 0.8802937381556916\n",
            "Epoch 64  \tTraining Loss: 0.8785141798915508\tValidation Loss: 0.8799065058128028\n",
            "Epoch 65  \tTraining Loss: 0.8781370710827007\tValidation Loss: 0.8795404274825914\n",
            "Epoch 66  \tTraining Loss: 0.8777808330947722\tValidation Loss: 0.8791943404570497\n",
            "Epoch 67  \tTraining Loss: 0.8774443107900305\tValidation Loss: 0.8788671457582398\n",
            "Epoch 68  \tTraining Loss: 0.8771264129623552\tValidation Loss: 0.8785578050200656\n",
            "Epoch 69  \tTraining Loss: 0.8768261087988208\tValidation Loss: 0.878265337139443\n",
            "Epoch 70  \tTraining Loss: 0.8765424245370911\tValidation Loss: 0.8779888151126521\n",
            "Epoch 71  \tTraining Loss: 0.8762744403078347\tValidation Loss: 0.8777273630466351\n",
            "Epoch 72  \tTraining Loss: 0.8760212871515632\tValidation Loss: 0.8774801533340951\n",
            "Epoch 73  \tTraining Loss: 0.8757821441998691\tValidation Loss: 0.8772464039885273\n",
            "Epoch 74  \tTraining Loss: 0.8755562360163047\tValidation Loss: 0.877025376123817\n",
            "Epoch 75  \tTraining Loss: 0.8753428300808822\tValidation Loss: 0.8768163715732666\n",
            "Epoch 76  \tTraining Loss: 0.8751412344144599\tValidation Loss: 0.8766187306406569\n",
            "Epoch 77  \tTraining Loss: 0.8749507953349023\tValidation Loss: 0.8764318299756421\n",
            "Epoch 78  \tTraining Loss: 0.8747708953374208\tValidation Loss: 0.8762550805665961\n",
            "Epoch 79  \tTraining Loss: 0.8746009510922323\tValidation Loss: 0.8760879258444138\n",
            "Epoch 80  \tTraining Loss: 0.8744404115530333\tValidation Loss: 0.8759298398911299\n",
            "Epoch 81  \tTraining Loss: 0.8742887561701636\tValidation Loss: 0.8757803257475612\n",
            "Epoch 82  \tTraining Loss: 0.8741454932026594\tValidation Loss: 0.8756389138144851\n",
            "Epoch 83  \tTraining Loss: 0.8740101581237288\tValidation Loss: 0.8755051603422598\n",
            "Epoch 84  \tTraining Loss: 0.8738823121149157\tValidation Loss: 0.875378646004783\n",
            "Epoch 85  \tTraining Loss: 0.8737615373978068\tValidation Loss: 0.8752589667165434\n",
            "Epoch 86  \tTraining Loss: 0.8736474339674868\tValidation Loss: 0.8751457556735798\n",
            "Epoch 87  \tTraining Loss: 0.8735396430035869\tValidation Loss: 0.8750386579567807\n",
            "Epoch 88  \tTraining Loss: 0.8734378126900895\tValidation Loss: 0.8749373372054307\n",
            "Epoch 89  \tTraining Loss: 0.8733416124485939\tValidation Loss: 0.8748414794316053\n",
            "Epoch 90  \tTraining Loss: 0.8732507334811991\tValidation Loss: 0.8747507857641987\n",
            "Epoch 91  \tTraining Loss: 0.8731648810012875\tValidation Loss: 0.8746649735992229\n",
            "Epoch 92  \tTraining Loss: 0.873083776419218\tValidation Loss: 0.8745837760251892\n",
            "Epoch 93  \tTraining Loss: 0.8730071566389803\tValidation Loss: 0.8745069407770735\n",
            "Epoch 94  \tTraining Loss: 0.8729347731087451\tValidation Loss: 0.8744342294277142\n",
            "Epoch 95  \tTraining Loss: 0.8728663740411461\tValidation Loss: 0.8743653880677815\n",
            "Epoch 96  \tTraining Loss: 0.8728017228620663\tValidation Loss: 0.874300125980458\n",
            "Epoch 97  \tTraining Loss: 0.8727406394694224\tValidation Loss: 0.8742383434107381\n",
            "Epoch 98  \tTraining Loss: 0.8726829252626221\tValidation Loss: 0.8741798493442678\n",
            "Epoch 99  \tTraining Loss: 0.8726283918618286\tValidation Loss: 0.874124433466712\n",
            "Epoch 100  \tTraining Loss: 0.872576809113281\tValidation Loss: 0.8740718866966265\n",
            "Epoch 101  \tTraining Loss: 0.872527945126609\tValidation Loss: 0.8740218295861287\n",
            "Epoch 102  \tTraining Loss: 0.8724813220977836\tValidation Loss: 0.8739741504330286\n",
            "Epoch 103  \tTraining Loss: 0.8724370263081974\tValidation Loss: 0.873928731920704\n",
            "Epoch 104  \tTraining Loss: 0.8723947207913\tValidation Loss: 0.8738853314464625\n",
            "Epoch 105  \tTraining Loss: 0.8723542459320842\tValidation Loss: 0.8738431511857976\n",
            "Epoch 106  \tTraining Loss: 0.8723151006776522\tValidation Loss: 0.8738010591811399\n",
            "Epoch 107  \tTraining Loss: 0.872275990979351\tValidation Loss: 0.8737541053418192\n",
            "Epoch 108  \tTraining Loss: 0.8722317818131606\tValidation Loss: 0.8736707967040981\n",
            "Epoch 109  \tTraining Loss: 0.8721630324308288\tValidation Loss: 0.8734565747003962\n",
            "Epoch 110  \tTraining Loss: 0.871967453111955\tValidation Loss: 0.8709846453900089\n",
            "Epoch 111  \tTraining Loss: 0.869248671635285\tValidation Loss: 0.931646166942967\n",
            "Epoch 112  \tTraining Loss: 0.9277306720464344\tValidation Loss: 0.8736853314115463\n",
            "Epoch 113  \tTraining Loss: 0.8721890675324174\tValidation Loss: 0.8736561404785168\n",
            "Epoch 114  \tTraining Loss: 0.8721620329529269\tValidation Loss: 0.8736285062977989\n",
            "Epoch 115  \tTraining Loss: 0.8721364946046536\tValidation Loss: 0.8736023443617326\n",
            "Epoch 116  \tTraining Loss: 0.8721123696785471\tValidation Loss: 0.8735775747934061\n",
            "Epoch 117  \tTraining Loss: 0.8720895799486313\tValidation Loss: 0.8735541220916668\n",
            "Epoch 118  \tTraining Loss: 0.8720680515183539\tValidation Loss: 0.8735319148902094\n",
            "Epoch 119  \tTraining Loss: 0.8720477145809734\tValidation Loss: 0.8735108857299595\n",
            "Epoch 120  \tTraining Loss: 0.8720285031932089\tValidation Loss: 0.8734909708440229\n",
            "Epoch 121  \tTraining Loss: 0.8720103550614167\tValidation Loss: 0.8734721099545011\n",
            "Epoch 122  \tTraining Loss: 0.8719932113396001\tValidation Loss: 0.8734542460805212\n",
            "Epoch 123  \tTraining Loss: 0.8719770164385988\tValidation Loss: 0.873437325356858\n",
            "Epoch 124  \tTraining Loss: 0.8719617178458405\tValidation Loss: 0.8734212968625625\n",
            "Epoch 125  \tTraining Loss: 0.8719472659550651\tValidation Loss: 0.8734061124590482\n",
            "Epoch 126  \tTraining Loss: 0.8719336139054756\tValidation Loss: 0.873391726637106\n",
            "Epoch 127  \tTraining Loss: 0.8719207174297907\tValidation Loss: 0.8733780963723594\n",
            "Epoch 128  \tTraining Loss: 0.8719085347107045\tValidation Loss: 0.873365180988694\n",
            "Epoch 129  \tTraining Loss: 0.8718970262452949\tValidation Loss: 0.873352942029214\n",
            "Epoch 130  \tTraining Loss: 0.871886154716931\tValidation Loss: 0.8733413431343198\n",
            "Epoch 131  \tTraining Loss: 0.871875884874276\tValidation Loss: 0.8733303499265049\n",
            "Epoch 132  \tTraining Loss: 0.8718661834169801\tValidation Loss: 0.8733199299015056\n",
            "Epoch 133  \tTraining Loss: 0.8718570188877052\tValidation Loss: 0.873310052325451\n",
            "Epoch 134  \tTraining Loss: 0.8718483615701215\tValidation Loss: 0.8733006881376816\n",
            "Epoch 135  \tTraining Loss: 0.8718401833925529\tValidation Loss: 0.8732918098589241\n",
            "Epoch 136  \tTraining Loss: 0.871832457836953\tValidation Loss: 0.873283391504528\n",
            "Epoch 137  \tTraining Loss: 0.8718251598529173\tValidation Loss: 0.8732754085024802\n",
            "Epoch 138  \tTraining Loss: 0.8718182657764604\tValidation Loss: 0.8732678376159378\n",
            "Epoch 139  \tTraining Loss: 0.871811753253281\tValidation Loss: 0.8732606568700286\n",
            "Epoch 140  \tTraining Loss: 0.8718056011662786\tValidation Loss: 0.8732538454826831\n",
            "Epoch 141  \tTraining Loss: 0.871799789567081\tValidation Loss: 0.8732473837992747\n",
            "Epoch 142  \tTraining Loss: 0.8717942996113601\tValidation Loss: 0.8732412532308601\n",
            "Epoch 143  \tTraining Loss: 0.8717891134977293\tValidation Loss: 0.8732354361958193\n",
            "Epoch 144  \tTraining Loss: 0.8717842144100205\tValidation Loss: 0.8732299160647083\n",
            "Epoch 145  \tTraining Loss: 0.8717795864627589\tValidation Loss: 0.8732246771081463\n",
            "Epoch 146  \tTraining Loss: 0.8717752146496518\tValidation Loss: 0.8732197044475718\n",
            "Epoch 147  \tTraining Loss: 0.8717710847949315\tValidation Loss: 0.8732149840087072\n",
            "Epoch 148  \tTraining Loss: 0.87176718350739\tValidation Loss: 0.873210502477584\n",
            "Epoch 149  \tTraining Loss: 0.871763498136956\tValidation Loss: 0.873206247258986\n",
            "Epoch 150  \tTraining Loss: 0.8717600167336776\tValidation Loss: 0.8732022064371786\n",
            "Epoch 151  \tTraining Loss: 0.871756728008975\tValidation Loss: 0.8731983687387965\n",
            "Epoch 152  \tTraining Loss: 0.8717536212990348\tValidation Loss: 0.8731947234977723\n",
            "Epoch 153  \tTraining Loss: 0.8717506865302335\tValidation Loss: 0.8731912606221915\n",
            "Epoch 154  \tTraining Loss: 0.871747914186474\tValidation Loss: 0.8731879705629706\n",
            "Epoch 155  \tTraining Loss: 0.8717452952783277\tValidation Loss: 0.8731848442842535\n",
            "Epoch 156  \tTraining Loss: 0.8717428213138865\tValidation Loss: 0.873181873235437\n",
            "Epoch 157  \tTraining Loss: 0.8717404842712282\tValidation Loss: 0.8731790493247297\n",
            "Epoch 158  \tTraining Loss: 0.8717382765724049\tValidation Loss: 0.8731763648941641\n",
            "Epoch 159  \tTraining Loss: 0.87173619105887\tValidation Loss: 0.8731738126959799\n",
            "Epoch 160  \tTraining Loss: 0.8717342209682681\tValidation Loss: 0.8731713858703041\n",
            "Epoch 161  \tTraining Loss: 0.8717323599125066\tValidation Loss: 0.8731690779240538\n",
            "Epoch 162  \tTraining Loss: 0.8717306018570424\tValidation Loss: 0.8731668827109985\n",
            "Epoch 163  \tTraining Loss: 0.8717289411013149\tValidation Loss: 0.8731647944129138\n",
            "Epoch 164  \tTraining Loss: 0.8717273722602615\tValidation Loss: 0.8731628075217694\n",
            "Epoch 165  \tTraining Loss: 0.8717258902468569\tValidation Loss: 0.8731609168228933\n",
            "Epoch 166  \tTraining Loss: 0.8717244902556158\tValidation Loss: 0.873159117379057\n",
            "Epoch 167  \tTraining Loss: 0.8717231677470153\tValidation Loss: 0.8731574045154356\n",
            "Epoch 168  \tTraining Loss: 0.8717219184327715\tValidation Loss: 0.8731557738053877\n",
            "Epoch 169  \tTraining Loss: 0.8717207382619361\tValidation Loss: 0.8731542210570169\n",
            "Epoch 170  \tTraining Loss: 0.8717196234077613\tValidation Loss: 0.8731527423004662\n",
            "Epoch 171  \tTraining Loss: 0.8717185702552901\tValidation Loss: 0.8731513337759101\n",
            "Epoch 172  \tTraining Loss: 0.8717175753896365\tValidation Loss: 0.8731499919222014\n",
            "Epoch 173  \tTraining Loss: 0.8717166355849114\tValidation Loss: 0.8731487133661391\n",
            "Epoch 174  \tTraining Loss: 0.8717157477937628\tValidation Loss: 0.8731474949123242\n",
            "Epoch 175  \tTraining Loss: 0.8717149091374948\tValidation Loss: 0.8731463335335677\n",
            "Epoch 176  \tTraining Loss: 0.8717141168967326\tValidation Loss: 0.8731452263618238\n",
            "Epoch 177  \tTraining Loss: 0.8717133685026064\tValidation Loss: 0.8731441706796179\n",
            "Epoch 178  \tTraining Loss: 0.8717126615284199\tValidation Loss: 0.8731431639119426\n",
            "Epoch 179  \tTraining Loss: 0.8717119936817826\tValidation Loss: 0.8731422036185945\n",
            "Epoch 180  \tTraining Loss: 0.8717113627971778\tValidation Loss: 0.873141287486931\n",
            "Epoch 181  \tTraining Loss: 0.8717107668289372\tValidation Loss: 0.8731404133250196\n",
            "Epoch 182  \tTraining Loss: 0.8717102038446123\tValidation Loss: 0.8731395790551627\n",
            "Epoch 183  \tTraining Loss: 0.8717096720187051\tValidation Loss: 0.8731387827077728\n",
            "Epoch 184  \tTraining Loss: 0.8717091696267504\tValidation Loss: 0.8731380224155831\n",
            "Epoch 185  \tTraining Loss: 0.8717086950397234\tValidation Loss: 0.8731372964081721\n",
            "Epoch 186  \tTraining Loss: 0.8717082467187576\tValidation Loss: 0.8731366030067854\n",
            "Epoch 187  \tTraining Loss: 0.8717078232101557\tValidation Loss: 0.8731359406194408\n",
            "Epoch 188  \tTraining Loss: 0.8717074231406752\tValidation Loss: 0.8731353077362971\n",
            "Epoch 189  \tTraining Loss: 0.871707045213076\tValidation Loss: 0.873134702925276\n",
            "Epoch 190  \tTraining Loss: 0.8717066882019133\tValidation Loss: 0.8731341248279211\n",
            "Epoch 191  \tTraining Loss: 0.8717063509495657\tValidation Loss: 0.8731335721554813\n",
            "Epoch 192  \tTraining Loss: 0.871706032362479\tValidation Loss: 0.8731330436852068\n",
            "Epoch 193  \tTraining Loss: 0.8717057314076232\tValidation Loss: 0.873132538256846\n",
            "Epoch 194  \tTraining Loss: 0.871705447109141\tValidation Loss: 0.8731320547693309\n",
            "Epoch 195  \tTraining Loss: 0.8717051785451834\tValidation Loss: 0.873131592177644\n",
            "Epoch 196  \tTraining Loss: 0.8717049248449221\tValidation Loss: 0.8731311494898525\n",
            "Epoch 197  \tTraining Loss: 0.8717046851857245\tValidation Loss: 0.8731307257643035\n",
            "Epoch 198  \tTraining Loss: 0.8717044587904865\tValidation Loss: 0.873130320106972\n",
            "Epoch 199  \tTraining Loss: 0.8717042449251133\tValidation Loss: 0.8731299316689497\n",
            "Epoch 200  \tTraining Loss: 0.8717040428961382\tValidation Loss: 0.873129559644071\n",
            "Epoch 201  \tTraining Loss: 0.8717038520484748\tValidation Loss: 0.8731292032666667\n",
            "Epoch 202  \tTraining Loss: 0.8717036717632924\tValidation Loss: 0.8731288618094377\n",
            "Epoch 203  \tTraining Loss: 0.8717035014560097\tValidation Loss: 0.8731285345814443\n",
            "Epoch 204  \tTraining Loss: 0.8717033405743984\tValidation Loss: 0.8731282209262036\n",
            "Epoch 205  \tTraining Loss: 0.8717031885967944\tValidation Loss: 0.8731279202198867\n",
            "Epoch 206  \tTraining Loss: 0.8717030450304045\tValidation Loss: 0.8731276318696177\n",
            "Epoch 207  \tTraining Loss: 0.8717029094097093\tValidation Loss: 0.8731273553118585\n",
            "Epoch 208  \tTraining Loss: 0.8717027812949537\tValidation Loss: 0.8731270900108836\n",
            "Epoch 209  \tTraining Loss: 0.8717026602707204\tValidation Loss: 0.8731268354573353\n",
            "Epoch 210  \tTraining Loss: 0.8717025459445844\tValidation Loss: 0.8731265911668568\n",
            "Epoch 211  \tTraining Loss: 0.8717024379458386\tValidation Loss: 0.8731263566787975\n",
            "Epoch 212  \tTraining Loss: 0.8717023359242928\tValidation Loss: 0.8731261315549889\n",
            "Epoch 213  \tTraining Loss: 0.8717022395491386\tValidation Loss: 0.8731259153785838\n",
            "Epoch 214  \tTraining Loss: 0.8717021485078762\tValidation Loss: 0.8731257077529591\n",
            "Epoch 215  \tTraining Loss: 0.8717020625053005\tValidation Loss: 0.8731255083006767\n",
            "Epoch 216  \tTraining Loss: 0.8717019812625448\tValidation Loss: 0.8731253166624984\n",
            "Epoch 217  \tTraining Loss: 0.8717019045161768\tValidation Loss: 0.873125132496455\n",
            "Epoch 218  \tTraining Loss: 0.8717018320173433\tValidation Loss: 0.8731249554769633\n",
            "Epoch 219  \tTraining Loss: 0.8717017635309646\tValidation Loss: 0.8731247852939907\n",
            "Epoch 220  \tTraining Loss: 0.8717016988349703\tValidation Loss: 0.8731246216522641\n",
            "Epoch 221  \tTraining Loss: 0.8717016377195815\tValidation Loss: 0.87312446427052\n",
            "Epoch 222  \tTraining Loss: 0.8717015799866293\tValidation Loss: 0.8731243128807948\n",
            "Epoch 223  \tTraining Loss: 0.871701525448913\tValidation Loss: 0.8731241672277521\n",
            "Epoch 224  \tTraining Loss: 0.8717014739295911\tValidation Loss: 0.8731240270680464\n",
            "Epoch 225  \tTraining Loss: 0.871701425261611\tValidation Loss: 0.8731238921697181\n",
            "Epoch 226  \tTraining Loss: 0.8717013792871641\tValidation Loss: 0.8731237623116235\n",
            "Epoch 227  \tTraining Loss: 0.8717013358571775\tValidation Loss: 0.8731236372828919\n",
            "Epoch 228  \tTraining Loss: 0.8717012948308271\tValidation Loss: 0.8731235168824122\n",
            "Epoch 229  \tTraining Loss: 0.8717012560750839\tValidation Loss: 0.8731234009183468\n",
            "Epoch 230  \tTraining Loss: 0.8717012194642807\tValidation Loss: 0.8731232892076706\n",
            "Epoch 231  \tTraining Loss: 0.8717011848797057\tValidation Loss: 0.8731231815757332\n",
            "Epoch 232  \tTraining Loss: 0.8717011522092171\tValidation Loss: 0.8731230778558453\n",
            "Epoch 233  \tTraining Loss: 0.8717011213468795\tValidation Loss: 0.873122977888886\n",
            "Epoch 234  \tTraining Loss: 0.8717010921926209\tValidation Loss: 0.8731228815229299\n",
            "Epoch 235  \tTraining Loss: 0.8717010646519074\tValidation Loss: 0.873122788612894\n",
            "Epoch 236  \tTraining Loss: 0.8717010386354369\tValidation Loss: 0.8731226990202041\n",
            "Epoch 237  \tTraining Loss: 0.8717010140588504\tValidation Loss: 0.8731226126124759\n",
            "Epoch 238  \tTraining Loss: 0.8717009908424574\tValidation Loss: 0.8731225292632148\n",
            "Epoch 239  \tTraining Loss: 0.8717009689109776\tValidation Loss: 0.8731224488515298\n",
            "Epoch 240  \tTraining Loss: 0.8717009481932976\tValidation Loss: 0.8731223712618629\n",
            "Epoch 241  \tTraining Loss: 0.8717009286222395\tValidation Loss: 0.8731222963837316\n",
            "Epoch 242  \tTraining Loss: 0.8717009101343433\tValidation Loss: 0.8731222241114859\n",
            "Epoch 243  \tTraining Loss: 0.8717008926696612\tValidation Loss: 0.8731221543440759\n",
            "Epoch 244  \tTraining Loss: 0.8717008761715636\tValidation Loss: 0.8731220869848328\n",
            "Epoch 245  \tTraining Loss: 0.8717008605865547\tValidation Loss: 0.8731220219412601\n",
            "Epoch 246  \tTraining Loss: 0.8717008458640997\tValidation Loss: 0.873121959124836\n",
            "Epoch 247  \tTraining Loss: 0.87170083195646\tValidation Loss: 0.8731218984508252\n",
            "Epoch 248  \tTraining Loss: 0.8717008188185399\tValidation Loss: 0.8731218398381011\n",
            "Epoch 249  \tTraining Loss: 0.8717008064077393\tValidation Loss: 0.8731217832089756\n",
            "Epoch 250  \tTraining Loss: 0.8717007946838152\tValidation Loss: 0.8731217284890395\n",
            "Epoch 251  \tTraining Loss: 0.8717007836087532\tValidation Loss: 0.8731216756070085\n",
            "Epoch 252  \tTraining Loss: 0.8717007731466413\tValidation Loss: 0.8731216244945786\n",
            "Epoch 253  \tTraining Loss: 0.8717007632635558\tValidation Loss: 0.8731215750862884\n",
            "Epoch 254  \tTraining Loss: 0.8717007539274508\tValidation Loss: 0.8731215273193882\n",
            "Epoch 255  \tTraining Loss: 0.871700745108053\tValidation Loss: 0.8731214811337146\n",
            "Epoch 256  \tTraining Loss: 0.871700736776766\tValidation Loss: 0.8731214364715735\n",
            "Epoch 257  \tTraining Loss: 0.8717007289065747\tValidation Loss: 0.8731213932776267\n",
            "Epoch 258  \tTraining Loss: 0.8717007214719598\tValidation Loss: 0.8731213514987852\n",
            "Epoch 259  \tTraining Loss: 0.8717007144488143\tValidation Loss: 0.873121311084108\n",
            "Epoch 260  \tTraining Loss: 0.8717007078143653\tValidation Loss: 0.8731212719847042\n",
            "Epoch 261  \tTraining Loss: 0.8717007015471006\tValidation Loss: 0.8731212341536431\n",
            "Epoch 262  \tTraining Loss: 0.8717006956266983\tValidation Loss: 0.8731211975458647\n",
            "Epoch 263  \tTraining Loss: 0.8717006900339611\tValidation Loss: 0.8731211621180976\n",
            "Epoch 264  \tTraining Loss: 0.8717006847507544\tValidation Loss: 0.8731211278287794\n",
            "Epoch 265  \tTraining Loss: 0.8717006797599473\tValidation Loss: 0.8731210946379819\n",
            "Epoch 266  \tTraining Loss: 0.8717006750453568\tValidation Loss: 0.8731210625073386\n",
            "Epoch 267  \tTraining Loss: 0.8717006705916959\tValidation Loss: 0.8731210313999771\n",
            "Epoch 268  \tTraining Loss: 0.8717006663845233\tValidation Loss: 0.8731210012804534\n",
            "Epoch 269  \tTraining Loss: 0.8717006624101967\tValidation Loss: 0.8731209721146911\n",
            "Epoch 270  \tTraining Loss: 0.8717006586558296\tValidation Loss: 0.8731209438699209\n",
            "Epoch 271  \tTraining Loss: 0.8717006551092487\tValidation Loss: 0.873120916514626\n",
            "Epoch 272  \tTraining Loss: 0.8717006517589534\tValidation Loss: 0.8731208900184877\n",
            "Epoch 273  \tTraining Loss: 0.8717006485940803\tValidation Loss: 0.873120864352334\n",
            "Epoch 274  \tTraining Loss: 0.8717006456043674\tValidation Loss: 0.8731208394880929\n",
            "Epoch 275  \tTraining Loss: 0.8717006427801207\tValidation Loss: 0.8731208153987432\n",
            "Epoch 276  \tTraining Loss: 0.871700640112182\tValidation Loss: 0.8731207920582729\n",
            "Epoch 277  \tTraining Loss: 0.8717006375919009\tValidation Loss: 0.8731207694416355\n",
            "Epoch 278  \tTraining Loss: 0.8717006352111046\tValidation Loss: 0.8731207475247109\n",
            "Epoch 279  \tTraining Loss: 0.8717006329620738\tValidation Loss: 0.8731207262842661\n",
            "Epoch 280  \tTraining Loss: 0.8717006308375159\tValidation Loss: 0.8731207056979197\n",
            "Epoch 281  \tTraining Loss: 0.8717006288305418\tValidation Loss: 0.8731206857441053\n",
            "Epoch 282  \tTraining Loss: 0.8717006269346437\tValidation Loss: 0.8731206664020407\n",
            "Epoch 283  \tTraining Loss: 0.8717006251436744\tValidation Loss: 0.8731206476516934\n",
            "Epoch 284  \tTraining Loss: 0.8717006234518266\tValidation Loss: 0.8731206294737525\n",
            "Epoch 285  \tTraining Loss: 0.8717006218536141\tValidation Loss: 0.8731206118495981\n",
            "Epoch 286  \tTraining Loss: 0.8717006203438548\tValidation Loss: 0.8731205947612747\n",
            "Epoch 287  \tTraining Loss: 0.8717006189176533\tValidation Loss: 0.8731205781914639\n",
            "Epoch 288  \tTraining Loss: 0.871700617570385\tValidation Loss: 0.87312056212346\n",
            "Epoch 289  \tTraining Loss: 0.8717006162976814\tValidation Loss: 0.8731205465411449\n",
            "Epoch 290  \tTraining Loss: 0.8717006150954159\tValidation Loss: 0.8731205314289655\n",
            "Epoch 291  \tTraining Loss: 0.8717006139596898\tValidation Loss: 0.8731205167719117\n",
            "Epoch 292  \tTraining Loss: 0.8717006128868202\tValidation Loss: 0.8731205025554954\n",
            "Epoch 293  \tTraining Loss: 0.8717006118733291\tValidation Loss: 0.8731204887657295\n",
            "Epoch 294  \tTraining Loss: 0.8717006109159297\tValidation Loss: 0.8731204753891092\n",
            "Epoch 295  \tTraining Loss: 0.8717006100115172\tValidation Loss: 0.8731204624125938\n",
            "Epoch 296  \tTraining Loss: 0.8717006091571601\tValidation Loss: 0.8731204498235884\n",
            "Epoch 297  \tTraining Loss: 0.8717006083500872\tValidation Loss: 0.873120437609927\n",
            "Epoch 298  \tTraining Loss: 0.871700607587682\tValidation Loss: 0.8731204257598565\n",
            "Epoch 299  \tTraining Loss: 0.8717006068674718\tValidation Loss: 0.8731204142620215\n",
            "Epoch 300  \tTraining Loss: 0.871700606187122\tValidation Loss: 0.873120403105448\n",
            "Epoch 301  \tTraining Loss: 0.8717006055444261\tValidation Loss: 0.8731203922795316\n",
            "Epoch 302  \tTraining Loss: 0.8717006049373004\tValidation Loss: 0.8731203817740215\n",
            "Epoch 303  \tTraining Loss: 0.871700604363776\tValidation Loss: 0.8731203715790081\n",
            "Epoch 304  \tTraining Loss: 0.8717006038219932\tValidation Loss: 0.8731203616849114\n",
            "Epoch 305  \tTraining Loss: 0.8717006033101957\tValidation Loss: 0.8731203520824674\n",
            "Epoch 306  \tTraining Loss: 0.8717006028267237\tValidation Loss: 0.8731203427627179\n",
            "Epoch 307  \tTraining Loss: 0.8717006023700093\tValidation Loss: 0.873120333716999\n",
            "Epoch 308  \tTraining Loss: 0.8717006019385719\tValidation Loss: 0.8731203249369299\n",
            "Epoch 309  \tTraining Loss: 0.8717006015310124\tValidation Loss: 0.8731203164144036\n",
            "Epoch 310  \tTraining Loss: 0.8717006011460092\tValidation Loss: 0.8731203081415763\n",
            "Epoch 311  \tTraining Loss: 0.8717006007823143\tValidation Loss: 0.8731203001108585\n",
            "Epoch 312  \tTraining Loss: 0.8717006004387482\tValidation Loss: 0.8731202923149065\n",
            "Epoch 313  \tTraining Loss: 0.8717006001141966\tValidation Loss: 0.8731202847466121\n",
            "Epoch 314  \tTraining Loss: 0.8717005998076073\tValidation Loss: 0.8731202773990955\n",
            "Epoch 315  \tTraining Loss: 0.8717005995179861\tValidation Loss: 0.873120270265698\n",
            "Epoch 316  \tTraining Loss: 0.8717005992443941\tValidation Loss: 0.8731202633399726\n",
            "Epoch 317  \tTraining Loss: 0.8717005989859443\tValidation Loss: 0.8731202566156775\n",
            "Epoch 318  \tTraining Loss: 0.8717005987417983\tValidation Loss: 0.8731202500867692\n",
            "Epoch 319  \tTraining Loss: 0.8717005985111645\tValidation Loss: 0.8731202437473955\n",
            "Epoch 320  \tTraining Loss: 0.8717005982932953\tValidation Loss: 0.8731202375918882\n",
            "Epoch 321  \tTraining Loss: 0.8717005980874838\tValidation Loss: 0.8731202316147579\n",
            "Epoch 322  \tTraining Loss: 0.8717005978930632\tValidation Loss: 0.8731202258106883\n",
            "Epoch 323  \tTraining Loss: 0.8717005977094029\tValidation Loss: 0.8731202201745276\n",
            "Epoch 324  \tTraining Loss: 0.8717005975359069\tValidation Loss: 0.8731202147012868\n",
            "Epoch 325  \tTraining Loss: 0.8717005973720132\tValidation Loss: 0.8731202093861307\n",
            "Epoch 326  \tTraining Loss: 0.8717005972171904\tValidation Loss: 0.8731202042243753\n",
            "Epoch 327  \tTraining Loss: 0.8717005970709365\tValidation Loss: 0.8731201992114817\n",
            "Epoch 328  \tTraining Loss: 0.8717005969327767\tValidation Loss: 0.8731201943430505\n",
            "Epoch 329  \tTraining Loss: 0.8717005968022634\tValidation Loss: 0.8731201896148191\n",
            "Epoch 330  \tTraining Loss: 0.8717005966789731\tValidation Loss: 0.8731201850226553\n",
            "Epoch 331  \tTraining Loss: 0.8717005965625066\tValidation Loss: 0.8731201805625539\n",
            "Epoch 332  \tTraining Loss: 0.8717005964524861\tValidation Loss: 0.8731201762306332\n",
            "Epoch 333  \tTraining Loss: 0.8717005963485545\tValidation Loss: 0.8731201720231286\n",
            "Epoch 334  \tTraining Loss: 0.8717005962503752\tValidation Loss: 0.8731201679363919\n",
            "Epoch 335  \tTraining Loss: 0.8717005961576293\tValidation Loss: 0.8731201639668853\n",
            "Epoch 336  \tTraining Loss: 0.8717005960700167\tValidation Loss: 0.8731201601111784\n",
            "Epoch 337  \tTraining Loss: 0.8717005959872528\tValidation Loss: 0.8731201563659449\n",
            "Epoch 338  \tTraining Loss: 0.8717005959090697\tValidation Loss: 0.8731201527279595\n",
            "Epoch 339  \tTraining Loss: 0.8717005958352135\tValidation Loss: 0.873120149194094\n",
            "Epoch 340  \tTraining Loss: 0.8717005957654449\tValidation Loss: 0.8731201457613145\n",
            "Epoch 341  \tTraining Loss: 0.871700595699538\tValidation Loss: 0.8731201424266787\n",
            "Epoch 342  \tTraining Loss: 0.871700595637278\tValidation Loss: 0.8731201391873324\n",
            "Epoch 343  \tTraining Loss: 0.8717005955784644\tValidation Loss: 0.8731201360405074\n",
            "Epoch 344  \tTraining Loss: 0.8717005955229056\tValidation Loss: 0.8731201329835181\n",
            "Epoch 345  \tTraining Loss: 0.8717005954704218\tValidation Loss: 0.8731201300137589\n",
            "Epoch 346  \tTraining Loss: 0.8717005954208427\tValidation Loss: 0.8731201271287029\n",
            "Epoch 347  \tTraining Loss: 0.8717005953740077\tValidation Loss: 0.8731201243258977\n",
            "Epoch 348  \tTraining Loss: 0.8717005953297644\tValidation Loss: 0.8731201216029638\n",
            "Epoch 349  \tTraining Loss: 0.8717005952879701\tValidation Loss: 0.873120118957593\n",
            "Epoch 350  \tTraining Loss: 0.8717005952484886\tValidation Loss: 0.8731201163875454\n",
            "Epoch 351  \tTraining Loss: 0.8717005952111926\tValidation Loss: 0.8731201138906473\n",
            "Epoch 352  \tTraining Loss: 0.8717005951759604\tValidation Loss: 0.873120111464789\n",
            "Epoch 353  \tTraining Loss: 0.8717005951426784\tValidation Loss: 0.8731201091079238\n",
            "Epoch 354  \tTraining Loss: 0.8717005951112382\tValidation Loss: 0.873120106818065\n",
            "Epoch 355  \tTraining Loss: 0.871700595081538\tValidation Loss: 0.8731201045932845\n",
            "Epoch 356  \tTraining Loss: 0.8717005950534819\tValidation Loss: 0.8731201024317108\n",
            "Epoch 357  \tTraining Loss: 0.8717005950269782\tValidation Loss: 0.8731201003315278\n",
            "Epoch 358  \tTraining Loss: 0.8717005950019414\tValidation Loss: 0.8731200982909724\n",
            "Epoch 359  \tTraining Loss: 0.8717005949782906\tValidation Loss: 0.8731200963083331\n",
            "Epoch 360  \tTraining Loss: 0.8717005949559483\tValidation Loss: 0.8731200943819489\n",
            "Epoch 361  \tTraining Loss: 0.8717005949348428\tValidation Loss: 0.8731200925102071\n",
            "Epoch 362  \tTraining Loss: 0.8717005949149053\tValidation Loss: 0.8731200906915423\n",
            "Epoch 363  \tTraining Loss: 0.8717005948960713\tValidation Loss: 0.8731200889244346\n",
            "Epoch 364  \tTraining Loss: 0.8717005948782797\tValidation Loss: 0.8731200872074085\n",
            "Epoch 365  \tTraining Loss: 0.8717005948614727\tValidation Loss: 0.8731200855390318\n",
            "Epoch 366  \tTraining Loss: 0.8717005948455959\tValidation Loss: 0.8731200839179133\n",
            "Epoch 367  \tTraining Loss: 0.8717005948305977\tValidation Loss: 0.8731200823427031\n",
            "Epoch 368  \tTraining Loss: 0.8717005948164298\tValidation Loss: 0.8731200808120898\n",
            "Epoch 369  \tTraining Loss: 0.871700594803046\tValidation Loss: 0.8731200793248004\n",
            "Epoch 370  \tTraining Loss: 0.8717005947904027\tValidation Loss: 0.8731200778795988\n",
            "Epoch 371  \tTraining Loss: 0.8717005947784593\tValidation Loss: 0.8731200764752847\n",
            "Epoch 372  \tTraining Loss: 0.8717005947671771\tValidation Loss: 0.8731200751106923\n",
            "Epoch 373  \tTraining Loss: 0.8717005947565192\tValidation Loss: 0.8731200737846898\n",
            "Epoch 374  \tTraining Loss: 0.8717005947464509\tValidation Loss: 0.8731200724961781\n",
            "Epoch 375  \tTraining Loss: 0.87170059473694\tValidation Loss: 0.8731200712440896\n",
            "Epoch 376  \tTraining Loss: 0.8717005947279555\tValidation Loss: 0.8731200700273882\n",
            "Epoch 377  \tTraining Loss: 0.8717005947194681\tValidation Loss: 0.8731200688450665\n",
            "Epoch 378  \tTraining Loss: 0.8717005947114507\tValidation Loss: 0.873120067696147\n",
            "Epoch 379  \tTraining Loss: 0.8717005947038768\tValidation Loss: 0.8731200665796804\n",
            "Epoch 380  \tTraining Loss: 0.8717005946967223\tValidation Loss: 0.873120065494744\n",
            "Epoch 381  \tTraining Loss: 0.8717005946899635\tValidation Loss: 0.8731200644404421\n",
            "Epoch 382  \tTraining Loss: 0.871700594683579\tValidation Loss: 0.873120063415905\n",
            "Epoch 383  \tTraining Loss: 0.8717005946775478\tValidation Loss: 0.8731200624202873\n",
            "Epoch 384  \tTraining Loss: 0.8717005946718503\tValidation Loss: 0.8731200614527688\n",
            "Epoch 385  \tTraining Loss: 0.8717005946664681\tValidation Loss: 0.8731200605125514\n",
            "Epoch 386  \tTraining Loss: 0.8717005946613838\tValidation Loss: 0.8731200595988617\n",
            "Epoch 387  \tTraining Loss: 0.8717005946565812\tValidation Loss: 0.8731200587109467\n",
            "Epoch 388  \tTraining Loss: 0.8717005946520442\tValidation Loss: 0.8731200578480762\n",
            "Epoch 389  \tTraining Loss: 0.8717005946477583\tValidation Loss: 0.8731200570095401\n",
            "Epoch 390  \tTraining Loss: 0.8717005946437093\tValidation Loss: 0.8731200561946493\n",
            "Epoch 391  \tTraining Loss: 0.8717005946398847\tValidation Loss: 0.8731200554027339\n",
            "Epoch 392  \tTraining Loss: 0.8717005946362715\tValidation Loss: 0.8731200546331431\n",
            "Epoch 393  \tTraining Loss: 0.8717005946328586\tValidation Loss: 0.8731200538852452\n",
            "Epoch 394  \tTraining Loss: 0.8717005946296347\tValidation Loss: 0.8731200531584259\n",
            "Epoch 395  \tTraining Loss: 0.8717005946265889\tValidation Loss: 0.8731200524520891\n",
            "Epoch 396  \tTraining Loss: 0.8717005946237119\tValidation Loss: 0.8731200517656551\n",
            "Epoch 397  \tTraining Loss: 0.8717005946209939\tValidation Loss: 0.8731200510985608\n",
            "Epoch 398  \tTraining Loss: 0.8717005946184264\tValidation Loss: 0.8731200504502595\n",
            "Epoch 399  \tTraining Loss: 0.8717005946160011\tValidation Loss: 0.8731200498202201\n",
            "Epoch 400  \tTraining Loss: 0.8717005946137097\tValidation Loss: 0.8731200492079257\n",
            "lr, batch_size: (0.01, 400)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 914.3434050937504\n",
            "Epoch 2  \tTraining Loss: 914.8765254955363\tValidation Loss: 0.9401842090383703\n",
            "Epoch 3  \tTraining Loss: 0.9370181172349683\tValidation Loss: 0.938417920087931\n",
            "Epoch 4  \tTraining Loss: 0.9352944862056255\tValidation Loss: 0.936747075070756\n",
            "Epoch 5  \tTraining Loss: 0.933665246573885\tValidation Loss: 0.9351675675169631\n",
            "Epoch 6  \tTraining Loss: 0.9321261771003404\tValidation Loss: 0.9336743741716047\n",
            "Epoch 7  \tTraining Loss: 0.9306722873049843\tValidation Loss: 0.932262748898413\n",
            "Epoch 8  \tTraining Loss: 0.9292988629063693\tValidation Loss: 0.930928207367703\n",
            "Epoch 9  \tTraining Loss: 0.9280014505353674\tValidation Loss: 0.9296665125910237\n",
            "Epoch 10  \tTraining Loss: 0.9267758432949464\tValidation Loss: 0.9284736612557136\n",
            "Epoch 11  \tTraining Loss: 0.9256180671191448\tValidation Loss: 0.9273458708151057\n",
            "Epoch 12  \tTraining Loss: 0.9245243678870094\tValidation Loss: 0.9262795672925838\n",
            "Epoch 13  \tTraining Loss: 0.9234911992497139\tValidation Loss: 0.9252713737599956\n",
            "Epoch 14  \tTraining Loss: 0.9225152111313887\tValidation Loss: 0.9243180994531209\n",
            "Epoch 15  \tTraining Loss: 0.9215932388663703\tValidation Loss: 0.9234167294889514\n",
            "Epoch 16  \tTraining Loss: 0.9207222929376548\tValidation Loss: 0.9225644151514933\n",
            "Epoch 17  \tTraining Loss: 0.9198995492832761\tValidation Loss: 0.9217584647146432\n",
            "Epoch 18  \tTraining Loss: 0.9191223401391766\tValidation Loss: 0.920996334772427\n",
            "Epoch 19  \tTraining Loss: 0.9183881453888855\tValidation Loss: 0.9202756220485412\n",
            "Epoch 20  \tTraining Loss: 0.917694584391944\tValidation Loss: 0.9195940556586788\n",
            "Epoch 21  \tTraining Loss: 0.9170394082645927\tValidation Loss: 0.918949489800599\n",
            "Epoch 22  \tTraining Loss: 0.9164204925876811\tValidation Loss: 0.9183398968482793\n",
            "Epoch 23  \tTraining Loss: 0.9158358305181611\tValidation Loss: 0.9177633608278012\n",
            "Epoch 24  \tTraining Loss: 0.9152835262821043\tValidation Loss: 0.917218071253747\n",
            "Epoch 25  \tTraining Loss: 0.9147617890290651\tValidation Loss: 0.9167023173061593\n",
            "Epoch 26  \tTraining Loss: 0.9142689270216375\tValidation Loss: 0.9162144823298196\n",
            "Epoch 27  \tTraining Loss: 0.913803342150826\tValidation Loss: 0.9157530386373933\n",
            "Epoch 28  \tTraining Loss: 0.9133635247550738\tValidation Loss: 0.9153165425996779\n",
            "Epoch 29  \tTraining Loss: 0.9129480487251275\tValidation Loss: 0.9149036300071866\n",
            "Epoch 30  \tTraining Loss: 0.9125555668798249\tValidation Loss: 0.9145130116880591\n",
            "Epoch 31  \tTraining Loss: 0.9121848065978083\tValidation Loss: 0.914143469368121\n",
            "Epoch 32  \tTraining Loss: 0.9118345656910037\tValidation Loss: 0.9137933444379073\n",
            "Epoch 33  \tTraining Loss: 0.911503201974437\tValidation Loss: 0.911566019941909\n",
            "Epoch 34  \tTraining Loss: 0.9093003221542041\tValidation Loss: 0.909461300535763\n",
            "Epoch 35  \tTraining Loss: 0.9072193610344994\tValidation Loss: 0.9074724193138101\n",
            "Epoch 36  \tTraining Loss: 0.9052535710026187\tValidation Loss: 0.9055929833596824\n",
            "Epoch 37  \tTraining Loss: 0.9033965778934165\tValidation Loss: 0.9038169530633082\n",
            "Epoch 38  \tTraining Loss: 0.9016423603208193\tValidation Loss: 0.9021386225815272\n",
            "Epoch 39  \tTraining Loss: 0.8999852301532403\tValidation Loss: 0.9005526013797065\n",
            "Epoch 40  \tTraining Loss: 0.8984198140695785\tValidation Loss: 0.8990537967945418\n",
            "Epoch 41  \tTraining Loss: 0.8969410361360065\tValidation Loss: 0.8976373975615347\n",
            "Epoch 42  \tTraining Loss: 0.8955441013470419\tValidation Loss: 0.8962988582537708\n",
            "Epoch 43  \tTraining Loss: 0.8942244800775405\tValidation Loss: 0.8950338845815689\n",
            "Epoch 44  \tTraining Loss: 0.8929778933951952\tValidation Loss: 0.8938384195053729\n",
            "Epoch 45  \tTraining Loss: 0.8918002991859131\tValidation Loss: 0.8927086301168793\n",
            "Epoch 46  \tTraining Loss: 0.890687879047086\tValidation Loss: 0.8916408952459005\n",
            "Epoch 47  \tTraining Loss: 0.8896370259062526\tValidation Loss: 0.8906317937528014\n",
            "Epoch 48  \tTraining Loss: 0.888644331381172\tValidation Loss: 0.889678088680907\n",
            "Epoch 49  \tTraining Loss: 0.8877065731272301\tValidation Loss: 0.8887767313528164\n",
            "Epoch 50  \tTraining Loss: 0.8868207152704439\tValidation Loss: 0.8879248367355169\n",
            "Epoch 51  \tTraining Loss: 0.885983885353116\tValidation Loss: 0.8871196791267131\n",
            "Epoch 52  \tTraining Loss: 0.8851933698948844\tValidation Loss: 0.8863586833462141\n",
            "Epoch 53  \tTraining Loss: 0.8844466055940425\tValidation Loss: 0.885639416414571\n",
            "Epoch 54  \tTraining Loss: 0.883741171015827\tValidation Loss: 0.8849595796919971\n",
            "Epoch 55  \tTraining Loss: 0.8830747787407209\tValidation Loss: 0.8843170014521096\n",
            "Epoch 56  \tTraining Loss: 0.8824452679473164\tValidation Loss: 0.8837096298664311\n",
            "Epoch 57  \tTraining Loss: 0.8818505974056793\tValidation Loss: 0.8831355263769239\n",
            "Epoch 58  \tTraining Loss: 0.8812888388585035\tValidation Loss: 0.8825928594350911\n",
            "Epoch 59  \tTraining Loss: 0.880758170768586\tValidation Loss: 0.8820798985873566\n",
            "Epoch 60  \tTraining Loss: 0.8802568724123494\tValidation Loss: 0.8815950088875736\n",
            "Epoch 61  \tTraining Loss: 0.8797833183002653\tValidation Loss: 0.8811366456185561\n",
            "Epoch 62  \tTraining Loss: 0.8793359728749386\tValidation Loss: 0.8807033479480376\n",
            "Epoch 63  \tTraining Loss: 0.8789133823364135\tValidation Loss: 0.8802937381556916\n",
            "Epoch 64  \tTraining Loss: 0.8785141798915508\tValidation Loss: 0.8799065058128028\n",
            "Epoch 65  \tTraining Loss: 0.8781370710827007\tValidation Loss: 0.8795404274825914\n",
            "Epoch 66  \tTraining Loss: 0.8777808330947722\tValidation Loss: 0.8791943404570497\n",
            "Epoch 67  \tTraining Loss: 0.8774443107900305\tValidation Loss: 0.8788671457582398\n",
            "Epoch 68  \tTraining Loss: 0.8771264129623552\tValidation Loss: 0.8785578050200656\n",
            "Epoch 69  \tTraining Loss: 0.8768261087988208\tValidation Loss: 0.878265337139443\n",
            "Epoch 70  \tTraining Loss: 0.8765424245370911\tValidation Loss: 0.8779888151126521\n",
            "Epoch 71  \tTraining Loss: 0.8762744403078347\tValidation Loss: 0.8777273630466351\n",
            "Epoch 72  \tTraining Loss: 0.8760212871515632\tValidation Loss: 0.8774801533340951\n",
            "Epoch 73  \tTraining Loss: 0.8757821441998691\tValidation Loss: 0.8772464039885273\n",
            "Epoch 74  \tTraining Loss: 0.8755562360163047\tValidation Loss: 0.877025376123817\n",
            "Epoch 75  \tTraining Loss: 0.8753428300808822\tValidation Loss: 0.8768163715732666\n",
            "Epoch 76  \tTraining Loss: 0.8751412344144599\tValidation Loss: 0.8766187306406569\n",
            "Epoch 77  \tTraining Loss: 0.8749507953349023\tValidation Loss: 0.8764318299756421\n",
            "Epoch 78  \tTraining Loss: 0.8747708953374208\tValidation Loss: 0.8762550805665961\n",
            "Epoch 79  \tTraining Loss: 0.8746009510922323\tValidation Loss: 0.8760879258444138\n",
            "Epoch 80  \tTraining Loss: 0.8744404115530333\tValidation Loss: 0.8759298398911299\n",
            "Epoch 81  \tTraining Loss: 0.8742887561701636\tValidation Loss: 0.8757803257475612\n",
            "Epoch 82  \tTraining Loss: 0.8741454932026594\tValidation Loss: 0.8756389138144851\n",
            "Epoch 83  \tTraining Loss: 0.8740101581237288\tValidation Loss: 0.8755051603422598\n",
            "Epoch 84  \tTraining Loss: 0.8738823121149157\tValidation Loss: 0.875378646004783\n",
            "Epoch 85  \tTraining Loss: 0.8737615373978068\tValidation Loss: 0.8752589667165434\n",
            "Epoch 86  \tTraining Loss: 0.8736474339674868\tValidation Loss: 0.8751457556735798\n",
            "Epoch 87  \tTraining Loss: 0.8735396430035869\tValidation Loss: 0.8750386579567807\n",
            "Epoch 88  \tTraining Loss: 0.8734378126900895\tValidation Loss: 0.8749373372054307\n",
            "Epoch 89  \tTraining Loss: 0.8733416124485939\tValidation Loss: 0.8748414794316053\n",
            "Epoch 90  \tTraining Loss: 0.8732507334811991\tValidation Loss: 0.8747507857641987\n",
            "Epoch 91  \tTraining Loss: 0.8731648810012875\tValidation Loss: 0.8746649735992229\n",
            "Epoch 92  \tTraining Loss: 0.873083776419218\tValidation Loss: 0.8745837760251892\n",
            "Epoch 93  \tTraining Loss: 0.8730071566389803\tValidation Loss: 0.8745069407770735\n",
            "Epoch 94  \tTraining Loss: 0.8729347731087451\tValidation Loss: 0.8744342294277142\n",
            "Epoch 95  \tTraining Loss: 0.8728663740411461\tValidation Loss: 0.8743653880677815\n",
            "Epoch 96  \tTraining Loss: 0.8728017228620663\tValidation Loss: 0.874300125980458\n",
            "Epoch 97  \tTraining Loss: 0.8727406394694224\tValidation Loss: 0.8742383434107381\n",
            "Epoch 98  \tTraining Loss: 0.8726829252626221\tValidation Loss: 0.8741798493442678\n",
            "Epoch 99  \tTraining Loss: 0.8726283918618286\tValidation Loss: 0.874124433466712\n",
            "Epoch 100  \tTraining Loss: 0.872576809113281\tValidation Loss: 0.8740718866966265\n",
            "Epoch 101  \tTraining Loss: 0.872527945126609\tValidation Loss: 0.8740218295861287\n",
            "Epoch 102  \tTraining Loss: 0.8724813220977836\tValidation Loss: 0.8739741504330286\n",
            "Epoch 103  \tTraining Loss: 0.8724370263081974\tValidation Loss: 0.873928731920704\n",
            "Epoch 104  \tTraining Loss: 0.8723947207913\tValidation Loss: 0.8738853314464625\n",
            "Epoch 105  \tTraining Loss: 0.8723542459320842\tValidation Loss: 0.8738431511857976\n",
            "Epoch 106  \tTraining Loss: 0.8723151006776522\tValidation Loss: 0.8738010591811399\n",
            "Epoch 107  \tTraining Loss: 0.872275990979351\tValidation Loss: 0.8737541053418192\n",
            "Epoch 108  \tTraining Loss: 0.8722317818131606\tValidation Loss: 0.8736707967040981\n",
            "Epoch 109  \tTraining Loss: 0.8721630324308288\tValidation Loss: 0.8734565747003962\n",
            "Epoch 110  \tTraining Loss: 0.871967453111955\tValidation Loss: 0.8709846453900089\n",
            "Epoch 111  \tTraining Loss: 0.869248671635285\tValidation Loss: 0.931646166942967\n",
            "Epoch 112  \tTraining Loss: 0.9277306720464344\tValidation Loss: 0.8736853314115463\n",
            "Epoch 113  \tTraining Loss: 0.8721890675324174\tValidation Loss: 0.8736561404785168\n",
            "Epoch 114  \tTraining Loss: 0.8721620329529269\tValidation Loss: 0.8736285062977989\n",
            "Epoch 115  \tTraining Loss: 0.8721364946046536\tValidation Loss: 0.8736023443617326\n",
            "Epoch 116  \tTraining Loss: 0.8721123696785471\tValidation Loss: 0.8735775747934061\n",
            "Epoch 117  \tTraining Loss: 0.8720895799486313\tValidation Loss: 0.8735541220916668\n",
            "Epoch 118  \tTraining Loss: 0.8720680515183539\tValidation Loss: 0.8735319148902094\n",
            "Epoch 119  \tTraining Loss: 0.8720477145809734\tValidation Loss: 0.8735108857299595\n",
            "Epoch 120  \tTraining Loss: 0.8720285031932089\tValidation Loss: 0.8734909708440229\n",
            "Epoch 121  \tTraining Loss: 0.8720103550614167\tValidation Loss: 0.8734721099545011\n",
            "Epoch 122  \tTraining Loss: 0.8719932113396001\tValidation Loss: 0.8734542460805212\n",
            "Epoch 123  \tTraining Loss: 0.8719770164385988\tValidation Loss: 0.873437325356858\n",
            "Epoch 124  \tTraining Loss: 0.8719617178458405\tValidation Loss: 0.8734212968625625\n",
            "Epoch 125  \tTraining Loss: 0.8719472659550651\tValidation Loss: 0.8734061124590482\n",
            "Epoch 126  \tTraining Loss: 0.8719336139054756\tValidation Loss: 0.873391726637106\n",
            "Epoch 127  \tTraining Loss: 0.8719207174297907\tValidation Loss: 0.8733780963723594\n",
            "Epoch 128  \tTraining Loss: 0.8719085347107045\tValidation Loss: 0.873365180988694\n",
            "Epoch 129  \tTraining Loss: 0.8718970262452949\tValidation Loss: 0.873352942029214\n",
            "Epoch 130  \tTraining Loss: 0.871886154716931\tValidation Loss: 0.8733413431343198\n",
            "Epoch 131  \tTraining Loss: 0.871875884874276\tValidation Loss: 0.8733303499265049\n",
            "Epoch 132  \tTraining Loss: 0.8718661834169801\tValidation Loss: 0.8733199299015056\n",
            "Epoch 133  \tTraining Loss: 0.8718570188877052\tValidation Loss: 0.873310052325451\n",
            "Epoch 134  \tTraining Loss: 0.8718483615701215\tValidation Loss: 0.8733006881376816\n",
            "Epoch 135  \tTraining Loss: 0.8718401833925529\tValidation Loss: 0.8732918098589241\n",
            "Epoch 136  \tTraining Loss: 0.871832457836953\tValidation Loss: 0.873283391504528\n",
            "Epoch 137  \tTraining Loss: 0.8718251598529173\tValidation Loss: 0.8732754085024802\n",
            "Epoch 138  \tTraining Loss: 0.8718182657764604\tValidation Loss: 0.8732678376159378\n",
            "Epoch 139  \tTraining Loss: 0.871811753253281\tValidation Loss: 0.8732606568700286\n",
            "Epoch 140  \tTraining Loss: 0.8718056011662786\tValidation Loss: 0.8732538454826831\n",
            "Epoch 141  \tTraining Loss: 0.871799789567081\tValidation Loss: 0.8732473837992747\n",
            "Epoch 142  \tTraining Loss: 0.8717942996113601\tValidation Loss: 0.8732412532308601\n",
            "Epoch 143  \tTraining Loss: 0.8717891134977293\tValidation Loss: 0.8732354361958193\n",
            "Epoch 144  \tTraining Loss: 0.8717842144100205\tValidation Loss: 0.8732299160647083\n",
            "Epoch 145  \tTraining Loss: 0.8717795864627589\tValidation Loss: 0.8732246771081463\n",
            "Epoch 146  \tTraining Loss: 0.8717752146496518\tValidation Loss: 0.8732197044475718\n",
            "Epoch 147  \tTraining Loss: 0.8717710847949315\tValidation Loss: 0.8732149840087072\n",
            "Epoch 148  \tTraining Loss: 0.87176718350739\tValidation Loss: 0.873210502477584\n",
            "Epoch 149  \tTraining Loss: 0.871763498136956\tValidation Loss: 0.873206247258986\n",
            "Epoch 150  \tTraining Loss: 0.8717600167336776\tValidation Loss: 0.8732022064371786\n",
            "Epoch 151  \tTraining Loss: 0.871756728008975\tValidation Loss: 0.8731983687387965\n",
            "Epoch 152  \tTraining Loss: 0.8717536212990348\tValidation Loss: 0.8731947234977723\n",
            "Epoch 153  \tTraining Loss: 0.8717506865302335\tValidation Loss: 0.8731912606221915\n",
            "Epoch 154  \tTraining Loss: 0.871747914186474\tValidation Loss: 0.8731879705629706\n",
            "Epoch 155  \tTraining Loss: 0.8717452952783277\tValidation Loss: 0.8731848442842535\n",
            "Epoch 156  \tTraining Loss: 0.8717428213138865\tValidation Loss: 0.873181873235437\n",
            "Epoch 157  \tTraining Loss: 0.8717404842712282\tValidation Loss: 0.8731790493247297\n",
            "Epoch 158  \tTraining Loss: 0.8717382765724049\tValidation Loss: 0.8731763648941641\n",
            "Epoch 159  \tTraining Loss: 0.87173619105887\tValidation Loss: 0.8731738126959799\n",
            "Epoch 160  \tTraining Loss: 0.8717342209682681\tValidation Loss: 0.8731713858703041\n",
            "Epoch 161  \tTraining Loss: 0.8717323599125066\tValidation Loss: 0.8731690779240538\n",
            "Epoch 162  \tTraining Loss: 0.8717306018570424\tValidation Loss: 0.8731668827109985\n",
            "Epoch 163  \tTraining Loss: 0.8717289411013149\tValidation Loss: 0.8731647944129138\n",
            "Epoch 164  \tTraining Loss: 0.8717273722602615\tValidation Loss: 0.8731628075217694\n",
            "Epoch 165  \tTraining Loss: 0.8717258902468569\tValidation Loss: 0.8731609168228933\n",
            "Epoch 166  \tTraining Loss: 0.8717244902556158\tValidation Loss: 0.873159117379057\n",
            "Epoch 167  \tTraining Loss: 0.8717231677470153\tValidation Loss: 0.8731574045154356\n",
            "Epoch 168  \tTraining Loss: 0.8717219184327715\tValidation Loss: 0.8731557738053877\n",
            "Epoch 169  \tTraining Loss: 0.8717207382619361\tValidation Loss: 0.8731542210570169\n",
            "Epoch 170  \tTraining Loss: 0.8717196234077613\tValidation Loss: 0.8731527423004662\n",
            "Epoch 171  \tTraining Loss: 0.8717185702552901\tValidation Loss: 0.8731513337759101\n",
            "Epoch 172  \tTraining Loss: 0.8717175753896365\tValidation Loss: 0.8731499919222014\n",
            "Epoch 173  \tTraining Loss: 0.8717166355849114\tValidation Loss: 0.8731487133661391\n",
            "Epoch 174  \tTraining Loss: 0.8717157477937628\tValidation Loss: 0.8731474949123242\n",
            "Epoch 175  \tTraining Loss: 0.8717149091374948\tValidation Loss: 0.8731463335335677\n",
            "Epoch 176  \tTraining Loss: 0.8717141168967326\tValidation Loss: 0.8731452263618238\n",
            "Epoch 177  \tTraining Loss: 0.8717133685026064\tValidation Loss: 0.8731441706796179\n",
            "Epoch 178  \tTraining Loss: 0.8717126615284199\tValidation Loss: 0.8731431639119426\n",
            "Epoch 179  \tTraining Loss: 0.8717119936817826\tValidation Loss: 0.8731422036185945\n",
            "Epoch 180  \tTraining Loss: 0.8717113627971778\tValidation Loss: 0.873141287486931\n",
            "Epoch 181  \tTraining Loss: 0.8717107668289372\tValidation Loss: 0.8731404133250196\n",
            "Epoch 182  \tTraining Loss: 0.8717102038446123\tValidation Loss: 0.8731395790551627\n",
            "Epoch 183  \tTraining Loss: 0.8717096720187051\tValidation Loss: 0.8731387827077728\n",
            "Epoch 184  \tTraining Loss: 0.8717091696267504\tValidation Loss: 0.8731380224155831\n",
            "Epoch 185  \tTraining Loss: 0.8717086950397234\tValidation Loss: 0.8731372964081721\n",
            "Epoch 186  \tTraining Loss: 0.8717082467187576\tValidation Loss: 0.8731366030067854\n",
            "Epoch 187  \tTraining Loss: 0.8717078232101557\tValidation Loss: 0.8731359406194408\n",
            "Epoch 188  \tTraining Loss: 0.8717074231406752\tValidation Loss: 0.8731353077362971\n",
            "Epoch 189  \tTraining Loss: 0.871707045213076\tValidation Loss: 0.873134702925276\n",
            "Epoch 190  \tTraining Loss: 0.8717066882019133\tValidation Loss: 0.8731341248279211\n",
            "Epoch 191  \tTraining Loss: 0.8717063509495657\tValidation Loss: 0.8731335721554813\n",
            "Epoch 192  \tTraining Loss: 0.871706032362479\tValidation Loss: 0.8731330436852068\n",
            "Epoch 193  \tTraining Loss: 0.8717057314076232\tValidation Loss: 0.873132538256846\n",
            "Epoch 194  \tTraining Loss: 0.871705447109141\tValidation Loss: 0.8731320547693309\n",
            "Epoch 195  \tTraining Loss: 0.8717051785451834\tValidation Loss: 0.873131592177644\n",
            "Epoch 196  \tTraining Loss: 0.8717049248449221\tValidation Loss: 0.8731311494898525\n",
            "Epoch 197  \tTraining Loss: 0.8717046851857245\tValidation Loss: 0.8731307257643035\n",
            "Epoch 198  \tTraining Loss: 0.8717044587904865\tValidation Loss: 0.873130320106972\n",
            "Epoch 199  \tTraining Loss: 0.8717042449251133\tValidation Loss: 0.8731299316689497\n",
            "Epoch 200  \tTraining Loss: 0.8717040428961382\tValidation Loss: 0.873129559644071\n",
            "Epoch 201  \tTraining Loss: 0.8717038520484748\tValidation Loss: 0.8731292032666667\n",
            "Epoch 202  \tTraining Loss: 0.8717036717632924\tValidation Loss: 0.8731288618094377\n",
            "Epoch 203  \tTraining Loss: 0.8717035014560097\tValidation Loss: 0.8731285345814443\n",
            "Epoch 204  \tTraining Loss: 0.8717033405743984\tValidation Loss: 0.8731282209262036\n",
            "Epoch 205  \tTraining Loss: 0.8717031885967944\tValidation Loss: 0.8731279202198867\n",
            "Epoch 206  \tTraining Loss: 0.8717030450304045\tValidation Loss: 0.8731276318696177\n",
            "Epoch 207  \tTraining Loss: 0.8717029094097093\tValidation Loss: 0.8731273553118585\n",
            "Epoch 208  \tTraining Loss: 0.8717027812949537\tValidation Loss: 0.8731270900108836\n",
            "Epoch 209  \tTraining Loss: 0.8717026602707204\tValidation Loss: 0.8731268354573353\n",
            "Epoch 210  \tTraining Loss: 0.8717025459445844\tValidation Loss: 0.8731265911668568\n",
            "Epoch 211  \tTraining Loss: 0.8717024379458386\tValidation Loss: 0.8731263566787975\n",
            "Epoch 212  \tTraining Loss: 0.8717023359242928\tValidation Loss: 0.8731261315549889\n",
            "Epoch 213  \tTraining Loss: 0.8717022395491386\tValidation Loss: 0.8731259153785838\n",
            "Epoch 214  \tTraining Loss: 0.8717021485078762\tValidation Loss: 0.8731257077529591\n",
            "Epoch 215  \tTraining Loss: 0.8717020625053005\tValidation Loss: 0.8731255083006767\n",
            "Epoch 216  \tTraining Loss: 0.8717019812625448\tValidation Loss: 0.8731253166624984\n",
            "Epoch 217  \tTraining Loss: 0.8717019045161768\tValidation Loss: 0.873125132496455\n",
            "Epoch 218  \tTraining Loss: 0.8717018320173433\tValidation Loss: 0.8731249554769633\n",
            "Epoch 219  \tTraining Loss: 0.8717017635309646\tValidation Loss: 0.8731247852939907\n",
            "Epoch 220  \tTraining Loss: 0.8717016988349703\tValidation Loss: 0.8731246216522641\n",
            "Epoch 221  \tTraining Loss: 0.8717016377195815\tValidation Loss: 0.87312446427052\n",
            "Epoch 222  \tTraining Loss: 0.8717015799866293\tValidation Loss: 0.8731243128807948\n",
            "Epoch 223  \tTraining Loss: 0.871701525448913\tValidation Loss: 0.8731241672277521\n",
            "Epoch 224  \tTraining Loss: 0.8717014739295911\tValidation Loss: 0.8731240270680464\n",
            "Epoch 225  \tTraining Loss: 0.871701425261611\tValidation Loss: 0.8731238921697181\n",
            "Epoch 226  \tTraining Loss: 0.8717013792871641\tValidation Loss: 0.8731237623116235\n",
            "Epoch 227  \tTraining Loss: 0.8717013358571775\tValidation Loss: 0.8731236372828919\n",
            "Epoch 228  \tTraining Loss: 0.8717012948308271\tValidation Loss: 0.8731235168824122\n",
            "Epoch 229  \tTraining Loss: 0.8717012560750839\tValidation Loss: 0.8731234009183468\n",
            "Epoch 230  \tTraining Loss: 0.8717012194642807\tValidation Loss: 0.8731232892076706\n",
            "Epoch 231  \tTraining Loss: 0.8717011848797057\tValidation Loss: 0.8731231815757332\n",
            "Epoch 232  \tTraining Loss: 0.8717011522092171\tValidation Loss: 0.8731230778558453\n",
            "Epoch 233  \tTraining Loss: 0.8717011213468795\tValidation Loss: 0.873122977888886\n",
            "Epoch 234  \tTraining Loss: 0.8717010921926209\tValidation Loss: 0.8731228815229299\n",
            "Epoch 235  \tTraining Loss: 0.8717010646519074\tValidation Loss: 0.873122788612894\n",
            "Epoch 236  \tTraining Loss: 0.8717010386354369\tValidation Loss: 0.8731226990202041\n",
            "Epoch 237  \tTraining Loss: 0.8717010140588504\tValidation Loss: 0.8731226126124759\n",
            "Epoch 238  \tTraining Loss: 0.8717009908424574\tValidation Loss: 0.8731225292632148\n",
            "Epoch 239  \tTraining Loss: 0.8717009689109776\tValidation Loss: 0.8731224488515298\n",
            "Epoch 240  \tTraining Loss: 0.8717009481932976\tValidation Loss: 0.8731223712618629\n",
            "Epoch 241  \tTraining Loss: 0.8717009286222395\tValidation Loss: 0.8731222963837316\n",
            "Epoch 242  \tTraining Loss: 0.8717009101343433\tValidation Loss: 0.8731222241114859\n",
            "Epoch 243  \tTraining Loss: 0.8717008926696612\tValidation Loss: 0.8731221543440759\n",
            "Epoch 244  \tTraining Loss: 0.8717008761715636\tValidation Loss: 0.8731220869848328\n",
            "Epoch 245  \tTraining Loss: 0.8717008605865547\tValidation Loss: 0.8731220219412601\n",
            "Epoch 246  \tTraining Loss: 0.8717008458640997\tValidation Loss: 0.873121959124836\n",
            "Epoch 247  \tTraining Loss: 0.87170083195646\tValidation Loss: 0.8731218984508252\n",
            "Epoch 248  \tTraining Loss: 0.8717008188185399\tValidation Loss: 0.8731218398381011\n",
            "Epoch 249  \tTraining Loss: 0.8717008064077393\tValidation Loss: 0.8731217832089756\n",
            "Epoch 250  \tTraining Loss: 0.8717007946838152\tValidation Loss: 0.8731217284890395\n",
            "Epoch 251  \tTraining Loss: 0.8717007836087532\tValidation Loss: 0.8731216756070085\n",
            "Epoch 252  \tTraining Loss: 0.8717007731466413\tValidation Loss: 0.8731216244945786\n",
            "Epoch 253  \tTraining Loss: 0.8717007632635558\tValidation Loss: 0.8731215750862884\n",
            "Epoch 254  \tTraining Loss: 0.8717007539274508\tValidation Loss: 0.8731215273193882\n",
            "Epoch 255  \tTraining Loss: 0.871700745108053\tValidation Loss: 0.8731214811337146\n",
            "Epoch 256  \tTraining Loss: 0.871700736776766\tValidation Loss: 0.8731214364715735\n",
            "Epoch 257  \tTraining Loss: 0.8717007289065747\tValidation Loss: 0.8731213932776267\n",
            "Epoch 258  \tTraining Loss: 0.8717007214719598\tValidation Loss: 0.8731213514987852\n",
            "Epoch 259  \tTraining Loss: 0.8717007144488143\tValidation Loss: 0.873121311084108\n",
            "Epoch 260  \tTraining Loss: 0.8717007078143653\tValidation Loss: 0.8731212719847042\n",
            "Epoch 261  \tTraining Loss: 0.8717007015471006\tValidation Loss: 0.8731212341536431\n",
            "Epoch 262  \tTraining Loss: 0.8717006956266983\tValidation Loss: 0.8731211975458647\n",
            "Epoch 263  \tTraining Loss: 0.8717006900339611\tValidation Loss: 0.8731211621180976\n",
            "Epoch 264  \tTraining Loss: 0.8717006847507544\tValidation Loss: 0.8731211278287794\n",
            "Epoch 265  \tTraining Loss: 0.8717006797599473\tValidation Loss: 0.8731210946379819\n",
            "Epoch 266  \tTraining Loss: 0.8717006750453568\tValidation Loss: 0.8731210625073386\n",
            "Epoch 267  \tTraining Loss: 0.8717006705916959\tValidation Loss: 0.8731210313999771\n",
            "Epoch 268  \tTraining Loss: 0.8717006663845233\tValidation Loss: 0.8731210012804534\n",
            "Epoch 269  \tTraining Loss: 0.8717006624101967\tValidation Loss: 0.8731209721146911\n",
            "Epoch 270  \tTraining Loss: 0.8717006586558296\tValidation Loss: 0.8731209438699209\n",
            "Epoch 271  \tTraining Loss: 0.8717006551092487\tValidation Loss: 0.873120916514626\n",
            "Epoch 272  \tTraining Loss: 0.8717006517589534\tValidation Loss: 0.8731208900184877\n",
            "Epoch 273  \tTraining Loss: 0.8717006485940803\tValidation Loss: 0.873120864352334\n",
            "Epoch 274  \tTraining Loss: 0.8717006456043674\tValidation Loss: 0.8731208394880929\n",
            "Epoch 275  \tTraining Loss: 0.8717006427801207\tValidation Loss: 0.8731208153987432\n",
            "Epoch 276  \tTraining Loss: 0.871700640112182\tValidation Loss: 0.8731207920582729\n",
            "Epoch 277  \tTraining Loss: 0.8717006375919009\tValidation Loss: 0.8731207694416355\n",
            "Epoch 278  \tTraining Loss: 0.8717006352111046\tValidation Loss: 0.8731207475247109\n",
            "Epoch 279  \tTraining Loss: 0.8717006329620738\tValidation Loss: 0.8731207262842661\n",
            "Epoch 280  \tTraining Loss: 0.8717006308375159\tValidation Loss: 0.8731207056979197\n",
            "Epoch 281  \tTraining Loss: 0.8717006288305418\tValidation Loss: 0.8731206857441053\n",
            "Epoch 282  \tTraining Loss: 0.8717006269346437\tValidation Loss: 0.8731206664020407\n",
            "Epoch 283  \tTraining Loss: 0.8717006251436744\tValidation Loss: 0.8731206476516934\n",
            "Epoch 284  \tTraining Loss: 0.8717006234518266\tValidation Loss: 0.8731206294737525\n",
            "Epoch 285  \tTraining Loss: 0.8717006218536141\tValidation Loss: 0.8731206118495981\n",
            "Epoch 286  \tTraining Loss: 0.8717006203438548\tValidation Loss: 0.8731205947612747\n",
            "Epoch 287  \tTraining Loss: 0.8717006189176533\tValidation Loss: 0.8731205781914639\n",
            "Epoch 288  \tTraining Loss: 0.871700617570385\tValidation Loss: 0.87312056212346\n",
            "Epoch 289  \tTraining Loss: 0.8717006162976814\tValidation Loss: 0.8731205465411449\n",
            "Epoch 290  \tTraining Loss: 0.8717006150954159\tValidation Loss: 0.8731205314289655\n",
            "Epoch 291  \tTraining Loss: 0.8717006139596898\tValidation Loss: 0.8731205167719117\n",
            "Epoch 292  \tTraining Loss: 0.8717006128868202\tValidation Loss: 0.8731205025554954\n",
            "Epoch 293  \tTraining Loss: 0.8717006118733291\tValidation Loss: 0.8731204887657295\n",
            "Epoch 294  \tTraining Loss: 0.8717006109159297\tValidation Loss: 0.8731204753891092\n",
            "Epoch 295  \tTraining Loss: 0.8717006100115172\tValidation Loss: 0.8731204624125938\n",
            "Epoch 296  \tTraining Loss: 0.8717006091571601\tValidation Loss: 0.8731204498235884\n",
            "Epoch 297  \tTraining Loss: 0.8717006083500872\tValidation Loss: 0.873120437609927\n",
            "Epoch 298  \tTraining Loss: 0.871700607587682\tValidation Loss: 0.8731204257598565\n",
            "Epoch 299  \tTraining Loss: 0.8717006068674718\tValidation Loss: 0.8731204142620215\n",
            "Epoch 300  \tTraining Loss: 0.871700606187122\tValidation Loss: 0.873120403105448\n",
            "Epoch 301  \tTraining Loss: 0.8717006055444261\tValidation Loss: 0.8731203922795316\n",
            "Epoch 302  \tTraining Loss: 0.8717006049373004\tValidation Loss: 0.8731203817740215\n",
            "Epoch 303  \tTraining Loss: 0.871700604363776\tValidation Loss: 0.8731203715790081\n",
            "Epoch 304  \tTraining Loss: 0.8717006038219932\tValidation Loss: 0.8731203616849114\n",
            "Epoch 305  \tTraining Loss: 0.8717006033101957\tValidation Loss: 0.8731203520824674\n",
            "Epoch 306  \tTraining Loss: 0.8717006028267237\tValidation Loss: 0.8731203427627179\n",
            "Epoch 307  \tTraining Loss: 0.8717006023700093\tValidation Loss: 0.873120333716999\n",
            "Epoch 308  \tTraining Loss: 0.8717006019385719\tValidation Loss: 0.8731203249369299\n",
            "Epoch 309  \tTraining Loss: 0.8717006015310124\tValidation Loss: 0.8731203164144036\n",
            "Epoch 310  \tTraining Loss: 0.8717006011460092\tValidation Loss: 0.8731203081415763\n",
            "Epoch 311  \tTraining Loss: 0.8717006007823143\tValidation Loss: 0.8731203001108585\n",
            "Epoch 312  \tTraining Loss: 0.8717006004387482\tValidation Loss: 0.8731202923149065\n",
            "Epoch 313  \tTraining Loss: 0.8717006001141966\tValidation Loss: 0.8731202847466121\n",
            "Epoch 314  \tTraining Loss: 0.8717005998076073\tValidation Loss: 0.8731202773990955\n",
            "Epoch 315  \tTraining Loss: 0.8717005995179861\tValidation Loss: 0.873120270265698\n",
            "Epoch 316  \tTraining Loss: 0.8717005992443941\tValidation Loss: 0.8731202633399726\n",
            "Epoch 317  \tTraining Loss: 0.8717005989859443\tValidation Loss: 0.8731202566156775\n",
            "Epoch 318  \tTraining Loss: 0.8717005987417983\tValidation Loss: 0.8731202500867692\n",
            "Epoch 319  \tTraining Loss: 0.8717005985111645\tValidation Loss: 0.8731202437473955\n",
            "Epoch 320  \tTraining Loss: 0.8717005982932953\tValidation Loss: 0.8731202375918882\n",
            "Epoch 321  \tTraining Loss: 0.8717005980874838\tValidation Loss: 0.8731202316147579\n",
            "Epoch 322  \tTraining Loss: 0.8717005978930632\tValidation Loss: 0.8731202258106883\n",
            "Epoch 323  \tTraining Loss: 0.8717005977094029\tValidation Loss: 0.8731202201745276\n",
            "Epoch 324  \tTraining Loss: 0.8717005975359069\tValidation Loss: 0.8731202147012868\n",
            "Epoch 325  \tTraining Loss: 0.8717005973720132\tValidation Loss: 0.8731202093861307\n",
            "Epoch 326  \tTraining Loss: 0.8717005972171904\tValidation Loss: 0.8731202042243753\n",
            "Epoch 327  \tTraining Loss: 0.8717005970709365\tValidation Loss: 0.8731201992114817\n",
            "Epoch 328  \tTraining Loss: 0.8717005969327767\tValidation Loss: 0.8731201943430505\n",
            "Epoch 329  \tTraining Loss: 0.8717005968022634\tValidation Loss: 0.8731201896148191\n",
            "Epoch 330  \tTraining Loss: 0.8717005966789731\tValidation Loss: 0.8731201850226553\n",
            "Epoch 331  \tTraining Loss: 0.8717005965625066\tValidation Loss: 0.8731201805625539\n",
            "Epoch 332  \tTraining Loss: 0.8717005964524861\tValidation Loss: 0.8731201762306332\n",
            "Epoch 333  \tTraining Loss: 0.8717005963485545\tValidation Loss: 0.8731201720231286\n",
            "Epoch 334  \tTraining Loss: 0.8717005962503752\tValidation Loss: 0.8731201679363919\n",
            "Epoch 335  \tTraining Loss: 0.8717005961576293\tValidation Loss: 0.8731201639668853\n",
            "Epoch 336  \tTraining Loss: 0.8717005960700167\tValidation Loss: 0.8731201601111784\n",
            "Epoch 337  \tTraining Loss: 0.8717005959872528\tValidation Loss: 0.8731201563659449\n",
            "Epoch 338  \tTraining Loss: 0.8717005959090697\tValidation Loss: 0.8731201527279595\n",
            "Epoch 339  \tTraining Loss: 0.8717005958352135\tValidation Loss: 0.873120149194094\n",
            "Epoch 340  \tTraining Loss: 0.8717005957654449\tValidation Loss: 0.8731201457613145\n",
            "Epoch 341  \tTraining Loss: 0.871700595699538\tValidation Loss: 0.8731201424266787\n",
            "Epoch 342  \tTraining Loss: 0.871700595637278\tValidation Loss: 0.8731201391873324\n",
            "Epoch 343  \tTraining Loss: 0.8717005955784644\tValidation Loss: 0.8731201360405074\n",
            "Epoch 344  \tTraining Loss: 0.8717005955229056\tValidation Loss: 0.8731201329835181\n",
            "Epoch 345  \tTraining Loss: 0.8717005954704218\tValidation Loss: 0.8731201300137589\n",
            "Epoch 346  \tTraining Loss: 0.8717005954208427\tValidation Loss: 0.8731201271287029\n",
            "Epoch 347  \tTraining Loss: 0.8717005953740077\tValidation Loss: 0.8731201243258977\n",
            "Epoch 348  \tTraining Loss: 0.8717005953297644\tValidation Loss: 0.8731201216029638\n",
            "Epoch 349  \tTraining Loss: 0.8717005952879701\tValidation Loss: 0.873120118957593\n",
            "Epoch 350  \tTraining Loss: 0.8717005952484886\tValidation Loss: 0.8731201163875454\n",
            "Epoch 351  \tTraining Loss: 0.8717005952111926\tValidation Loss: 0.8731201138906473\n",
            "Epoch 352  \tTraining Loss: 0.8717005951759604\tValidation Loss: 0.873120111464789\n",
            "Epoch 353  \tTraining Loss: 0.8717005951426784\tValidation Loss: 0.8731201091079238\n",
            "Epoch 354  \tTraining Loss: 0.8717005951112382\tValidation Loss: 0.873120106818065\n",
            "Epoch 355  \tTraining Loss: 0.871700595081538\tValidation Loss: 0.8731201045932845\n",
            "Epoch 356  \tTraining Loss: 0.8717005950534819\tValidation Loss: 0.8731201024317108\n",
            "Epoch 357  \tTraining Loss: 0.8717005950269782\tValidation Loss: 0.8731201003315278\n",
            "Epoch 358  \tTraining Loss: 0.8717005950019414\tValidation Loss: 0.8731200982909724\n",
            "Epoch 359  \tTraining Loss: 0.8717005949782906\tValidation Loss: 0.8731200963083331\n",
            "Epoch 360  \tTraining Loss: 0.8717005949559483\tValidation Loss: 0.8731200943819489\n",
            "Epoch 361  \tTraining Loss: 0.8717005949348428\tValidation Loss: 0.8731200925102071\n",
            "Epoch 362  \tTraining Loss: 0.8717005949149053\tValidation Loss: 0.8731200906915423\n",
            "Epoch 363  \tTraining Loss: 0.8717005948960713\tValidation Loss: 0.8731200889244346\n",
            "Epoch 364  \tTraining Loss: 0.8717005948782797\tValidation Loss: 0.8731200872074085\n",
            "Epoch 365  \tTraining Loss: 0.8717005948614727\tValidation Loss: 0.8731200855390318\n",
            "Epoch 366  \tTraining Loss: 0.8717005948455959\tValidation Loss: 0.8731200839179133\n",
            "Epoch 367  \tTraining Loss: 0.8717005948305977\tValidation Loss: 0.8731200823427031\n",
            "Epoch 368  \tTraining Loss: 0.8717005948164298\tValidation Loss: 0.8731200808120898\n",
            "Epoch 369  \tTraining Loss: 0.871700594803046\tValidation Loss: 0.8731200793248004\n",
            "Epoch 370  \tTraining Loss: 0.8717005947904027\tValidation Loss: 0.8731200778795988\n",
            "Epoch 371  \tTraining Loss: 0.8717005947784593\tValidation Loss: 0.8731200764752847\n",
            "Epoch 372  \tTraining Loss: 0.8717005947671771\tValidation Loss: 0.8731200751106923\n",
            "Epoch 373  \tTraining Loss: 0.8717005947565192\tValidation Loss: 0.8731200737846898\n",
            "Epoch 374  \tTraining Loss: 0.8717005947464509\tValidation Loss: 0.8731200724961781\n",
            "Epoch 375  \tTraining Loss: 0.87170059473694\tValidation Loss: 0.8731200712440896\n",
            "Epoch 376  \tTraining Loss: 0.8717005947279555\tValidation Loss: 0.8731200700273882\n",
            "Epoch 377  \tTraining Loss: 0.8717005947194681\tValidation Loss: 0.8731200688450665\n",
            "Epoch 378  \tTraining Loss: 0.8717005947114507\tValidation Loss: 0.873120067696147\n",
            "Epoch 379  \tTraining Loss: 0.8717005947038768\tValidation Loss: 0.8731200665796804\n",
            "Epoch 380  \tTraining Loss: 0.8717005946967223\tValidation Loss: 0.873120065494744\n",
            "Epoch 381  \tTraining Loss: 0.8717005946899635\tValidation Loss: 0.8731200644404421\n",
            "Epoch 382  \tTraining Loss: 0.871700594683579\tValidation Loss: 0.873120063415905\n",
            "Epoch 383  \tTraining Loss: 0.8717005946775478\tValidation Loss: 0.8731200624202873\n",
            "Epoch 384  \tTraining Loss: 0.8717005946718503\tValidation Loss: 0.8731200614527688\n",
            "Epoch 385  \tTraining Loss: 0.8717005946664681\tValidation Loss: 0.8731200605125514\n",
            "Epoch 386  \tTraining Loss: 0.8717005946613838\tValidation Loss: 0.8731200595988617\n",
            "Epoch 387  \tTraining Loss: 0.8717005946565812\tValidation Loss: 0.8731200587109467\n",
            "Epoch 388  \tTraining Loss: 0.8717005946520442\tValidation Loss: 0.8731200578480762\n",
            "Epoch 389  \tTraining Loss: 0.8717005946477583\tValidation Loss: 0.8731200570095401\n",
            "Epoch 390  \tTraining Loss: 0.8717005946437093\tValidation Loss: 0.8731200561946493\n",
            "Epoch 391  \tTraining Loss: 0.8717005946398847\tValidation Loss: 0.8731200554027339\n",
            "Epoch 392  \tTraining Loss: 0.8717005946362715\tValidation Loss: 0.8731200546331431\n",
            "Epoch 393  \tTraining Loss: 0.8717005946328586\tValidation Loss: 0.8731200538852452\n",
            "Epoch 394  \tTraining Loss: 0.8717005946296347\tValidation Loss: 0.8731200531584259\n",
            "Epoch 395  \tTraining Loss: 0.8717005946265889\tValidation Loss: 0.8731200524520891\n",
            "Epoch 396  \tTraining Loss: 0.8717005946237119\tValidation Loss: 0.8731200517656551\n",
            "Epoch 397  \tTraining Loss: 0.8717005946209939\tValidation Loss: 0.8731200510985608\n",
            "Epoch 398  \tTraining Loss: 0.8717005946184264\tValidation Loss: 0.8731200504502595\n",
            "Epoch 399  \tTraining Loss: 0.8717005946160011\tValidation Loss: 0.8731200498202201\n",
            "Epoch 400  \tTraining Loss: 0.8717005946137097\tValidation Loss: 0.8731200492079257\n",
            "lr, batch_size: (0.01, 500)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 914.3434050937504\n",
            "Epoch 2  \tTraining Loss: 914.8765254955363\tValidation Loss: 0.9401842090383703\n",
            "Epoch 3  \tTraining Loss: 0.9370181172349683\tValidation Loss: 0.938417920087931\n",
            "Epoch 4  \tTraining Loss: 0.9352944862056255\tValidation Loss: 0.936747075070756\n",
            "Epoch 5  \tTraining Loss: 0.933665246573885\tValidation Loss: 0.9351675675169631\n",
            "Epoch 6  \tTraining Loss: 0.9321261771003404\tValidation Loss: 0.9336743741716047\n",
            "Epoch 7  \tTraining Loss: 0.9306722873049843\tValidation Loss: 0.932262748898413\n",
            "Epoch 8  \tTraining Loss: 0.9292988629063693\tValidation Loss: 0.930928207367703\n",
            "Epoch 9  \tTraining Loss: 0.9280014505353674\tValidation Loss: 0.9296665125910237\n",
            "Epoch 10  \tTraining Loss: 0.9267758432949464\tValidation Loss: 0.9284736612557136\n",
            "Epoch 11  \tTraining Loss: 0.9256180671191448\tValidation Loss: 0.9273458708151057\n",
            "Epoch 12  \tTraining Loss: 0.9245243678870094\tValidation Loss: 0.9262795672925838\n",
            "Epoch 13  \tTraining Loss: 0.9234911992497139\tValidation Loss: 0.9252713737599956\n",
            "Epoch 14  \tTraining Loss: 0.9225152111313887\tValidation Loss: 0.9243180994531209\n",
            "Epoch 15  \tTraining Loss: 0.9215932388663703\tValidation Loss: 0.9234167294889514\n",
            "Epoch 16  \tTraining Loss: 0.9207222929376548\tValidation Loss: 0.9225644151514933\n",
            "Epoch 17  \tTraining Loss: 0.9198995492832761\tValidation Loss: 0.9217584647146432\n",
            "Epoch 18  \tTraining Loss: 0.9191223401391766\tValidation Loss: 0.920996334772427\n",
            "Epoch 19  \tTraining Loss: 0.9183881453888855\tValidation Loss: 0.9202756220485412\n",
            "Epoch 20  \tTraining Loss: 0.917694584391944\tValidation Loss: 0.9195940556586788\n",
            "Epoch 21  \tTraining Loss: 0.9170394082645927\tValidation Loss: 0.918949489800599\n",
            "Epoch 22  \tTraining Loss: 0.9164204925876811\tValidation Loss: 0.9183398968482793\n",
            "Epoch 23  \tTraining Loss: 0.9158358305181611\tValidation Loss: 0.9177633608278012\n",
            "Epoch 24  \tTraining Loss: 0.9152835262821043\tValidation Loss: 0.917218071253747\n",
            "Epoch 25  \tTraining Loss: 0.9147617890290651\tValidation Loss: 0.9167023173061593\n",
            "Epoch 26  \tTraining Loss: 0.9142689270216375\tValidation Loss: 0.9162144823298196\n",
            "Epoch 27  \tTraining Loss: 0.913803342150826\tValidation Loss: 0.9157530386373933\n",
            "Epoch 28  \tTraining Loss: 0.9133635247550738\tValidation Loss: 0.9153165425996779\n",
            "Epoch 29  \tTraining Loss: 0.9129480487251275\tValidation Loss: 0.9149036300071866\n",
            "Epoch 30  \tTraining Loss: 0.9125555668798249\tValidation Loss: 0.9145130116880591\n",
            "Epoch 31  \tTraining Loss: 0.9121848065978083\tValidation Loss: 0.914143469368121\n",
            "Epoch 32  \tTraining Loss: 0.9118345656910037\tValidation Loss: 0.9137933444379073\n",
            "Epoch 33  \tTraining Loss: 0.911503201974437\tValidation Loss: 0.911566019941909\n",
            "Epoch 34  \tTraining Loss: 0.9093003221542041\tValidation Loss: 0.909461300535763\n",
            "Epoch 35  \tTraining Loss: 0.9072193610344994\tValidation Loss: 0.9074724193138101\n",
            "Epoch 36  \tTraining Loss: 0.9052535710026187\tValidation Loss: 0.9055929833596824\n",
            "Epoch 37  \tTraining Loss: 0.9033965778934165\tValidation Loss: 0.9038169530633082\n",
            "Epoch 38  \tTraining Loss: 0.9016423603208193\tValidation Loss: 0.9021386225815272\n",
            "Epoch 39  \tTraining Loss: 0.8999852301532403\tValidation Loss: 0.9005526013797065\n",
            "Epoch 40  \tTraining Loss: 0.8984198140695785\tValidation Loss: 0.8990537967945418\n",
            "Epoch 41  \tTraining Loss: 0.8969410361360065\tValidation Loss: 0.8976373975615347\n",
            "Epoch 42  \tTraining Loss: 0.8955441013470419\tValidation Loss: 0.8962988582537708\n",
            "Epoch 43  \tTraining Loss: 0.8942244800775405\tValidation Loss: 0.8950338845815689\n",
            "Epoch 44  \tTraining Loss: 0.8929778933951952\tValidation Loss: 0.8938384195053729\n",
            "Epoch 45  \tTraining Loss: 0.8918002991859131\tValidation Loss: 0.8927086301168793\n",
            "Epoch 46  \tTraining Loss: 0.890687879047086\tValidation Loss: 0.8916408952459005\n",
            "Epoch 47  \tTraining Loss: 0.8896370259062526\tValidation Loss: 0.8906317937528014\n",
            "Epoch 48  \tTraining Loss: 0.888644331381172\tValidation Loss: 0.889678088680907\n",
            "Epoch 49  \tTraining Loss: 0.8877065731272301\tValidation Loss: 0.8887767313528164\n",
            "Epoch 50  \tTraining Loss: 0.8868207152704439\tValidation Loss: 0.8879248367355169\n",
            "Epoch 51  \tTraining Loss: 0.885983885353116\tValidation Loss: 0.8871196791267131\n",
            "Epoch 52  \tTraining Loss: 0.8851933698948844\tValidation Loss: 0.8863586833462141\n",
            "Epoch 53  \tTraining Loss: 0.8844466055940425\tValidation Loss: 0.885639416414571\n",
            "Epoch 54  \tTraining Loss: 0.883741171015827\tValidation Loss: 0.8849595796919971\n",
            "Epoch 55  \tTraining Loss: 0.8830747787407209\tValidation Loss: 0.8843170014521096\n",
            "Epoch 56  \tTraining Loss: 0.8824452679473164\tValidation Loss: 0.8837096298664311\n",
            "Epoch 57  \tTraining Loss: 0.8818505974056793\tValidation Loss: 0.8831355263769239\n",
            "Epoch 58  \tTraining Loss: 0.8812888388585035\tValidation Loss: 0.8825928594350911\n",
            "Epoch 59  \tTraining Loss: 0.880758170768586\tValidation Loss: 0.8820798985873566\n",
            "Epoch 60  \tTraining Loss: 0.8802568724123494\tValidation Loss: 0.8815950088875736\n",
            "Epoch 61  \tTraining Loss: 0.8797833183002653\tValidation Loss: 0.8811366456185561\n",
            "Epoch 62  \tTraining Loss: 0.8793359728749386\tValidation Loss: 0.8807033479480376\n",
            "Epoch 63  \tTraining Loss: 0.8789133823364135\tValidation Loss: 0.8802937381556916\n",
            "Epoch 64  \tTraining Loss: 0.8785141798915508\tValidation Loss: 0.8799065058128028\n",
            "Epoch 65  \tTraining Loss: 0.8781370710827007\tValidation Loss: 0.8795404274825914\n",
            "Epoch 66  \tTraining Loss: 0.8777808330947722\tValidation Loss: 0.8791943404570497\n",
            "Epoch 67  \tTraining Loss: 0.8774443107900305\tValidation Loss: 0.8788671457582398\n",
            "Epoch 68  \tTraining Loss: 0.8771264129623552\tValidation Loss: 0.8785578050200656\n",
            "Epoch 69  \tTraining Loss: 0.8768261087988208\tValidation Loss: 0.878265337139443\n",
            "Epoch 70  \tTraining Loss: 0.8765424245370911\tValidation Loss: 0.8779888151126521\n",
            "Epoch 71  \tTraining Loss: 0.8762744403078347\tValidation Loss: 0.8777273630466351\n",
            "Epoch 72  \tTraining Loss: 0.8760212871515632\tValidation Loss: 0.8774801533340951\n",
            "Epoch 73  \tTraining Loss: 0.8757821441998691\tValidation Loss: 0.8772464039885273\n",
            "Epoch 74  \tTraining Loss: 0.8755562360163047\tValidation Loss: 0.877025376123817\n",
            "Epoch 75  \tTraining Loss: 0.8753428300808822\tValidation Loss: 0.8768163715732666\n",
            "Epoch 76  \tTraining Loss: 0.8751412344144599\tValidation Loss: 0.8766187306406569\n",
            "Epoch 77  \tTraining Loss: 0.8749507953349023\tValidation Loss: 0.8764318299756421\n",
            "Epoch 78  \tTraining Loss: 0.8747708953374208\tValidation Loss: 0.8762550805665961\n",
            "Epoch 79  \tTraining Loss: 0.8746009510922323\tValidation Loss: 0.8760879258444138\n",
            "Epoch 80  \tTraining Loss: 0.8744404115530333\tValidation Loss: 0.8759298398911299\n",
            "Epoch 81  \tTraining Loss: 0.8742887561701636\tValidation Loss: 0.8757803257475612\n",
            "Epoch 82  \tTraining Loss: 0.8741454932026594\tValidation Loss: 0.8756389138144851\n",
            "Epoch 83  \tTraining Loss: 0.8740101581237288\tValidation Loss: 0.8755051603422598\n",
            "Epoch 84  \tTraining Loss: 0.8738823121149157\tValidation Loss: 0.875378646004783\n",
            "Epoch 85  \tTraining Loss: 0.8737615373978068\tValidation Loss: 0.8752589667165434\n",
            "Epoch 86  \tTraining Loss: 0.8736474339674868\tValidation Loss: 0.8751457556735798\n",
            "Epoch 87  \tTraining Loss: 0.8735396430035869\tValidation Loss: 0.8750386579567807\n",
            "Epoch 88  \tTraining Loss: 0.8734378126900895\tValidation Loss: 0.8749373372054307\n",
            "Epoch 89  \tTraining Loss: 0.8733416124485939\tValidation Loss: 0.8748414794316053\n",
            "Epoch 90  \tTraining Loss: 0.8732507334811991\tValidation Loss: 0.8747507857641987\n",
            "Epoch 91  \tTraining Loss: 0.8731648810012875\tValidation Loss: 0.8746649735992229\n",
            "Epoch 92  \tTraining Loss: 0.873083776419218\tValidation Loss: 0.8745837760251892\n",
            "Epoch 93  \tTraining Loss: 0.8730071566389803\tValidation Loss: 0.8745069407770735\n",
            "Epoch 94  \tTraining Loss: 0.8729347731087451\tValidation Loss: 0.8744342294277142\n",
            "Epoch 95  \tTraining Loss: 0.8728663740411461\tValidation Loss: 0.8743653880677815\n",
            "Epoch 96  \tTraining Loss: 0.8728017228620663\tValidation Loss: 0.874300125980458\n",
            "Epoch 97  \tTraining Loss: 0.8727406394694224\tValidation Loss: 0.8742383434107381\n",
            "Epoch 98  \tTraining Loss: 0.8726829252626221\tValidation Loss: 0.8741798493442678\n",
            "Epoch 99  \tTraining Loss: 0.8726283918618286\tValidation Loss: 0.874124433466712\n",
            "Epoch 100  \tTraining Loss: 0.872576809113281\tValidation Loss: 0.8740718866966265\n",
            "Epoch 101  \tTraining Loss: 0.872527945126609\tValidation Loss: 0.8740218295861287\n",
            "Epoch 102  \tTraining Loss: 0.8724813220977836\tValidation Loss: 0.8739741504330286\n",
            "Epoch 103  \tTraining Loss: 0.8724370263081974\tValidation Loss: 0.873928731920704\n",
            "Epoch 104  \tTraining Loss: 0.8723947207913\tValidation Loss: 0.8738853314464625\n",
            "Epoch 105  \tTraining Loss: 0.8723542459320842\tValidation Loss: 0.8738431511857976\n",
            "Epoch 106  \tTraining Loss: 0.8723151006776522\tValidation Loss: 0.8738010591811399\n",
            "Epoch 107  \tTraining Loss: 0.872275990979351\tValidation Loss: 0.8737541053418192\n",
            "Epoch 108  \tTraining Loss: 0.8722317818131606\tValidation Loss: 0.8736707967040981\n",
            "Epoch 109  \tTraining Loss: 0.8721630324308288\tValidation Loss: 0.8734565747003962\n",
            "Epoch 110  \tTraining Loss: 0.871967453111955\tValidation Loss: 0.8709846453900089\n",
            "Epoch 111  \tTraining Loss: 0.869248671635285\tValidation Loss: 0.931646166942967\n",
            "Epoch 112  \tTraining Loss: 0.9277306720464344\tValidation Loss: 0.8736853314115463\n",
            "Epoch 113  \tTraining Loss: 0.8721890675324174\tValidation Loss: 0.8736561404785168\n",
            "Epoch 114  \tTraining Loss: 0.8721620329529269\tValidation Loss: 0.8736285062977989\n",
            "Epoch 115  \tTraining Loss: 0.8721364946046536\tValidation Loss: 0.8736023443617326\n",
            "Epoch 116  \tTraining Loss: 0.8721123696785471\tValidation Loss: 0.8735775747934061\n",
            "Epoch 117  \tTraining Loss: 0.8720895799486313\tValidation Loss: 0.8735541220916668\n",
            "Epoch 118  \tTraining Loss: 0.8720680515183539\tValidation Loss: 0.8735319148902094\n",
            "Epoch 119  \tTraining Loss: 0.8720477145809734\tValidation Loss: 0.8735108857299595\n",
            "Epoch 120  \tTraining Loss: 0.8720285031932089\tValidation Loss: 0.8734909708440229\n",
            "Epoch 121  \tTraining Loss: 0.8720103550614167\tValidation Loss: 0.8734721099545011\n",
            "Epoch 122  \tTraining Loss: 0.8719932113396001\tValidation Loss: 0.8734542460805212\n",
            "Epoch 123  \tTraining Loss: 0.8719770164385988\tValidation Loss: 0.873437325356858\n",
            "Epoch 124  \tTraining Loss: 0.8719617178458405\tValidation Loss: 0.8734212968625625\n",
            "Epoch 125  \tTraining Loss: 0.8719472659550651\tValidation Loss: 0.8734061124590482\n",
            "Epoch 126  \tTraining Loss: 0.8719336139054756\tValidation Loss: 0.873391726637106\n",
            "Epoch 127  \tTraining Loss: 0.8719207174297907\tValidation Loss: 0.8733780963723594\n",
            "Epoch 128  \tTraining Loss: 0.8719085347107045\tValidation Loss: 0.873365180988694\n",
            "Epoch 129  \tTraining Loss: 0.8718970262452949\tValidation Loss: 0.873352942029214\n",
            "Epoch 130  \tTraining Loss: 0.871886154716931\tValidation Loss: 0.8733413431343198\n",
            "Epoch 131  \tTraining Loss: 0.871875884874276\tValidation Loss: 0.8733303499265049\n",
            "Epoch 132  \tTraining Loss: 0.8718661834169801\tValidation Loss: 0.8733199299015056\n",
            "Epoch 133  \tTraining Loss: 0.8718570188877052\tValidation Loss: 0.873310052325451\n",
            "Epoch 134  \tTraining Loss: 0.8718483615701215\tValidation Loss: 0.8733006881376816\n",
            "Epoch 135  \tTraining Loss: 0.8718401833925529\tValidation Loss: 0.8732918098589241\n",
            "Epoch 136  \tTraining Loss: 0.871832457836953\tValidation Loss: 0.873283391504528\n",
            "Epoch 137  \tTraining Loss: 0.8718251598529173\tValidation Loss: 0.8732754085024802\n",
            "Epoch 138  \tTraining Loss: 0.8718182657764604\tValidation Loss: 0.8732678376159378\n",
            "Epoch 139  \tTraining Loss: 0.871811753253281\tValidation Loss: 0.8732606568700286\n",
            "Epoch 140  \tTraining Loss: 0.8718056011662786\tValidation Loss: 0.8732538454826831\n",
            "Epoch 141  \tTraining Loss: 0.871799789567081\tValidation Loss: 0.8732473837992747\n",
            "Epoch 142  \tTraining Loss: 0.8717942996113601\tValidation Loss: 0.8732412532308601\n",
            "Epoch 143  \tTraining Loss: 0.8717891134977293\tValidation Loss: 0.8732354361958193\n",
            "Epoch 144  \tTraining Loss: 0.8717842144100205\tValidation Loss: 0.8732299160647083\n",
            "Epoch 145  \tTraining Loss: 0.8717795864627589\tValidation Loss: 0.8732246771081463\n",
            "Epoch 146  \tTraining Loss: 0.8717752146496518\tValidation Loss: 0.8732197044475718\n",
            "Epoch 147  \tTraining Loss: 0.8717710847949315\tValidation Loss: 0.8732149840087072\n",
            "Epoch 148  \tTraining Loss: 0.87176718350739\tValidation Loss: 0.873210502477584\n",
            "Epoch 149  \tTraining Loss: 0.871763498136956\tValidation Loss: 0.873206247258986\n",
            "Epoch 150  \tTraining Loss: 0.8717600167336776\tValidation Loss: 0.8732022064371786\n",
            "Epoch 151  \tTraining Loss: 0.871756728008975\tValidation Loss: 0.8731983687387965\n",
            "Epoch 152  \tTraining Loss: 0.8717536212990348\tValidation Loss: 0.8731947234977723\n",
            "Epoch 153  \tTraining Loss: 0.8717506865302335\tValidation Loss: 0.8731912606221915\n",
            "Epoch 154  \tTraining Loss: 0.871747914186474\tValidation Loss: 0.8731879705629706\n",
            "Epoch 155  \tTraining Loss: 0.8717452952783277\tValidation Loss: 0.8731848442842535\n",
            "Epoch 156  \tTraining Loss: 0.8717428213138865\tValidation Loss: 0.873181873235437\n",
            "Epoch 157  \tTraining Loss: 0.8717404842712282\tValidation Loss: 0.8731790493247297\n",
            "Epoch 158  \tTraining Loss: 0.8717382765724049\tValidation Loss: 0.8731763648941641\n",
            "Epoch 159  \tTraining Loss: 0.87173619105887\tValidation Loss: 0.8731738126959799\n",
            "Epoch 160  \tTraining Loss: 0.8717342209682681\tValidation Loss: 0.8731713858703041\n",
            "Epoch 161  \tTraining Loss: 0.8717323599125066\tValidation Loss: 0.8731690779240538\n",
            "Epoch 162  \tTraining Loss: 0.8717306018570424\tValidation Loss: 0.8731668827109985\n",
            "Epoch 163  \tTraining Loss: 0.8717289411013149\tValidation Loss: 0.8731647944129138\n",
            "Epoch 164  \tTraining Loss: 0.8717273722602615\tValidation Loss: 0.8731628075217694\n",
            "Epoch 165  \tTraining Loss: 0.8717258902468569\tValidation Loss: 0.8731609168228933\n",
            "Epoch 166  \tTraining Loss: 0.8717244902556158\tValidation Loss: 0.873159117379057\n",
            "Epoch 167  \tTraining Loss: 0.8717231677470153\tValidation Loss: 0.8731574045154356\n",
            "Epoch 168  \tTraining Loss: 0.8717219184327715\tValidation Loss: 0.8731557738053877\n",
            "Epoch 169  \tTraining Loss: 0.8717207382619361\tValidation Loss: 0.8731542210570169\n",
            "Epoch 170  \tTraining Loss: 0.8717196234077613\tValidation Loss: 0.8731527423004662\n",
            "Epoch 171  \tTraining Loss: 0.8717185702552901\tValidation Loss: 0.8731513337759101\n",
            "Epoch 172  \tTraining Loss: 0.8717175753896365\tValidation Loss: 0.8731499919222014\n",
            "Epoch 173  \tTraining Loss: 0.8717166355849114\tValidation Loss: 0.8731487133661391\n",
            "Epoch 174  \tTraining Loss: 0.8717157477937628\tValidation Loss: 0.8731474949123242\n",
            "Epoch 175  \tTraining Loss: 0.8717149091374948\tValidation Loss: 0.8731463335335677\n",
            "Epoch 176  \tTraining Loss: 0.8717141168967326\tValidation Loss: 0.8731452263618238\n",
            "Epoch 177  \tTraining Loss: 0.8717133685026064\tValidation Loss: 0.8731441706796179\n",
            "Epoch 178  \tTraining Loss: 0.8717126615284199\tValidation Loss: 0.8731431639119426\n",
            "Epoch 179  \tTraining Loss: 0.8717119936817826\tValidation Loss: 0.8731422036185945\n",
            "Epoch 180  \tTraining Loss: 0.8717113627971778\tValidation Loss: 0.873141287486931\n",
            "Epoch 181  \tTraining Loss: 0.8717107668289372\tValidation Loss: 0.8731404133250196\n",
            "Epoch 182  \tTraining Loss: 0.8717102038446123\tValidation Loss: 0.8731395790551627\n",
            "Epoch 183  \tTraining Loss: 0.8717096720187051\tValidation Loss: 0.8731387827077728\n",
            "Epoch 184  \tTraining Loss: 0.8717091696267504\tValidation Loss: 0.8731380224155831\n",
            "Epoch 185  \tTraining Loss: 0.8717086950397234\tValidation Loss: 0.8731372964081721\n",
            "Epoch 186  \tTraining Loss: 0.8717082467187576\tValidation Loss: 0.8731366030067854\n",
            "Epoch 187  \tTraining Loss: 0.8717078232101557\tValidation Loss: 0.8731359406194408\n",
            "Epoch 188  \tTraining Loss: 0.8717074231406752\tValidation Loss: 0.8731353077362971\n",
            "Epoch 189  \tTraining Loss: 0.871707045213076\tValidation Loss: 0.873134702925276\n",
            "Epoch 190  \tTraining Loss: 0.8717066882019133\tValidation Loss: 0.8731341248279211\n",
            "Epoch 191  \tTraining Loss: 0.8717063509495657\tValidation Loss: 0.8731335721554813\n",
            "Epoch 192  \tTraining Loss: 0.871706032362479\tValidation Loss: 0.8731330436852068\n",
            "Epoch 193  \tTraining Loss: 0.8717057314076232\tValidation Loss: 0.873132538256846\n",
            "Epoch 194  \tTraining Loss: 0.871705447109141\tValidation Loss: 0.8731320547693309\n",
            "Epoch 195  \tTraining Loss: 0.8717051785451834\tValidation Loss: 0.873131592177644\n",
            "Epoch 196  \tTraining Loss: 0.8717049248449221\tValidation Loss: 0.8731311494898525\n",
            "Epoch 197  \tTraining Loss: 0.8717046851857245\tValidation Loss: 0.8731307257643035\n",
            "Epoch 198  \tTraining Loss: 0.8717044587904865\tValidation Loss: 0.873130320106972\n",
            "Epoch 199  \tTraining Loss: 0.8717042449251133\tValidation Loss: 0.8731299316689497\n",
            "Epoch 200  \tTraining Loss: 0.8717040428961382\tValidation Loss: 0.873129559644071\n",
            "Epoch 201  \tTraining Loss: 0.8717038520484748\tValidation Loss: 0.8731292032666667\n",
            "Epoch 202  \tTraining Loss: 0.8717036717632924\tValidation Loss: 0.8731288618094377\n",
            "Epoch 203  \tTraining Loss: 0.8717035014560097\tValidation Loss: 0.8731285345814443\n",
            "Epoch 204  \tTraining Loss: 0.8717033405743984\tValidation Loss: 0.8731282209262036\n",
            "Epoch 205  \tTraining Loss: 0.8717031885967944\tValidation Loss: 0.8731279202198867\n",
            "Epoch 206  \tTraining Loss: 0.8717030450304045\tValidation Loss: 0.8731276318696177\n",
            "Epoch 207  \tTraining Loss: 0.8717029094097093\tValidation Loss: 0.8731273553118585\n",
            "Epoch 208  \tTraining Loss: 0.8717027812949537\tValidation Loss: 0.8731270900108836\n",
            "Epoch 209  \tTraining Loss: 0.8717026602707204\tValidation Loss: 0.8731268354573353\n",
            "Epoch 210  \tTraining Loss: 0.8717025459445844\tValidation Loss: 0.8731265911668568\n",
            "Epoch 211  \tTraining Loss: 0.8717024379458386\tValidation Loss: 0.8731263566787975\n",
            "Epoch 212  \tTraining Loss: 0.8717023359242928\tValidation Loss: 0.8731261315549889\n",
            "Epoch 213  \tTraining Loss: 0.8717022395491386\tValidation Loss: 0.8731259153785838\n",
            "Epoch 214  \tTraining Loss: 0.8717021485078762\tValidation Loss: 0.8731257077529591\n",
            "Epoch 215  \tTraining Loss: 0.8717020625053005\tValidation Loss: 0.8731255083006767\n",
            "Epoch 216  \tTraining Loss: 0.8717019812625448\tValidation Loss: 0.8731253166624984\n",
            "Epoch 217  \tTraining Loss: 0.8717019045161768\tValidation Loss: 0.873125132496455\n",
            "Epoch 218  \tTraining Loss: 0.8717018320173433\tValidation Loss: 0.8731249554769633\n",
            "Epoch 219  \tTraining Loss: 0.8717017635309646\tValidation Loss: 0.8731247852939907\n",
            "Epoch 220  \tTraining Loss: 0.8717016988349703\tValidation Loss: 0.8731246216522641\n",
            "Epoch 221  \tTraining Loss: 0.8717016377195815\tValidation Loss: 0.87312446427052\n",
            "Epoch 222  \tTraining Loss: 0.8717015799866293\tValidation Loss: 0.8731243128807948\n",
            "Epoch 223  \tTraining Loss: 0.871701525448913\tValidation Loss: 0.8731241672277521\n",
            "Epoch 224  \tTraining Loss: 0.8717014739295911\tValidation Loss: 0.8731240270680464\n",
            "Epoch 225  \tTraining Loss: 0.871701425261611\tValidation Loss: 0.8731238921697181\n",
            "Epoch 226  \tTraining Loss: 0.8717013792871641\tValidation Loss: 0.8731237623116235\n",
            "Epoch 227  \tTraining Loss: 0.8717013358571775\tValidation Loss: 0.8731236372828919\n",
            "Epoch 228  \tTraining Loss: 0.8717012948308271\tValidation Loss: 0.8731235168824122\n",
            "Epoch 229  \tTraining Loss: 0.8717012560750839\tValidation Loss: 0.8731234009183468\n",
            "Epoch 230  \tTraining Loss: 0.8717012194642807\tValidation Loss: 0.8731232892076706\n",
            "Epoch 231  \tTraining Loss: 0.8717011848797057\tValidation Loss: 0.8731231815757332\n",
            "Epoch 232  \tTraining Loss: 0.8717011522092171\tValidation Loss: 0.8731230778558453\n",
            "Epoch 233  \tTraining Loss: 0.8717011213468795\tValidation Loss: 0.873122977888886\n",
            "Epoch 234  \tTraining Loss: 0.8717010921926209\tValidation Loss: 0.8731228815229299\n",
            "Epoch 235  \tTraining Loss: 0.8717010646519074\tValidation Loss: 0.873122788612894\n",
            "Epoch 236  \tTraining Loss: 0.8717010386354369\tValidation Loss: 0.8731226990202041\n",
            "Epoch 237  \tTraining Loss: 0.8717010140588504\tValidation Loss: 0.8731226126124759\n",
            "Epoch 238  \tTraining Loss: 0.8717009908424574\tValidation Loss: 0.8731225292632148\n",
            "Epoch 239  \tTraining Loss: 0.8717009689109776\tValidation Loss: 0.8731224488515298\n",
            "Epoch 240  \tTraining Loss: 0.8717009481932976\tValidation Loss: 0.8731223712618629\n",
            "Epoch 241  \tTraining Loss: 0.8717009286222395\tValidation Loss: 0.8731222963837316\n",
            "Epoch 242  \tTraining Loss: 0.8717009101343433\tValidation Loss: 0.8731222241114859\n",
            "Epoch 243  \tTraining Loss: 0.8717008926696612\tValidation Loss: 0.8731221543440759\n",
            "Epoch 244  \tTraining Loss: 0.8717008761715636\tValidation Loss: 0.8731220869848328\n",
            "Epoch 245  \tTraining Loss: 0.8717008605865547\tValidation Loss: 0.8731220219412601\n",
            "Epoch 246  \tTraining Loss: 0.8717008458640997\tValidation Loss: 0.873121959124836\n",
            "Epoch 247  \tTraining Loss: 0.87170083195646\tValidation Loss: 0.8731218984508252\n",
            "Epoch 248  \tTraining Loss: 0.8717008188185399\tValidation Loss: 0.8731218398381011\n",
            "Epoch 249  \tTraining Loss: 0.8717008064077393\tValidation Loss: 0.8731217832089756\n",
            "Epoch 250  \tTraining Loss: 0.8717007946838152\tValidation Loss: 0.8731217284890395\n",
            "Epoch 251  \tTraining Loss: 0.8717007836087532\tValidation Loss: 0.8731216756070085\n",
            "Epoch 252  \tTraining Loss: 0.8717007731466413\tValidation Loss: 0.8731216244945786\n",
            "Epoch 253  \tTraining Loss: 0.8717007632635558\tValidation Loss: 0.8731215750862884\n",
            "Epoch 254  \tTraining Loss: 0.8717007539274508\tValidation Loss: 0.8731215273193882\n",
            "Epoch 255  \tTraining Loss: 0.871700745108053\tValidation Loss: 0.8731214811337146\n",
            "Epoch 256  \tTraining Loss: 0.871700736776766\tValidation Loss: 0.8731214364715735\n",
            "Epoch 257  \tTraining Loss: 0.8717007289065747\tValidation Loss: 0.8731213932776267\n",
            "Epoch 258  \tTraining Loss: 0.8717007214719598\tValidation Loss: 0.8731213514987852\n",
            "Epoch 259  \tTraining Loss: 0.8717007144488143\tValidation Loss: 0.873121311084108\n",
            "Epoch 260  \tTraining Loss: 0.8717007078143653\tValidation Loss: 0.8731212719847042\n",
            "Epoch 261  \tTraining Loss: 0.8717007015471006\tValidation Loss: 0.8731212341536431\n",
            "Epoch 262  \tTraining Loss: 0.8717006956266983\tValidation Loss: 0.8731211975458647\n",
            "Epoch 263  \tTraining Loss: 0.8717006900339611\tValidation Loss: 0.8731211621180976\n",
            "Epoch 264  \tTraining Loss: 0.8717006847507544\tValidation Loss: 0.8731211278287794\n",
            "Epoch 265  \tTraining Loss: 0.8717006797599473\tValidation Loss: 0.8731210946379819\n",
            "Epoch 266  \tTraining Loss: 0.8717006750453568\tValidation Loss: 0.8731210625073386\n",
            "Epoch 267  \tTraining Loss: 0.8717006705916959\tValidation Loss: 0.8731210313999771\n",
            "Epoch 268  \tTraining Loss: 0.8717006663845233\tValidation Loss: 0.8731210012804534\n",
            "Epoch 269  \tTraining Loss: 0.8717006624101967\tValidation Loss: 0.8731209721146911\n",
            "Epoch 270  \tTraining Loss: 0.8717006586558296\tValidation Loss: 0.8731209438699209\n",
            "Epoch 271  \tTraining Loss: 0.8717006551092487\tValidation Loss: 0.873120916514626\n",
            "Epoch 272  \tTraining Loss: 0.8717006517589534\tValidation Loss: 0.8731208900184877\n",
            "Epoch 273  \tTraining Loss: 0.8717006485940803\tValidation Loss: 0.873120864352334\n",
            "Epoch 274  \tTraining Loss: 0.8717006456043674\tValidation Loss: 0.8731208394880929\n",
            "Epoch 275  \tTraining Loss: 0.8717006427801207\tValidation Loss: 0.8731208153987432\n",
            "Epoch 276  \tTraining Loss: 0.871700640112182\tValidation Loss: 0.8731207920582729\n",
            "Epoch 277  \tTraining Loss: 0.8717006375919009\tValidation Loss: 0.8731207694416355\n",
            "Epoch 278  \tTraining Loss: 0.8717006352111046\tValidation Loss: 0.8731207475247109\n",
            "Epoch 279  \tTraining Loss: 0.8717006329620738\tValidation Loss: 0.8731207262842661\n",
            "Epoch 280  \tTraining Loss: 0.8717006308375159\tValidation Loss: 0.8731207056979197\n",
            "Epoch 281  \tTraining Loss: 0.8717006288305418\tValidation Loss: 0.8731206857441053\n",
            "Epoch 282  \tTraining Loss: 0.8717006269346437\tValidation Loss: 0.8731206664020407\n",
            "Epoch 283  \tTraining Loss: 0.8717006251436744\tValidation Loss: 0.8731206476516934\n",
            "Epoch 284  \tTraining Loss: 0.8717006234518266\tValidation Loss: 0.8731206294737525\n",
            "Epoch 285  \tTraining Loss: 0.8717006218536141\tValidation Loss: 0.8731206118495981\n",
            "Epoch 286  \tTraining Loss: 0.8717006203438548\tValidation Loss: 0.8731205947612747\n",
            "Epoch 287  \tTraining Loss: 0.8717006189176533\tValidation Loss: 0.8731205781914639\n",
            "Epoch 288  \tTraining Loss: 0.871700617570385\tValidation Loss: 0.87312056212346\n",
            "Epoch 289  \tTraining Loss: 0.8717006162976814\tValidation Loss: 0.8731205465411449\n",
            "Epoch 290  \tTraining Loss: 0.8717006150954159\tValidation Loss: 0.8731205314289655\n",
            "Epoch 291  \tTraining Loss: 0.8717006139596898\tValidation Loss: 0.8731205167719117\n",
            "Epoch 292  \tTraining Loss: 0.8717006128868202\tValidation Loss: 0.8731205025554954\n",
            "Epoch 293  \tTraining Loss: 0.8717006118733291\tValidation Loss: 0.8731204887657295\n",
            "Epoch 294  \tTraining Loss: 0.8717006109159297\tValidation Loss: 0.8731204753891092\n",
            "Epoch 295  \tTraining Loss: 0.8717006100115172\tValidation Loss: 0.8731204624125938\n",
            "Epoch 296  \tTraining Loss: 0.8717006091571601\tValidation Loss: 0.8731204498235884\n",
            "Epoch 297  \tTraining Loss: 0.8717006083500872\tValidation Loss: 0.873120437609927\n",
            "Epoch 298  \tTraining Loss: 0.871700607587682\tValidation Loss: 0.8731204257598565\n",
            "Epoch 299  \tTraining Loss: 0.8717006068674718\tValidation Loss: 0.8731204142620215\n",
            "Epoch 300  \tTraining Loss: 0.871700606187122\tValidation Loss: 0.873120403105448\n",
            "Epoch 301  \tTraining Loss: 0.8717006055444261\tValidation Loss: 0.8731203922795316\n",
            "Epoch 302  \tTraining Loss: 0.8717006049373004\tValidation Loss: 0.8731203817740215\n",
            "Epoch 303  \tTraining Loss: 0.871700604363776\tValidation Loss: 0.8731203715790081\n",
            "Epoch 304  \tTraining Loss: 0.8717006038219932\tValidation Loss: 0.8731203616849114\n",
            "Epoch 305  \tTraining Loss: 0.8717006033101957\tValidation Loss: 0.8731203520824674\n",
            "Epoch 306  \tTraining Loss: 0.8717006028267237\tValidation Loss: 0.8731203427627179\n",
            "Epoch 307  \tTraining Loss: 0.8717006023700093\tValidation Loss: 0.873120333716999\n",
            "Epoch 308  \tTraining Loss: 0.8717006019385719\tValidation Loss: 0.8731203249369299\n",
            "Epoch 309  \tTraining Loss: 0.8717006015310124\tValidation Loss: 0.8731203164144036\n",
            "Epoch 310  \tTraining Loss: 0.8717006011460092\tValidation Loss: 0.8731203081415763\n",
            "Epoch 311  \tTraining Loss: 0.8717006007823143\tValidation Loss: 0.8731203001108585\n",
            "Epoch 312  \tTraining Loss: 0.8717006004387482\tValidation Loss: 0.8731202923149065\n",
            "Epoch 313  \tTraining Loss: 0.8717006001141966\tValidation Loss: 0.8731202847466121\n",
            "Epoch 314  \tTraining Loss: 0.8717005998076073\tValidation Loss: 0.8731202773990955\n",
            "Epoch 315  \tTraining Loss: 0.8717005995179861\tValidation Loss: 0.873120270265698\n",
            "Epoch 316  \tTraining Loss: 0.8717005992443941\tValidation Loss: 0.8731202633399726\n",
            "Epoch 317  \tTraining Loss: 0.8717005989859443\tValidation Loss: 0.8731202566156775\n",
            "Epoch 318  \tTraining Loss: 0.8717005987417983\tValidation Loss: 0.8731202500867692\n",
            "Epoch 319  \tTraining Loss: 0.8717005985111645\tValidation Loss: 0.8731202437473955\n",
            "Epoch 320  \tTraining Loss: 0.8717005982932953\tValidation Loss: 0.8731202375918882\n",
            "Epoch 321  \tTraining Loss: 0.8717005980874838\tValidation Loss: 0.8731202316147579\n",
            "Epoch 322  \tTraining Loss: 0.8717005978930632\tValidation Loss: 0.8731202258106883\n",
            "Epoch 323  \tTraining Loss: 0.8717005977094029\tValidation Loss: 0.8731202201745276\n",
            "Epoch 324  \tTraining Loss: 0.8717005975359069\tValidation Loss: 0.8731202147012868\n",
            "Epoch 325  \tTraining Loss: 0.8717005973720132\tValidation Loss: 0.8731202093861307\n",
            "Epoch 326  \tTraining Loss: 0.8717005972171904\tValidation Loss: 0.8731202042243753\n",
            "Epoch 327  \tTraining Loss: 0.8717005970709365\tValidation Loss: 0.8731201992114817\n",
            "Epoch 328  \tTraining Loss: 0.8717005969327767\tValidation Loss: 0.8731201943430505\n",
            "Epoch 329  \tTraining Loss: 0.8717005968022634\tValidation Loss: 0.8731201896148191\n",
            "Epoch 330  \tTraining Loss: 0.8717005966789731\tValidation Loss: 0.8731201850226553\n",
            "Epoch 331  \tTraining Loss: 0.8717005965625066\tValidation Loss: 0.8731201805625539\n",
            "Epoch 332  \tTraining Loss: 0.8717005964524861\tValidation Loss: 0.8731201762306332\n",
            "Epoch 333  \tTraining Loss: 0.8717005963485545\tValidation Loss: 0.8731201720231286\n",
            "Epoch 334  \tTraining Loss: 0.8717005962503752\tValidation Loss: 0.8731201679363919\n",
            "Epoch 335  \tTraining Loss: 0.8717005961576293\tValidation Loss: 0.8731201639668853\n",
            "Epoch 336  \tTraining Loss: 0.8717005960700167\tValidation Loss: 0.8731201601111784\n",
            "Epoch 337  \tTraining Loss: 0.8717005959872528\tValidation Loss: 0.8731201563659449\n",
            "Epoch 338  \tTraining Loss: 0.8717005959090697\tValidation Loss: 0.8731201527279595\n",
            "Epoch 339  \tTraining Loss: 0.8717005958352135\tValidation Loss: 0.873120149194094\n",
            "Epoch 340  \tTraining Loss: 0.8717005957654449\tValidation Loss: 0.8731201457613145\n",
            "Epoch 341  \tTraining Loss: 0.871700595699538\tValidation Loss: 0.8731201424266787\n",
            "Epoch 342  \tTraining Loss: 0.871700595637278\tValidation Loss: 0.8731201391873324\n",
            "Epoch 343  \tTraining Loss: 0.8717005955784644\tValidation Loss: 0.8731201360405074\n",
            "Epoch 344  \tTraining Loss: 0.8717005955229056\tValidation Loss: 0.8731201329835181\n",
            "Epoch 345  \tTraining Loss: 0.8717005954704218\tValidation Loss: 0.8731201300137589\n",
            "Epoch 346  \tTraining Loss: 0.8717005954208427\tValidation Loss: 0.8731201271287029\n",
            "Epoch 347  \tTraining Loss: 0.8717005953740077\tValidation Loss: 0.8731201243258977\n",
            "Epoch 348  \tTraining Loss: 0.8717005953297644\tValidation Loss: 0.8731201216029638\n",
            "Epoch 349  \tTraining Loss: 0.8717005952879701\tValidation Loss: 0.873120118957593\n",
            "Epoch 350  \tTraining Loss: 0.8717005952484886\tValidation Loss: 0.8731201163875454\n",
            "Epoch 351  \tTraining Loss: 0.8717005952111926\tValidation Loss: 0.8731201138906473\n",
            "Epoch 352  \tTraining Loss: 0.8717005951759604\tValidation Loss: 0.873120111464789\n",
            "Epoch 353  \tTraining Loss: 0.8717005951426784\tValidation Loss: 0.8731201091079238\n",
            "Epoch 354  \tTraining Loss: 0.8717005951112382\tValidation Loss: 0.873120106818065\n",
            "Epoch 355  \tTraining Loss: 0.871700595081538\tValidation Loss: 0.8731201045932845\n",
            "Epoch 356  \tTraining Loss: 0.8717005950534819\tValidation Loss: 0.8731201024317108\n",
            "Epoch 357  \tTraining Loss: 0.8717005950269782\tValidation Loss: 0.8731201003315278\n",
            "Epoch 358  \tTraining Loss: 0.8717005950019414\tValidation Loss: 0.8731200982909724\n",
            "Epoch 359  \tTraining Loss: 0.8717005949782906\tValidation Loss: 0.8731200963083331\n",
            "Epoch 360  \tTraining Loss: 0.8717005949559483\tValidation Loss: 0.8731200943819489\n",
            "Epoch 361  \tTraining Loss: 0.8717005949348428\tValidation Loss: 0.8731200925102071\n",
            "Epoch 362  \tTraining Loss: 0.8717005949149053\tValidation Loss: 0.8731200906915423\n",
            "Epoch 363  \tTraining Loss: 0.8717005948960713\tValidation Loss: 0.8731200889244346\n",
            "Epoch 364  \tTraining Loss: 0.8717005948782797\tValidation Loss: 0.8731200872074085\n",
            "Epoch 365  \tTraining Loss: 0.8717005948614727\tValidation Loss: 0.8731200855390318\n",
            "Epoch 366  \tTraining Loss: 0.8717005948455959\tValidation Loss: 0.8731200839179133\n",
            "Epoch 367  \tTraining Loss: 0.8717005948305977\tValidation Loss: 0.8731200823427031\n",
            "Epoch 368  \tTraining Loss: 0.8717005948164298\tValidation Loss: 0.8731200808120898\n",
            "Epoch 369  \tTraining Loss: 0.871700594803046\tValidation Loss: 0.8731200793248004\n",
            "Epoch 370  \tTraining Loss: 0.8717005947904027\tValidation Loss: 0.8731200778795988\n",
            "Epoch 371  \tTraining Loss: 0.8717005947784593\tValidation Loss: 0.8731200764752847\n",
            "Epoch 372  \tTraining Loss: 0.8717005947671771\tValidation Loss: 0.8731200751106923\n",
            "Epoch 373  \tTraining Loss: 0.8717005947565192\tValidation Loss: 0.8731200737846898\n",
            "Epoch 374  \tTraining Loss: 0.8717005947464509\tValidation Loss: 0.8731200724961781\n",
            "Epoch 375  \tTraining Loss: 0.87170059473694\tValidation Loss: 0.8731200712440896\n",
            "Epoch 376  \tTraining Loss: 0.8717005947279555\tValidation Loss: 0.8731200700273882\n",
            "Epoch 377  \tTraining Loss: 0.8717005947194681\tValidation Loss: 0.8731200688450665\n",
            "Epoch 378  \tTraining Loss: 0.8717005947114507\tValidation Loss: 0.873120067696147\n",
            "Epoch 379  \tTraining Loss: 0.8717005947038768\tValidation Loss: 0.8731200665796804\n",
            "Epoch 380  \tTraining Loss: 0.8717005946967223\tValidation Loss: 0.873120065494744\n",
            "Epoch 381  \tTraining Loss: 0.8717005946899635\tValidation Loss: 0.8731200644404421\n",
            "Epoch 382  \tTraining Loss: 0.871700594683579\tValidation Loss: 0.873120063415905\n",
            "Epoch 383  \tTraining Loss: 0.8717005946775478\tValidation Loss: 0.8731200624202873\n",
            "Epoch 384  \tTraining Loss: 0.8717005946718503\tValidation Loss: 0.8731200614527688\n",
            "Epoch 385  \tTraining Loss: 0.8717005946664681\tValidation Loss: 0.8731200605125514\n",
            "Epoch 386  \tTraining Loss: 0.8717005946613838\tValidation Loss: 0.8731200595988617\n",
            "Epoch 387  \tTraining Loss: 0.8717005946565812\tValidation Loss: 0.8731200587109467\n",
            "Epoch 388  \tTraining Loss: 0.8717005946520442\tValidation Loss: 0.8731200578480762\n",
            "Epoch 389  \tTraining Loss: 0.8717005946477583\tValidation Loss: 0.8731200570095401\n",
            "Epoch 390  \tTraining Loss: 0.8717005946437093\tValidation Loss: 0.8731200561946493\n",
            "Epoch 391  \tTraining Loss: 0.8717005946398847\tValidation Loss: 0.8731200554027339\n",
            "Epoch 392  \tTraining Loss: 0.8717005946362715\tValidation Loss: 0.8731200546331431\n",
            "Epoch 393  \tTraining Loss: 0.8717005946328586\tValidation Loss: 0.8731200538852452\n",
            "Epoch 394  \tTraining Loss: 0.8717005946296347\tValidation Loss: 0.8731200531584259\n",
            "Epoch 395  \tTraining Loss: 0.8717005946265889\tValidation Loss: 0.8731200524520891\n",
            "Epoch 396  \tTraining Loss: 0.8717005946237119\tValidation Loss: 0.8731200517656551\n",
            "Epoch 397  \tTraining Loss: 0.8717005946209939\tValidation Loss: 0.8731200510985608\n",
            "Epoch 398  \tTraining Loss: 0.8717005946184264\tValidation Loss: 0.8731200504502595\n",
            "Epoch 399  \tTraining Loss: 0.8717005946160011\tValidation Loss: 0.8731200498202201\n",
            "Epoch 400  \tTraining Loss: 0.8717005946137097\tValidation Loss: 0.8731200492079257\n",
            "lr, batch_size: (0.001, 200)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.6243534526955732\n",
            "Epoch 2  \tTraining Loss: 0.6211419364869208\tValidation Loss: 0.8706964125768494\n",
            "Epoch 3  \tTraining Loss: 0.8652956414574802\tValidation Loss: 0.6080005424691387\n",
            "Epoch 4  \tTraining Loss: 0.6057258164438752\tValidation Loss: 0.6340736540992257\n",
            "Epoch 5  \tTraining Loss: 0.6252237430173914\tValidation Loss: 0.8651971395052276\n",
            "Epoch 6  \tTraining Loss: 0.8631873101975883\tValidation Loss: 0.5740676554933143\n",
            "Epoch 7  \tTraining Loss: 0.5681604855792008\tValidation Loss: 0.5709372474182237\n",
            "Epoch 8  \tTraining Loss: 0.5578663203610648\tValidation Loss: 0.6604647027530745\n",
            "Epoch 9  \tTraining Loss: 0.6453815262912882\tValidation Loss: 8.510887002784191\n",
            "Epoch 10  \tTraining Loss: 8.554905179409724\tValidation Loss: 0.9342282716618441\n",
            "Epoch 11  \tTraining Loss: 0.9311494197660802\tValidation Loss: 0.9321786454884309\n",
            "Epoch 12  \tTraining Loss: 0.9291057032192446\tValidation Loss: 0.9282034960205785\n",
            "Epoch 13  \tTraining Loss: 0.9249627130767325\tValidation Loss: 0.9063776912725549\n",
            "Epoch 14  \tTraining Loss: 0.9025341770683879\tValidation Loss: 0.7597925493872184\n",
            "Epoch 15  \tTraining Loss: 0.7561155980652636\tValidation Loss: 0.8569698807397439\n",
            "Epoch 16  \tTraining Loss: 0.8620547787858478\tValidation Loss: 0.9301583208540838\n",
            "Epoch 17  \tTraining Loss: 0.9270153094757775\tValidation Loss: 0.9281158515036863\n",
            "Epoch 18  \tTraining Loss: 0.9250051223871008\tValidation Loss: 0.9245802837732365\n",
            "Epoch 19  \tTraining Loss: 0.9214338045315198\tValidation Loss: 0.9101549180178962\n",
            "Epoch 20  \tTraining Loss: 0.9068119994847061\tValidation Loss: 0.765848939127548\n",
            "Epoch 21  \tTraining Loss: 0.7638876288693668\tValidation Loss: 1.2008148593786083\n",
            "Epoch 22  \tTraining Loss: 1.20506386086261\tValidation Loss: 0.9238496145517081\n",
            "Epoch 23  \tTraining Loss: 0.9208215042376782\tValidation Loss: 0.9073228248763365\n",
            "Epoch 24  \tTraining Loss: 0.9041775590340153\tValidation Loss: 0.7271733403852984\n",
            "Epoch 25  \tTraining Loss: 0.7237622127170513\tValidation Loss: 1.3020207933124877\n",
            "Epoch 26  \tTraining Loss: 1.301939602883523\tValidation Loss: 0.923379957497202\n",
            "Epoch 27  \tTraining Loss: 0.9203567467873853\tValidation Loss: 0.9216504099658089\n",
            "Epoch 28  \tTraining Loss: 0.9186392350714089\tValidation Loss: 0.9199305235425486\n",
            "Epoch 29  \tTraining Loss: 0.9169313507025068\tValidation Loss: 0.9182202441866457\n",
            "Epoch 30  \tTraining Loss: 0.9152330397155088\tValidation Loss: 0.9165195181408842\n",
            "Epoch 31  \tTraining Loss: 0.9135442484477425\tValidation Loss: 0.9148282919491134\n",
            "Epoch 32  \tTraining Loss: 0.9118649235373344\tValidation Loss: 0.9131465124545632\n",
            "Epoch 33  \tTraining Loss: 0.9101950119215273\tValidation Loss: 0.9114741267981648\n",
            "Epoch 34  \tTraining Loss: 0.9085344608350017\tValidation Loss: 0.9098110824168839\n",
            "Epoch 35  \tTraining Loss: 0.9068832178082096\tValidation Loss: 0.9081573270420613\n",
            "Epoch 36  \tTraining Loss: 0.905241230665715\tValidation Loss: 0.9065128086977641\n",
            "Epoch 37  \tTraining Loss: 0.9036084475245472\tValidation Loss: 0.904877475699145\n",
            "Epoch 38  \tTraining Loss: 0.9019848167925602\tValidation Loss: 0.9032512766508122\n",
            "Epoch 39  \tTraining Loss: 0.9003702871668022\tValidation Loss: 0.9016341604452063\n",
            "Epoch 40  \tTraining Loss: 0.8987648076318966\tValidation Loss: 0.9000260762609895\n",
            "Epoch 41  \tTraining Loss: 0.8971683274584276\tValidation Loss: 0.89842697356144\n",
            "Epoch 42  \tTraining Loss: 0.895580796201338\tValidation Loss: 0.8968368020928583\n",
            "Epoch 43  \tTraining Loss: 0.8940021636983363\tValidation Loss: 0.8952555118829819\n",
            "Epoch 44  \tTraining Loss: 0.8924323800683104\tValidation Loss: 0.8936830532394076\n",
            "Epoch 45  \tTraining Loss: 0.8908713957097525\tValidation Loss: 0.8921193767480233\n",
            "Epoch 46  \tTraining Loss: 0.8893191612991906\tValidation Loss: 0.8905644332714502\n",
            "Epoch 47  \tTraining Loss: 0.8877756277896311\tValidation Loss: 0.8890181739474907\n",
            "Epoch 48  \tTraining Loss: 0.8862407464090087\tValidation Loss: 0.8874805501875878\n",
            "Epoch 49  \tTraining Loss: 0.884714468658645\tValidation Loss: 0.8859515136752909\n",
            "Epoch 50  \tTraining Loss: 0.8831967463117163\tValidation Loss: 0.884431016364732\n",
            "Epoch 51  \tTraining Loss: 0.8816875314117302\tValidation Loss: 0.8829190104791098\n",
            "Epoch 52  \tTraining Loss: 0.8801867762710086\tValidation Loss: 0.8814154485091813\n",
            "Epoch 53  \tTraining Loss: 0.8786944334691841\tValidation Loss: 0.8799202832117639\n",
            "Epoch 54  \tTraining Loss: 0.8772104558516973\tValidation Loss: 0.8784334676082435\n",
            "Epoch 55  \tTraining Loss: 0.8757347965283108\tValidation Loss: 0.8769549549830928\n",
            "Epoch 56  \tTraining Loss: 0.8742674088716254\tValidation Loss: 0.8754846988823977\n",
            "Epoch 57  \tTraining Loss: 0.8728082465156076\tValidation Loss: 0.8740226531123906\n",
            "Epoch 58  \tTraining Loss: 0.8713572633541237\tValidation Loss: 0.872568771737994\n",
            "Epoch 59  \tTraining Loss: 0.8699144135394834\tValidation Loss: 0.8711230090813695\n",
            "Epoch 60  \tTraining Loss: 0.8684796514809922\tValidation Loss: 0.8696853197204786\n",
            "Epoch 61  \tTraining Loss: 0.8670529318435095\tValidation Loss: 0.868255658487648\n",
            "Epoch 62  \tTraining Loss: 0.8656342095460153\tValidation Loss: 0.8668339804681445\n",
            "Epoch 63  \tTraining Loss: 0.864223439760189\tValidation Loss: 0.8654202409987594\n",
            "Epoch 64  \tTraining Loss: 0.8628205779089888\tValidation Loss: 0.8640143956663967\n",
            "Epoch 65  \tTraining Loss: 0.8614255796652468\tValidation Loss: 0.8626164003066747\n",
            "Epoch 66  \tTraining Loss: 0.860038400950266\tValidation Loss: 0.8612262110025302\n",
            "Epoch 67  \tTraining Loss: 0.8586589979324282\tValidation Loss: 0.8598437840828338\n",
            "Epoch 68  \tTraining Loss: 0.8572873270258098\tValidation Loss: 0.8584690761210124\n",
            "Epoch 69  \tTraining Loss: 0.8559233448888031\tValidation Loss: 0.8571020439336782\n",
            "Epoch 70  \tTraining Loss: 0.8545670084227482\tValidation Loss: 0.8557426445792667\n",
            "Epoch 71  \tTraining Loss: 0.8532182747705707\tValidation Loss: 0.8543908353566817\n",
            "Epoch 72  \tTraining Loss: 0.8518771013154265\tValidation Loss: 0.8530465738039484\n",
            "Epoch 73  \tTraining Loss: 0.8505434456793572\tValidation Loss: 0.8517098176968728\n",
            "Epoch 74  \tTraining Loss: 0.8492172657219497\tValidation Loss: 0.8503805250477108\n",
            "Epoch 75  \tTraining Loss: 0.8478985195390043\tValidation Loss: 0.8490586541038424\n",
            "Epoch 76  \tTraining Loss: 0.8465871654612124\tValidation Loss: 0.8477441633464543\n",
            "Epoch 77  \tTraining Loss: 0.8452831620528375\tValidation Loss: 0.846437011489231\n",
            "Epoch 78  \tTraining Loss: 0.843986468110408\tValidation Loss: 0.8451371574770516\n",
            "Epoch 79  \tTraining Loss: 0.8426970426614138\tValidation Loss: 0.8438445604846941\n",
            "Epoch 80  \tTraining Loss: 0.841414844963013\tValidation Loss: 0.8425591799155485\n",
            "Epoch 81  \tTraining Loss: 0.840139834500743\tValidation Loss: 0.8412809754003345\n",
            "Epoch 82  \tTraining Loss: 0.8388719709872421\tValidation Loss: 0.8400099067958291\n",
            "Epoch 83  \tTraining Loss: 0.8376112143609743\tValidation Loss: 0.8387459341835997\n",
            "Epoch 84  \tTraining Loss: 0.8363575247849656\tValidation Loss: 0.8374890178687442\n",
            "Epoch 85  \tTraining Loss: 0.8351108626455442\tValidation Loss: 0.83623911837864\n",
            "Epoch 86  \tTraining Loss: 0.8338711885510885\tValidation Loss: 0.8349961964616968\n",
            "Epoch 87  \tTraining Loss: 0.8326384633307837\tValidation Loss: 0.8337602130861199\n",
            "Epoch 88  \tTraining Loss: 0.8314126480333824\tValidation Loss: 0.8325311294386779\n",
            "Epoch 89  \tTraining Loss: 0.8301937039259745\tValidation Loss: 0.8313089069234786\n",
            "Epoch 90  \tTraining Loss: 0.8289815924927648\tValidation Loss: 0.8300935071607509\n",
            "Epoch 91  \tTraining Loss: 0.8277762754338537\tValidation Loss: 0.8288848919856346\n",
            "Epoch 92  \tTraining Loss: 0.8265777146640277\tValidation Loss: 0.8276830234469758\n",
            "Epoch 93  \tTraining Loss: 0.8253858723115574\tValidation Loss: 0.8264878638061303\n",
            "Epoch 94  \tTraining Loss: 0.8242007107169989\tValidation Loss: 0.8252993755357718\n",
            "Epoch 95  \tTraining Loss: 0.823022192432005\tValidation Loss: 0.8241175213187097\n",
            "Epoch 96  \tTraining Loss: 0.8218502802181415\tValidation Loss: 0.8229422640467103\n",
            "Epoch 97  \tTraining Loss: 0.8206849370457109\tValidation Loss: 0.8217735668193266\n",
            "Epoch 98  \tTraining Loss: 0.819526126092582\tValidation Loss: 0.8206113929427351\n",
            "Epoch 99  \tTraining Loss: 0.818373810743026\tValidation Loss: 0.8194557059285766\n",
            "Epoch 100  \tTraining Loss: 0.8172279545865611\tValidation Loss: 0.8183064694928064\n",
            "Epoch 101  \tTraining Loss: 0.8160885214168001\tValidation Loss: 0.8171636475545492\n",
            "Epoch 102  \tTraining Loss: 0.8149554752303074\tValidation Loss: 0.8160272042349606\n",
            "Epoch 103  \tTraining Loss: 0.8138287802254619\tValidation Loss: 0.8148971038560955\n",
            "Epoch 104  \tTraining Loss: 0.8127084008013236\tValidation Loss: 0.8137733109397829\n",
            "Epoch 105  \tTraining Loss: 0.8115943015565118\tValidation Loss: 0.812655790206506\n",
            "Epoch 106  \tTraining Loss: 0.8104864472880837\tValidation Loss: 0.8115445065742901\n",
            "Epoch 107  \tTraining Loss: 0.8093848029904237\tValidation Loss: 0.8104394251575948\n",
            "Epoch 108  \tTraining Loss: 0.8082893338541367\tValidation Loss: 0.8093405112662146\n",
            "Epoch 109  \tTraining Loss: 0.8072000052649481\tValidation Loss: 0.8082477304041839\n",
            "Epoch 110  \tTraining Loss: 0.8061167828026107\tValidation Loss: 0.8071610482686888\n",
            "Epoch 111  \tTraining Loss: 0.8050396322398167\tValidation Loss: 0.806080430748986\n",
            "Epoch 112  \tTraining Loss: 0.8039685195411157\tValidation Loss: 0.8050058439253254\n",
            "Epoch 113  \tTraining Loss: 0.8029034108618404\tValidation Loss: 0.8039372540678803\n",
            "Epoch 114  \tTraining Loss: 0.8018442725470356\tValidation Loss: 0.8028746276356844\n",
            "Epoch 115  \tTraining Loss: 0.8007910711303967\tValidation Loss: 0.801817931275573\n",
            "Epoch 116  \tTraining Loss: 0.7997437733332109\tValidation Loss: 0.8007671318211305\n",
            "Epoch 117  \tTraining Loss: 0.7987023460633059\tValidation Loss: 0.7997221962916454\n",
            "Epoch 118  \tTraining Loss: 0.7976667564140045\tValidation Loss: 0.7986830918910687\n",
            "Epoch 119  \tTraining Loss: 0.7966369716630846\tValidation Loss: 0.79764978600698\n",
            "Epoch 120  \tTraining Loss: 0.7956129592717458\tValidation Loss: 0.7966222462095588\n",
            "Epoch 121  \tTraining Loss: 0.7945946868835801\tValidation Loss: 0.7956004402505611\n",
            "Epoch 122  \tTraining Loss: 0.793582122323551\tValidation Loss: 0.7945843360623027\n",
            "Epoch 123  \tTraining Loss: 0.7925752335969755\tValidation Loss: 0.7935739017566471\n",
            "Epoch 124  \tTraining Loss: 0.7915739888885143\tValidation Loss: 0.7925691056240004\n",
            "Epoch 125  \tTraining Loss: 0.7905783565611648\tValidation Loss: 0.7915699161323105\n",
            "Epoch 126  \tTraining Loss: 0.7895883051552642\tValidation Loss: 0.7905763019260721\n",
            "Epoch 127  \tTraining Loss: 0.7886038033874927\tValidation Loss: 0.7895882318253388\n",
            "Epoch 128  \tTraining Loss: 0.7876248201498862\tValidation Loss: 0.7886056748247384\n",
            "Epoch 129  \tTraining Loss: 0.7866513245088539\tValidation Loss: 0.7876286000924959\n",
            "Epoch 130  \tTraining Loss: 0.7856832857041998\tValidation Loss: 0.7866569769694604\n",
            "Epoch 131  \tTraining Loss: 0.784720673148151\tValidation Loss: 0.7856907749681376\n",
            "Epoch 132  \tTraining Loss: 0.7837634564243922\tValidation Loss: 0.7847299637717298\n",
            "Epoch 133  \tTraining Loss: 0.782811605287103\tValidation Loss: 0.7837745132331773\n",
            "Epoch 134  \tTraining Loss: 0.7818650896600042\tValidation Loss: 0.78282439337421\n",
            "Epoch 135  \tTraining Loss: 0.7809238796354045\tValidation Loss: 0.7818795743843996\n",
            "Epoch 136  \tTraining Loss: 0.7799879454732592\tValidation Loss: 0.7809400266202207\n",
            "Epoch 137  \tTraining Loss: 0.7790572576002271\tValidation Loss: 0.7800057206041152\n",
            "Epoch 138  \tTraining Loss: 0.7781317866087385\tValidation Loss: 0.7790766270235635\n",
            "Epoch 139  \tTraining Loss: 0.7772115032560645\tValidation Loss: 0.7781527167301581\n",
            "Epoch 140  \tTraining Loss: 0.7762963784633937\tValidation Loss: 0.7772339607386859\n",
            "Epoch 141  \tTraining Loss: 0.775386383314913\tValidation Loss: 0.7763203302262134\n",
            "Epoch 142  \tTraining Loss: 0.7744814890568942\tValidation Loss: 0.7754117965311776\n",
            "Epoch 143  \tTraining Loss: 0.7735816670967858\tValidation Loss: 0.7745083311524816\n",
            "Epoch 144  \tTraining Loss: 0.7726868890023083\tValidation Loss: 0.773609905748597\n",
            "Epoch 145  \tTraining Loss: 0.7717971265005573\tValidation Loss: 0.7727164921366677\n",
            "Epoch 146  \tTraining Loss: 0.7709123514771089\tValidation Loss: 0.7718280622916236\n",
            "Epoch 147  \tTraining Loss: 0.7700325359751322\tValidation Loss: 0.7709445883452947\n",
            "Epoch 148  \tTraining Loss: 0.7691576521945059\tValidation Loss: 0.7700660425855332\n",
            "Epoch 149  \tTraining Loss: 0.7682876724909391\tValidation Loss: 0.769192397455339\n",
            "Epoch 150  \tTraining Loss: 0.7674225693750985\tValidation Loss: 0.7683236255519912\n",
            "Epoch 151  \tTraining Loss: 0.7665623155117401\tValidation Loss: 0.7674596996261828\n",
            "Epoch 152  \tTraining Loss: 0.7657068837188438\tValidation Loss: 0.7666005925811621\n",
            "Epoch 153  \tTraining Loss: 0.7648562469667574\tValidation Loss: 0.7657462774718778\n",
            "Epoch 154  \tTraining Loss: 0.7640103783773389\tValidation Loss: 0.7648967275041296\n",
            "Epoch 155  \tTraining Loss: 0.7631692512231105\tValidation Loss: 0.7640519160337217\n",
            "Epoch 156  \tTraining Loss: 0.7623328389264126\tValidation Loss: 0.763211816565625\n",
            "Epoch 157  \tTraining Loss: 0.7615011150585639\tValidation Loss: 0.762376402753139\n",
            "Epoch 158  \tTraining Loss: 0.7606740533390258\tValidation Loss: 0.7615456483970623\n",
            "Epoch 159  \tTraining Loss: 0.759851627634574\tValidation Loss: 0.7607195274448659\n",
            "Epoch 160  \tTraining Loss: 0.7590338119584707\tValidation Loss: 0.759898013989872\n",
            "Epoch 161  \tTraining Loss: 0.7582205804696448\tValidation Loss: 0.7590810822704361\n",
            "Epoch 162  \tTraining Loss: 0.757411907471874\tValidation Loss: 0.7582687066691349\n",
            "Epoch 163  \tTraining Loss: 0.7566077674129748\tValidation Loss: 0.7574608617119589\n",
            "Epoch 164  \tTraining Loss: 0.7558081348839929\tValidation Loss: 0.7566575220675086\n",
            "Epoch 165  \tTraining Loss: 0.7550129846184022\tValidation Loss: 0.7558586625461955\n",
            "Epoch 166  \tTraining Loss: 0.7542222914913054\tValidation Loss: 0.755064258099448\n",
            "Epoch 167  \tTraining Loss: 0.7534360305186407\tValidation Loss: 0.7542742838189213\n",
            "Epoch 168  \tTraining Loss: 0.7526541768563914\tValidation Loss: 0.7534887149357117\n",
            "Epoch 169  \tTraining Loss: 0.7518767057998027\tValidation Loss: 0.7527075268195758\n",
            "Epoch 170  \tTraining Loss: 0.7511035927825985\tValidation Loss: 0.751930694978154\n",
            "Epoch 171  \tTraining Loss: 0.7503348133762077\tValidation Loss: 0.751158195056197\n",
            "Epoch 172  \tTraining Loss: 0.7495703432889907\tValidation Loss: 0.7503900028347992\n",
            "Epoch 173  \tTraining Loss: 0.7488101583654727\tValidation Loss: 0.7496260942306338\n",
            "Epoch 174  \tTraining Loss: 0.7480542345855795\tValidation Loss: 0.7488664452951935\n",
            "Epoch 175  \tTraining Loss: 0.7473025480638799\tValidation Loss: 0.7481110322140365\n",
            "Epoch 176  \tTraining Loss: 0.7465550750488295\tValidation Loss: 0.7473598313060339\n",
            "Epoch 177  \tTraining Loss: 0.7458117919220212\tValidation Loss: 0.746612819022623\n",
            "Epoch 178  \tTraining Loss: 0.7450726751974388\tValidation Loss: 0.7458699719470663\n",
            "Epoch 179  \tTraining Loss: 0.7443377015207148\tValidation Loss: 0.7451312667937113\n",
            "Epoch 180  \tTraining Loss: 0.7436068476683912\tValidation Loss: 0.7443966804072575\n",
            "Epoch 181  \tTraining Loss: 0.7428800905471885\tValidation Loss: 0.743666189762025\n",
            "Epoch 182  \tTraining Loss: 0.7421574071932736\tValidation Loss: 0.7429397719612294\n",
            "Epoch 183  \tTraining Loss: 0.7414387747715345\tValidation Loss: 0.7422174042362593\n",
            "Epoch 184  \tTraining Loss: 0.7407241705748601\tValidation Loss: 0.7414990639459584\n",
            "Epoch 185  \tTraining Loss: 0.7400135720234212\tValidation Loss: 0.7407847285759122\n",
            "Epoch 186  \tTraining Loss: 0.739306956663958\tValidation Loss: 0.7400743757377364\n",
            "Epoch 187  \tTraining Loss: 0.7386043021690701\tValidation Loss: 0.7393679831683727\n",
            "Epoch 188  \tTraining Loss: 0.7379055863365116\tValidation Loss: 0.738665528729386\n",
            "Epoch 189  \tTraining Loss: 0.7372107870884886\tValidation Loss: 0.7379669904062657\n",
            "Epoch 190  \tTraining Loss: 0.7365198824709625\tValidation Loss: 0.7372723463077331\n",
            "Epoch 191  \tTraining Loss: 0.7358328506529558\tValidation Loss: 0.7365815746650495\n",
            "Epoch 192  \tTraining Loss: 0.7351496699258625\tValidation Loss: 0.7358946538313306\n",
            "Epoch 193  \tTraining Loss: 0.734470318702762\tValidation Loss: 0.7352115622808643\n",
            "Epoch 194  \tTraining Loss: 0.7337947755177374\tValidation Loss: 0.7345322786084308\n",
            "Epoch 195  \tTraining Loss: 0.7331230190251969\tValidation Loss: 0.7338567815286289\n",
            "Epoch 196  \tTraining Loss: 0.732455027999199\tValidation Loss: 0.7331850498752037\n",
            "Epoch 197  \tTraining Loss: 0.7317907813327826\tValidation Loss: 0.7325170626003801\n",
            "Epoch 198  \tTraining Loss: 0.7311302580372991\tValidation Loss: 0.7318527987741987\n",
            "Epoch 199  \tTraining Loss: 0.7304734372417505\tValidation Loss: 0.7311922375838563\n",
            "Epoch 200  \tTraining Loss: 0.7298202981921288\tValidation Loss: 0.7305353583330486\n",
            "Epoch 201  \tTraining Loss: 0.7291708202507602\tValidation Loss: 0.7298821404413189\n",
            "Epoch 202  \tTraining Loss: 0.7285249828956543\tValidation Loss: 0.7292325634434087\n",
            "Epoch 203  \tTraining Loss: 0.7278827657198533\tValidation Loss: 0.7285866069886118\n",
            "Epoch 204  \tTraining Loss: 0.7272441484307909\tValidation Loss: 0.7279442508401339\n",
            "Epoch 205  \tTraining Loss: 0.7266091108496463\tValidation Loss: 0.7273054748744536\n",
            "Epoch 206  \tTraining Loss: 0.7259776329107114\tValidation Loss: 0.7266702590806877\n",
            "Epoch 207  \tTraining Loss: 0.7253496946607525\tValidation Loss: 0.7260385835599616\n",
            "Epoch 208  \tTraining Loss: 0.7247252762583835\tValidation Loss: 0.7254104285247804\n",
            "Epoch 209  \tTraining Loss: 0.7241043579734362\tValidation Loss: 0.7247857742984057\n",
            "Epoch 210  \tTraining Loss: 0.7234869201863384\tValidation Loss: 0.7241646013142357\n",
            "Epoch 211  \tTraining Loss: 0.7228729433874937\tValidation Loss: 0.7235468901151866\n",
            "Epoch 212  \tTraining Loss: 0.7222624081766642\tValidation Loss: 0.7229326213530821\n",
            "Epoch 213  \tTraining Loss: 0.7216552952623588\tValidation Loss: 0.7223217757880399\n",
            "Epoch 214  \tTraining Loss: 0.7210515854612232\tValidation Loss: 0.7217143342878675\n",
            "Epoch 215  \tTraining Loss: 0.7204512596974327\tValidation Loss: 0.7211102778274588\n",
            "Epoch 216  \tTraining Loss: 0.719854299002091\tValidation Loss: 0.7205095874881928\n",
            "Epoch 217  \tTraining Loss: 0.7192606845126297\tValidation Loss: 0.7199122444573389\n",
            "Epoch 218  \tTraining Loss: 0.7186703974722128\tValidation Loss: 0.7193182300274626\n",
            "Epoch 219  \tTraining Loss: 0.7180834192291445\tValidation Loss: 0.7187275255958364\n",
            "Epoch 220  \tTraining Loss: 0.7174997312362785\tValidation Loss: 0.7181401126638527\n",
            "Epoch 221  \tTraining Loss: 0.7169193150504335\tValidation Loss: 0.7175559728364416\n",
            "Epoch 222  \tTraining Loss: 0.7163421523318086\tValidation Loss: 0.7169750878214894\n",
            "Epoch 223  \tTraining Loss: 0.715768224843406\tValidation Loss: 0.7163974394292636\n",
            "Epoch 224  \tTraining Loss: 0.7151975144504533\tValidation Loss: 0.7158230095718382\n",
            "Epoch 225  \tTraining Loss: 0.7146300031198307\tValidation Loss: 0.7152517802625243\n",
            "Epoch 226  \tTraining Loss: 0.7140656729195011\tValidation Loss: 0.7146837336153026\n",
            "Epoch 227  \tTraining Loss: 0.7135045060179442\tValidation Loss: 0.7141188518442593\n",
            "Epoch 228  \tTraining Loss: 0.7129464846835921\tValidation Loss: 0.7135571172630262\n",
            "Epoch 229  \tTraining Loss: 0.7123915912842701\tValidation Loss: 0.7129985122842224\n",
            "Epoch 230  \tTraining Loss: 0.7118398082866383\tValidation Loss: 0.7124430194189001\n",
            "Epoch 231  \tTraining Loss: 0.711291118255639\tValidation Loss: 0.7118906212759937\n",
            "Epoch 232  \tTraining Loss: 0.7107455038539441\tValidation Loss: 0.7113413005617703\n",
            "Epoch 233  \tTraining Loss: 0.710202947841409\tValidation Loss: 0.7107950400792865\n",
            "Epoch 234  \tTraining Loss: 0.7096634330745268\tValidation Loss: 0.7102518227278444\n",
            "Epoch 235  \tTraining Loss: 0.7091269425058868\tValidation Loss: 0.7097116315024531\n",
            "Epoch 236  \tTraining Loss: 0.7085934591836361\tValidation Loss: 0.7091744494932937\n",
            "Epoch 237  \tTraining Loss: 0.7080629662509438\tValidation Loss: 0.7086402598851844\n",
            "Epoch 238  \tTraining Loss: 0.7075354469454681\tValidation Loss: 0.708109045957052\n",
            "Epoch 239  \tTraining Loss: 0.707010884598827\tValidation Loss: 0.7075807910814044\n",
            "Epoch 240  \tTraining Loss: 0.7064892626360717\tValidation Loss: 0.7070554787238054\n",
            "Epoch 241  \tTraining Loss: 0.7059705645751627\tValidation Loss: 0.7065330924423551\n",
            "Epoch 242  \tTraining Loss: 0.7054547740264484\tValidation Loss: 0.7060136158871706\n",
            "Epoch 243  \tTraining Loss: 0.7049418746921488\tValidation Loss: 0.7054970327998707\n",
            "Epoch 244  \tTraining Loss: 0.7044318503658383\tValidation Loss: 0.704983327013064\n",
            "Epoch 245  \tTraining Loss: 0.7039246849319365\tValidation Loss: 0.7044724824498386\n",
            "Epoch 246  \tTraining Loss: 0.7034203623651955\tValidation Loss: 0.7039644831232557\n",
            "Epoch 247  \tTraining Loss: 0.7029188667301968\tValidation Loss: 0.7034593131358451\n",
            "Epoch 248  \tTraining Loss: 0.7024201821808459\tValidation Loss: 0.7029569566791054\n",
            "Epoch 249  \tTraining Loss: 0.7019242929598715\tValidation Loss: 0.702457398033004\n",
            "Epoch 250  \tTraining Loss: 0.7014311833983293\tValidation Loss: 0.701960621565483\n",
            "Epoch 251  \tTraining Loss: 0.7009408379151045\tValidation Loss: 0.7014666117319659\n",
            "Epoch 252  \tTraining Loss: 0.7004532410164221\tValidation Loss: 0.7009753530748672\n",
            "Epoch 253  \tTraining Loss: 0.6999683772953552\tValidation Loss: 0.7004868302231062\n",
            "Epoch 254  \tTraining Loss: 0.6994862314313384\tValidation Loss: 0.7000010278916217\n",
            "Epoch 255  \tTraining Loss: 0.6990067881896852\tValidation Loss: 0.6995179308808901\n",
            "Epoch 256  \tTraining Loss: 0.6985300324211055\tValidation Loss: 0.6990375240764474\n",
            "Epoch 257  \tTraining Loss: 0.6980559490612267\tValidation Loss: 0.6985597924484115\n",
            "Epoch 258  \tTraining Loss: 0.6975845231301184\tValidation Loss: 0.6980847210510093\n",
            "Epoch 259  \tTraining Loss: 0.6971157397318187\tValidation Loss: 0.6976122950221054\n",
            "Epoch 260  \tTraining Loss: 0.6966495840538637\tValidation Loss: 0.6971424995827332\n",
            "Epoch 261  \tTraining Loss: 0.696186041366819\tValidation Loss: 0.6966753200366306\n",
            "Epoch 262  \tTraining Loss: 0.6957250970238149\tValidation Loss: 0.6962107417697746\n",
            "Epoch 263  \tTraining Loss: 0.6952667364600835\tValidation Loss: 0.6957487502499226\n",
            "Epoch 264  \tTraining Loss: 0.6948109451924979\tValidation Loss: 0.6952893310261533\n",
            "Epoch 265  \tTraining Loss: 0.6943577088191151\tValidation Loss: 0.6948324697284115\n",
            "Epoch 266  \tTraining Loss: 0.6939070130187208\tValidation Loss: 0.6943781520670557\n",
            "Epoch 267  \tTraining Loss: 0.6934588435503775\tValidation Loss: 0.6939263638324069\n",
            "Epoch 268  \tTraining Loss: 0.6930131862529728\tValidation Loss: 0.6934770908943011\n",
            "Epoch 269  \tTraining Loss: 0.6925700270447738\tValidation Loss: 0.6930303192016444\n",
            "Epoch 270  \tTraining Loss: 0.6921293519229811\tValidation Loss: 0.6925860347819697\n",
            "Epoch 271  \tTraining Loss: 0.691691146963287\tValidation Loss: 0.6921442237409962\n",
            "Epoch 272  \tTraining Loss: 0.6912553983194348\tValidation Loss: 0.6917048722621918\n",
            "Epoch 273  \tTraining Loss: 0.690822092222782\tValidation Loss: 0.6912679666063379\n",
            "Epoch 274  \tTraining Loss: 0.6903912149818645\tValidation Loss: 0.6908334931110958\n",
            "Epoch 275  \tTraining Loss: 0.6899627529819647\tValidation Loss: 0.6904014381905768\n",
            "Epoch 276  \tTraining Loss: 0.6895366926846808\tValidation Loss: 0.689971788334913\n",
            "Epoch 277  \tTraining Loss: 0.6891130206274995\tValidation Loss: 0.6895445301098332\n",
            "Epoch 278  \tTraining Loss: 0.6886917234233699\tValidation Loss: 0.6891196501562378\n",
            "Epoch 279  \tTraining Loss: 0.6882727877602813\tValidation Loss: 0.6886971351897789\n",
            "Epoch 280  \tTraining Loss: 0.687856200400842\tValidation Loss: 0.6882769720004412\n",
            "Epoch 281  \tTraining Loss: 0.6874419481818614\tValidation Loss: 0.687859147452126\n",
            "Epoch 282  \tTraining Loss: 0.6870300180139336\tValidation Loss: 0.6874436484822367\n",
            "Epoch 283  \tTraining Loss: 0.6866203968810245\tValidation Loss: 0.6870304621012676\n",
            "Epoch 284  \tTraining Loss: 0.6862130718400598\tValidation Loss: 0.686619575392395\n",
            "Epoch 285  \tTraining Loss: 0.6858080300205166\tValidation Loss: 0.6862109755110689\n",
            "Epoch 286  \tTraining Loss: 0.6854052586240162\tValidation Loss: 0.6858046496846097\n",
            "Epoch 287  \tTraining Loss: 0.6850047449239203\tValidation Loss: 0.6854005852118042\n",
            "Epoch 288  \tTraining Loss: 0.6846064762649281\tValidation Loss: 0.6849987694625068\n",
            "Epoch 289  \tTraining Loss: 0.6842104400626776\tValidation Loss: 0.6845991898772401\n",
            "Epoch 290  \tTraining Loss: 0.6838166238033466\tValidation Loss: 0.6842018339668008\n",
            "Epoch 291  \tTraining Loss: 0.6834250150432583\tValidation Loss: 0.6838066893118644\n",
            "Epoch 292  \tTraining Loss: 0.6830356014084872\tValidation Loss: 0.6834137435625952\n",
            "Epoch 293  \tTraining Loss: 0.6826483705944696\tValidation Loss: 0.6830229844382563\n",
            "Epoch 294  \tTraining Loss: 0.6822633103656127\tValidation Loss: 0.682634399726823\n",
            "Epoch 295  \tTraining Loss: 0.68188040855491\tValidation Loss: 0.6822479772845982\n",
            "Epoch 296  \tTraining Loss: 0.6814996530635556\tValidation Loss: 0.6818637050358289\n",
            "Epoch 297  \tTraining Loss: 0.6811210318605625\tValidation Loss: 0.6814815709723271\n",
            "Epoch 298  \tTraining Loss: 0.6807445329823821\tValidation Loss: 0.6811015631530898\n",
            "Epoch 299  \tTraining Loss: 0.6803701445325265\tValidation Loss: 0.6807236697039241\n",
            "Epoch 300  \tTraining Loss: 0.679997854681192\tValidation Loss: 0.6803478788170725\n",
            "Epoch 301  \tTraining Loss: 0.6796276516648864\tValidation Loss: 0.6799741787508403\n",
            "Epoch 302  \tTraining Loss: 0.6792595237860559\tValidation Loss: 0.679602557829227\n",
            "Epoch 303  \tTraining Loss: 0.6788934594127166\tValidation Loss: 0.6792330044415569\n",
            "Epoch 304  \tTraining Loss: 0.6785294469780867\tValidation Loss: 0.6788655070421146\n",
            "Epoch 305  \tTraining Loss: 0.6781674749802206\tValidation Loss: 0.6785000541497801\n",
            "Epoch 306  \tTraining Loss: 0.6778075319816459\tValidation Loss: 0.6781366343476678\n",
            "Epoch 307  \tTraining Loss: 0.6774496066090019\tValidation Loss: 0.6777752362827666\n",
            "Epoch 308  \tTraining Loss: 0.67709368755268\tValidation Loss: 0.6774158486655818\n",
            "Epoch 309  \tTraining Loss: 0.6767397635664664\tValidation Loss: 0.6770584602697801\n",
            "Epoch 310  \tTraining Loss: 0.6763878234671876\tValidation Loss: 0.6767030599318354\n",
            "Epoch 311  \tTraining Loss: 0.6760378561343553\tValidation Loss: 0.6763496365506774\n",
            "Epoch 312  \tTraining Loss: 0.6756898505098168\tValidation Loss: 0.6759981790873414\n",
            "Epoch 313  \tTraining Loss: 0.6753437955974047\tValidation Loss: 0.6756486765646206\n",
            "Epoch 314  \tTraining Loss: 0.6749996804625891\tValidation Loss: 0.6753011180667213\n",
            "Epoch 315  \tTraining Loss: 0.6746574942321331\tValidation Loss: 0.674955492738917\n",
            "Epoch 316  \tTraining Loss: 0.674317226093748\tValidation Loss: 0.6746117897872084\n",
            "Epoch 317  \tTraining Loss: 0.6739788652957535\tValidation Loss: 0.674269998477982\n",
            "Epoch 318  \tTraining Loss: 0.6736424011467356\tValidation Loss: 0.673930108137673\n",
            "Epoch 319  \tTraining Loss: 0.6733078230152107\tValidation Loss: 0.6735921081524278\n",
            "Epoch 320  \tTraining Loss: 0.6729751203292886\tValidation Loss: 0.6732559879677704\n",
            "Epoch 321  \tTraining Loss: 0.6726442825763391\tValidation Loss: 0.6729217370882703\n",
            "Epoch 322  \tTraining Loss: 0.6723152993026597\tValidation Loss: 0.672589345077211\n",
            "Epoch 323  \tTraining Loss: 0.6719881601131443\tValidation Loss: 0.6722588015562612\n",
            "Epoch 324  \tTraining Loss: 0.6716628546709565\tValidation Loss: 0.6719300962051491\n",
            "Epoch 325  \tTraining Loss: 0.6713393726972015\tValidation Loss: 0.6716032187613354\n",
            "Epoch 326  \tTraining Loss: 0.6710177039706021\tValidation Loss: 0.6712781590196918\n",
            "Epoch 327  \tTraining Loss: 0.6706978383271756\tValidation Loss: 0.6709549068321782\n",
            "Epoch 328  \tTraining Loss: 0.6703797656599118\tValidation Loss: 0.6706334521075237\n",
            "Epoch 329  \tTraining Loss: 0.6700634759184552\tValidation Loss: 0.6703137848109084\n",
            "Epoch 330  \tTraining Loss: 0.6697489591087865\tValidation Loss: 0.6699958949636479\n",
            "Epoch 331  \tTraining Loss: 0.6694362052929063\tValidation Loss: 0.6696797726428779\n",
            "Epoch 332  \tTraining Loss: 0.6691252045885226\tValidation Loss: 0.6693654079812424\n",
            "Epoch 333  \tTraining Loss: 0.6688159471687372\tValidation Loss: 0.6690527911665832\n",
            "Epoch 334  \tTraining Loss: 0.6685084232617351\tValidation Loss: 0.6687419124416294\n",
            "Epoch 335  \tTraining Loss: 0.6682026231504772\tValidation Loss: 0.6684327621036914\n",
            "Epoch 336  \tTraining Loss: 0.6678985371723917\tValidation Loss: 0.6681253305043551\n",
            "Epoch 337  \tTraining Loss: 0.6675961557190697\tValidation Loss: 0.6678196080491772\n",
            "Epoch 338  \tTraining Loss: 0.6672954692359614\tValidation Loss: 0.6675155851973836\n",
            "Epoch 339  \tTraining Loss: 0.6669964682220735\tValidation Loss: 0.667213252461569\n",
            "Epoch 340  \tTraining Loss: 0.6666991432296705\tValidation Loss: 0.6669126004073979\n",
            "Epoch 341  \tTraining Loss: 0.6664034848639747\tValidation Loss: 0.6666136196533066\n",
            "Epoch 342  \tTraining Loss: 0.6661094837828702\tValidation Loss: 0.6663163008702092\n",
            "Epoch 343  \tTraining Loss: 0.6658171306966069\tValidation Loss: 0.6660206347812029\n",
            "Epoch 344  \tTraining Loss: 0.6655264163675081\tValidation Loss: 0.6657266121612753\n",
            "Epoch 345  \tTraining Loss: 0.665237331609678\tValidation Loss: 0.6654342238370154\n",
            "Epoch 346  \tTraining Loss: 0.6649498672887111\tValidation Loss: 0.665143460686323\n",
            "Epoch 347  \tTraining Loss: 0.6646640143214041\tValidation Loss: 0.6648543136381224\n",
            "Epoch 348  \tTraining Loss: 0.6643797636754685\tValidation Loss: 0.6645667736720761\n",
            "Epoch 349  \tTraining Loss: 0.6640971063692461\tValidation Loss: 0.6642808318183014\n",
            "Epoch 350  \tTraining Loss: 0.6638160334714236\tValidation Loss: 0.663996479157087\n",
            "Epoch 351  \tTraining Loss: 0.6635365361007524\tValidation Loss: 0.6637137068186131\n",
            "Epoch 352  \tTraining Loss: 0.6632586054257654\tValidation Loss: 0.6634325059826708\n",
            "Epoch 353  \tTraining Loss: 0.6629822326645012\tValidation Loss: 0.6631528678783857\n",
            "Epoch 354  \tTraining Loss: 0.6627074090842233\tValidation Loss: 0.6628747837839403\n",
            "Epoch 355  \tTraining Loss: 0.6624341260011469\tValidation Loss: 0.6625982450263006\n",
            "Epoch 356  \tTraining Loss: 0.6621623747801625\tValidation Loss: 0.6623232429809418\n",
            "Epoch 357  \tTraining Loss: 0.6618921468345642\tValidation Loss: 0.6620497690715773\n",
            "Epoch 358  \tTraining Loss: 0.661623433625778\tValidation Loss: 0.6617778147698888\n",
            "Epoch 359  \tTraining Loss: 0.6613562266630919\tValidation Loss: 0.6615073715952572\n",
            "Epoch 360  \tTraining Loss: 0.6610905175033884\tValidation Loss: 0.6612384311144957\n",
            "Epoch 361  \tTraining Loss: 0.6608262977508764\tValidation Loss: 0.6609709849415841\n",
            "Epoch 362  \tTraining Loss: 0.6605635590568267\tValidation Loss: 0.6607050247374056\n",
            "Epoch 363  \tTraining Loss: 0.6603022931193088\tValidation Loss: 0.6604405422094823\n",
            "Epoch 364  \tTraining Loss: 0.6600424916829268\tValidation Loss: 0.6601775291117163\n",
            "Epoch 365  \tTraining Loss: 0.6597841465385601\tValidation Loss: 0.6599159772441281\n",
            "Epoch 366  \tTraining Loss: 0.6595272495231039\tValidation Loss: 0.6596558784525999\n",
            "Epoch 367  \tTraining Loss: 0.6592717925192105\tValidation Loss: 0.6593972246286174\n",
            "Epoch 368  \tTraining Loss: 0.6590177674550332\tValidation Loss: 0.6591400077090157\n",
            "Epoch 369  \tTraining Loss: 0.6587651663039714\tValidation Loss: 0.6588842196757246\n",
            "Epoch 370  \tTraining Loss: 0.6585139810844168\tValidation Loss: 0.6586298525555168\n",
            "Epoch 371  \tTraining Loss: 0.6582642038595012\tValidation Loss: 0.6583768984197556\n",
            "Epoch 372  \tTraining Loss: 0.658015826736846\tValidation Loss: 0.6581253493841467\n",
            "Epoch 373  \tTraining Loss: 0.6577688418683121\tValidation Loss: 0.657875197608489\n",
            "Epoch 374  \tTraining Loss: 0.657523241449753\tValidation Loss: 0.6576264352964281\n",
            "Epoch 375  \tTraining Loss: 0.6572790177207667\tValidation Loss: 0.6573790546952107\n",
            "Epoch 376  \tTraining Loss: 0.6570361629644523\tValidation Loss: 0.6571330480954405\n",
            "Epoch 377  \tTraining Loss: 0.6567946695071648\tValidation Loss: 0.656888407830835\n",
            "Epoch 378  \tTraining Loss: 0.6565545297182733\tValidation Loss: 0.656645126277985\n",
            "Epoch 379  \tTraining Loss: 0.6563157360099194\tValidation Loss: 0.6564031958561141\n",
            "Epoch 380  \tTraining Loss: 0.6560782808367781\tValidation Loss: 0.6561626090268398\n",
            "Epoch 381  \tTraining Loss: 0.6558421566958181\tValidation Loss: 0.6559233582939363\n",
            "Epoch 382  \tTraining Loss: 0.6556073561260669\tValidation Loss: 0.6556854362030986\n",
            "Epoch 383  \tTraining Loss: 0.655373871708373\tValidation Loss: 0.6554488353417078\n",
            "Epoch 384  \tTraining Loss: 0.6551416960651723\tValidation Loss: 0.6552135483385975\n",
            "Epoch 385  \tTraining Loss: 0.6549108218602547\tValidation Loss: 0.6549795678638219\n",
            "Epoch 386  \tTraining Loss: 0.6546812417985333\tValidation Loss: 0.6547468866284246\n",
            "Epoch 387  \tTraining Loss: 0.6544529486258123\tValidation Loss: 0.6545154973842102\n",
            "Epoch 388  \tTraining Loss: 0.654225935128559\tValidation Loss: 0.6542853929235146\n",
            "Epoch 389  \tTraining Loss: 0.6540001941336755\tValidation Loss: 0.6540565660789792\n",
            "Epoch 390  \tTraining Loss: 0.653775718508272\tValidation Loss: 0.6538290097233248\n",
            "Epoch 391  \tTraining Loss: 0.6535525011594411\tValidation Loss: 0.6536027167691272\n",
            "Epoch 392  \tTraining Loss: 0.6533305350340342\tValidation Loss: 0.6533776801685944\n",
            "Epoch 393  \tTraining Loss: 0.6531098131184386\tValidation Loss: 0.6531538929133442\n",
            "Epoch 394  \tTraining Loss: 0.6528903284383556\tValidation Loss: 0.6529313480341841\n",
            "Epoch 395  \tTraining Loss: 0.6526720740585804\tValidation Loss: 0.652710038600891\n",
            "Epoch 396  \tTraining Loss: 0.6524550430827827\tValidation Loss: 0.6524899577219946\n",
            "Epoch 397  \tTraining Loss: 0.6522392286532889\tValidation Loss: 0.6522710985445583\n",
            "Epoch 398  \tTraining Loss: 0.6520246239508652\tValidation Loss: 0.652053454253966\n",
            "Epoch 399  \tTraining Loss: 0.6518112221945029\tValidation Loss: 0.6518370180737053\n",
            "Epoch 400  \tTraining Loss: 0.6515990166412032\tValidation Loss: 0.6516217832651551\n",
            "lr, batch_size: (0.001, 300)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.6243534526955732\n",
            "Epoch 2  \tTraining Loss: 0.6211419364869208\tValidation Loss: 0.8706964125768494\n",
            "Epoch 3  \tTraining Loss: 0.8652956414574802\tValidation Loss: 0.6080005424691387\n",
            "Epoch 4  \tTraining Loss: 0.6057258164438752\tValidation Loss: 0.6340736540992257\n",
            "Epoch 5  \tTraining Loss: 0.6252237430173914\tValidation Loss: 0.8651971395052276\n",
            "Epoch 6  \tTraining Loss: 0.8631873101975883\tValidation Loss: 0.5740676554933143\n",
            "Epoch 7  \tTraining Loss: 0.5681604855792008\tValidation Loss: 0.5709372474182237\n",
            "Epoch 8  \tTraining Loss: 0.5578663203610648\tValidation Loss: 0.6604647027530745\n",
            "Epoch 9  \tTraining Loss: 0.6453815262912882\tValidation Loss: 8.510887002784191\n",
            "Epoch 10  \tTraining Loss: 8.554905179409724\tValidation Loss: 0.9342282716618441\n",
            "Epoch 11  \tTraining Loss: 0.9311494197660802\tValidation Loss: 0.9321786454884309\n",
            "Epoch 12  \tTraining Loss: 0.9291057032192446\tValidation Loss: 0.9282034960205785\n",
            "Epoch 13  \tTraining Loss: 0.9249627130767325\tValidation Loss: 0.9063776912725549\n",
            "Epoch 14  \tTraining Loss: 0.9025341770683879\tValidation Loss: 0.7597925493872184\n",
            "Epoch 15  \tTraining Loss: 0.7561155980652636\tValidation Loss: 0.8569698807397439\n",
            "Epoch 16  \tTraining Loss: 0.8620547787858478\tValidation Loss: 0.9301583208540838\n",
            "Epoch 17  \tTraining Loss: 0.9270153094757775\tValidation Loss: 0.9281158515036863\n",
            "Epoch 18  \tTraining Loss: 0.9250051223871008\tValidation Loss: 0.9245802837732365\n",
            "Epoch 19  \tTraining Loss: 0.9214338045315198\tValidation Loss: 0.9101549180178962\n",
            "Epoch 20  \tTraining Loss: 0.9068119994847061\tValidation Loss: 0.765848939127548\n",
            "Epoch 21  \tTraining Loss: 0.7638876288693668\tValidation Loss: 1.2008148593786083\n",
            "Epoch 22  \tTraining Loss: 1.20506386086261\tValidation Loss: 0.9238496145517081\n",
            "Epoch 23  \tTraining Loss: 0.9208215042376782\tValidation Loss: 0.9073228248763365\n",
            "Epoch 24  \tTraining Loss: 0.9041775590340153\tValidation Loss: 0.7271733403852984\n",
            "Epoch 25  \tTraining Loss: 0.7237622127170513\tValidation Loss: 1.3020207933124877\n",
            "Epoch 26  \tTraining Loss: 1.301939602883523\tValidation Loss: 0.923379957497202\n",
            "Epoch 27  \tTraining Loss: 0.9203567467873853\tValidation Loss: 0.9216504099658089\n",
            "Epoch 28  \tTraining Loss: 0.9186392350714089\tValidation Loss: 0.9199305235425486\n",
            "Epoch 29  \tTraining Loss: 0.9169313507025068\tValidation Loss: 0.9182202441866457\n",
            "Epoch 30  \tTraining Loss: 0.9152330397155088\tValidation Loss: 0.9165195181408842\n",
            "Epoch 31  \tTraining Loss: 0.9135442484477425\tValidation Loss: 0.9148282919491134\n",
            "Epoch 32  \tTraining Loss: 0.9118649235373344\tValidation Loss: 0.9131465124545632\n",
            "Epoch 33  \tTraining Loss: 0.9101950119215273\tValidation Loss: 0.9114741267981648\n",
            "Epoch 34  \tTraining Loss: 0.9085344608350017\tValidation Loss: 0.9098110824168839\n",
            "Epoch 35  \tTraining Loss: 0.9068832178082096\tValidation Loss: 0.9081573270420613\n",
            "Epoch 36  \tTraining Loss: 0.905241230665715\tValidation Loss: 0.9065128086977641\n",
            "Epoch 37  \tTraining Loss: 0.9036084475245472\tValidation Loss: 0.904877475699145\n",
            "Epoch 38  \tTraining Loss: 0.9019848167925602\tValidation Loss: 0.9032512766508122\n",
            "Epoch 39  \tTraining Loss: 0.9003702871668022\tValidation Loss: 0.9016341604452063\n",
            "Epoch 40  \tTraining Loss: 0.8987648076318966\tValidation Loss: 0.9000260762609895\n",
            "Epoch 41  \tTraining Loss: 0.8971683274584276\tValidation Loss: 0.89842697356144\n",
            "Epoch 42  \tTraining Loss: 0.895580796201338\tValidation Loss: 0.8968368020928583\n",
            "Epoch 43  \tTraining Loss: 0.8940021636983363\tValidation Loss: 0.8952555118829819\n",
            "Epoch 44  \tTraining Loss: 0.8924323800683104\tValidation Loss: 0.8936830532394076\n",
            "Epoch 45  \tTraining Loss: 0.8908713957097525\tValidation Loss: 0.8921193767480233\n",
            "Epoch 46  \tTraining Loss: 0.8893191612991906\tValidation Loss: 0.8905644332714502\n",
            "Epoch 47  \tTraining Loss: 0.8877756277896311\tValidation Loss: 0.8890181739474907\n",
            "Epoch 48  \tTraining Loss: 0.8862407464090087\tValidation Loss: 0.8874805501875878\n",
            "Epoch 49  \tTraining Loss: 0.884714468658645\tValidation Loss: 0.8859515136752909\n",
            "Epoch 50  \tTraining Loss: 0.8831967463117163\tValidation Loss: 0.884431016364732\n",
            "Epoch 51  \tTraining Loss: 0.8816875314117302\tValidation Loss: 0.8829190104791098\n",
            "Epoch 52  \tTraining Loss: 0.8801867762710086\tValidation Loss: 0.8814154485091813\n",
            "Epoch 53  \tTraining Loss: 0.8786944334691841\tValidation Loss: 0.8799202832117639\n",
            "Epoch 54  \tTraining Loss: 0.8772104558516973\tValidation Loss: 0.8784334676082435\n",
            "Epoch 55  \tTraining Loss: 0.8757347965283108\tValidation Loss: 0.8769549549830928\n",
            "Epoch 56  \tTraining Loss: 0.8742674088716254\tValidation Loss: 0.8754846988823977\n",
            "Epoch 57  \tTraining Loss: 0.8728082465156076\tValidation Loss: 0.8740226531123906\n",
            "Epoch 58  \tTraining Loss: 0.8713572633541237\tValidation Loss: 0.872568771737994\n",
            "Epoch 59  \tTraining Loss: 0.8699144135394834\tValidation Loss: 0.8711230090813695\n",
            "Epoch 60  \tTraining Loss: 0.8684796514809922\tValidation Loss: 0.8696853197204786\n",
            "Epoch 61  \tTraining Loss: 0.8670529318435095\tValidation Loss: 0.868255658487648\n",
            "Epoch 62  \tTraining Loss: 0.8656342095460153\tValidation Loss: 0.8668339804681445\n",
            "Epoch 63  \tTraining Loss: 0.864223439760189\tValidation Loss: 0.8654202409987594\n",
            "Epoch 64  \tTraining Loss: 0.8628205779089888\tValidation Loss: 0.8640143956663967\n",
            "Epoch 65  \tTraining Loss: 0.8614255796652468\tValidation Loss: 0.8626164003066747\n",
            "Epoch 66  \tTraining Loss: 0.860038400950266\tValidation Loss: 0.8612262110025302\n",
            "Epoch 67  \tTraining Loss: 0.8586589979324282\tValidation Loss: 0.8598437840828338\n",
            "Epoch 68  \tTraining Loss: 0.8572873270258098\tValidation Loss: 0.8584690761210124\n",
            "Epoch 69  \tTraining Loss: 0.8559233448888031\tValidation Loss: 0.8571020439336782\n",
            "Epoch 70  \tTraining Loss: 0.8545670084227482\tValidation Loss: 0.8557426445792667\n",
            "Epoch 71  \tTraining Loss: 0.8532182747705707\tValidation Loss: 0.8543908353566817\n",
            "Epoch 72  \tTraining Loss: 0.8518771013154265\tValidation Loss: 0.8530465738039484\n",
            "Epoch 73  \tTraining Loss: 0.8505434456793572\tValidation Loss: 0.8517098176968728\n",
            "Epoch 74  \tTraining Loss: 0.8492172657219497\tValidation Loss: 0.8503805250477108\n",
            "Epoch 75  \tTraining Loss: 0.8478985195390043\tValidation Loss: 0.8490586541038424\n",
            "Epoch 76  \tTraining Loss: 0.8465871654612124\tValidation Loss: 0.8477441633464543\n",
            "Epoch 77  \tTraining Loss: 0.8452831620528375\tValidation Loss: 0.846437011489231\n",
            "Epoch 78  \tTraining Loss: 0.843986468110408\tValidation Loss: 0.8451371574770516\n",
            "Epoch 79  \tTraining Loss: 0.8426970426614138\tValidation Loss: 0.8438445604846941\n",
            "Epoch 80  \tTraining Loss: 0.841414844963013\tValidation Loss: 0.8425591799155485\n",
            "Epoch 81  \tTraining Loss: 0.840139834500743\tValidation Loss: 0.8412809754003345\n",
            "Epoch 82  \tTraining Loss: 0.8388719709872421\tValidation Loss: 0.8400099067958291\n",
            "Epoch 83  \tTraining Loss: 0.8376112143609743\tValidation Loss: 0.8387459341835997\n",
            "Epoch 84  \tTraining Loss: 0.8363575247849656\tValidation Loss: 0.8374890178687442\n",
            "Epoch 85  \tTraining Loss: 0.8351108626455442\tValidation Loss: 0.83623911837864\n",
            "Epoch 86  \tTraining Loss: 0.8338711885510885\tValidation Loss: 0.8349961964616968\n",
            "Epoch 87  \tTraining Loss: 0.8326384633307837\tValidation Loss: 0.8337602130861199\n",
            "Epoch 88  \tTraining Loss: 0.8314126480333824\tValidation Loss: 0.8325311294386779\n",
            "Epoch 89  \tTraining Loss: 0.8301937039259745\tValidation Loss: 0.8313089069234786\n",
            "Epoch 90  \tTraining Loss: 0.8289815924927648\tValidation Loss: 0.8300935071607509\n",
            "Epoch 91  \tTraining Loss: 0.8277762754338537\tValidation Loss: 0.8288848919856346\n",
            "Epoch 92  \tTraining Loss: 0.8265777146640277\tValidation Loss: 0.8276830234469758\n",
            "Epoch 93  \tTraining Loss: 0.8253858723115574\tValidation Loss: 0.8264878638061303\n",
            "Epoch 94  \tTraining Loss: 0.8242007107169989\tValidation Loss: 0.8252993755357718\n",
            "Epoch 95  \tTraining Loss: 0.823022192432005\tValidation Loss: 0.8241175213187097\n",
            "Epoch 96  \tTraining Loss: 0.8218502802181415\tValidation Loss: 0.8229422640467103\n",
            "Epoch 97  \tTraining Loss: 0.8206849370457109\tValidation Loss: 0.8217735668193266\n",
            "Epoch 98  \tTraining Loss: 0.819526126092582\tValidation Loss: 0.8206113929427351\n",
            "Epoch 99  \tTraining Loss: 0.818373810743026\tValidation Loss: 0.8194557059285766\n",
            "Epoch 100  \tTraining Loss: 0.8172279545865611\tValidation Loss: 0.8183064694928064\n",
            "Epoch 101  \tTraining Loss: 0.8160885214168001\tValidation Loss: 0.8171636475545492\n",
            "Epoch 102  \tTraining Loss: 0.8149554752303074\tValidation Loss: 0.8160272042349606\n",
            "Epoch 103  \tTraining Loss: 0.8138287802254619\tValidation Loss: 0.8148971038560955\n",
            "Epoch 104  \tTraining Loss: 0.8127084008013236\tValidation Loss: 0.8137733109397829\n",
            "Epoch 105  \tTraining Loss: 0.8115943015565118\tValidation Loss: 0.812655790206506\n",
            "Epoch 106  \tTraining Loss: 0.8104864472880837\tValidation Loss: 0.8115445065742901\n",
            "Epoch 107  \tTraining Loss: 0.8093848029904237\tValidation Loss: 0.8104394251575948\n",
            "Epoch 108  \tTraining Loss: 0.8082893338541367\tValidation Loss: 0.8093405112662146\n",
            "Epoch 109  \tTraining Loss: 0.8072000052649481\tValidation Loss: 0.8082477304041839\n",
            "Epoch 110  \tTraining Loss: 0.8061167828026107\tValidation Loss: 0.8071610482686888\n",
            "Epoch 111  \tTraining Loss: 0.8050396322398167\tValidation Loss: 0.806080430748986\n",
            "Epoch 112  \tTraining Loss: 0.8039685195411157\tValidation Loss: 0.8050058439253254\n",
            "Epoch 113  \tTraining Loss: 0.8029034108618404\tValidation Loss: 0.8039372540678803\n",
            "Epoch 114  \tTraining Loss: 0.8018442725470356\tValidation Loss: 0.8028746276356844\n",
            "Epoch 115  \tTraining Loss: 0.8007910711303967\tValidation Loss: 0.801817931275573\n",
            "Epoch 116  \tTraining Loss: 0.7997437733332109\tValidation Loss: 0.8007671318211305\n",
            "Epoch 117  \tTraining Loss: 0.7987023460633059\tValidation Loss: 0.7997221962916454\n",
            "Epoch 118  \tTraining Loss: 0.7976667564140045\tValidation Loss: 0.7986830918910687\n",
            "Epoch 119  \tTraining Loss: 0.7966369716630846\tValidation Loss: 0.79764978600698\n",
            "Epoch 120  \tTraining Loss: 0.7956129592717458\tValidation Loss: 0.7966222462095588\n",
            "Epoch 121  \tTraining Loss: 0.7945946868835801\tValidation Loss: 0.7956004402505611\n",
            "Epoch 122  \tTraining Loss: 0.793582122323551\tValidation Loss: 0.7945843360623027\n",
            "Epoch 123  \tTraining Loss: 0.7925752335969755\tValidation Loss: 0.7935739017566471\n",
            "Epoch 124  \tTraining Loss: 0.7915739888885143\tValidation Loss: 0.7925691056240004\n",
            "Epoch 125  \tTraining Loss: 0.7905783565611648\tValidation Loss: 0.7915699161323105\n",
            "Epoch 126  \tTraining Loss: 0.7895883051552642\tValidation Loss: 0.7905763019260721\n",
            "Epoch 127  \tTraining Loss: 0.7886038033874927\tValidation Loss: 0.7895882318253388\n",
            "Epoch 128  \tTraining Loss: 0.7876248201498862\tValidation Loss: 0.7886056748247384\n",
            "Epoch 129  \tTraining Loss: 0.7866513245088539\tValidation Loss: 0.7876286000924959\n",
            "Epoch 130  \tTraining Loss: 0.7856832857041998\tValidation Loss: 0.7866569769694604\n",
            "Epoch 131  \tTraining Loss: 0.784720673148151\tValidation Loss: 0.7856907749681376\n",
            "Epoch 132  \tTraining Loss: 0.7837634564243922\tValidation Loss: 0.7847299637717298\n",
            "Epoch 133  \tTraining Loss: 0.782811605287103\tValidation Loss: 0.7837745132331773\n",
            "Epoch 134  \tTraining Loss: 0.7818650896600042\tValidation Loss: 0.78282439337421\n",
            "Epoch 135  \tTraining Loss: 0.7809238796354045\tValidation Loss: 0.7818795743843996\n",
            "Epoch 136  \tTraining Loss: 0.7799879454732592\tValidation Loss: 0.7809400266202207\n",
            "Epoch 137  \tTraining Loss: 0.7790572576002271\tValidation Loss: 0.7800057206041152\n",
            "Epoch 138  \tTraining Loss: 0.7781317866087385\tValidation Loss: 0.7790766270235635\n",
            "Epoch 139  \tTraining Loss: 0.7772115032560645\tValidation Loss: 0.7781527167301581\n",
            "Epoch 140  \tTraining Loss: 0.7762963784633937\tValidation Loss: 0.7772339607386859\n",
            "Epoch 141  \tTraining Loss: 0.775386383314913\tValidation Loss: 0.7763203302262134\n",
            "Epoch 142  \tTraining Loss: 0.7744814890568942\tValidation Loss: 0.7754117965311776\n",
            "Epoch 143  \tTraining Loss: 0.7735816670967858\tValidation Loss: 0.7745083311524816\n",
            "Epoch 144  \tTraining Loss: 0.7726868890023083\tValidation Loss: 0.773609905748597\n",
            "Epoch 145  \tTraining Loss: 0.7717971265005573\tValidation Loss: 0.7727164921366677\n",
            "Epoch 146  \tTraining Loss: 0.7709123514771089\tValidation Loss: 0.7718280622916236\n",
            "Epoch 147  \tTraining Loss: 0.7700325359751322\tValidation Loss: 0.7709445883452947\n",
            "Epoch 148  \tTraining Loss: 0.7691576521945059\tValidation Loss: 0.7700660425855332\n",
            "Epoch 149  \tTraining Loss: 0.7682876724909391\tValidation Loss: 0.769192397455339\n",
            "Epoch 150  \tTraining Loss: 0.7674225693750985\tValidation Loss: 0.7683236255519912\n",
            "Epoch 151  \tTraining Loss: 0.7665623155117401\tValidation Loss: 0.7674596996261828\n",
            "Epoch 152  \tTraining Loss: 0.7657068837188438\tValidation Loss: 0.7666005925811621\n",
            "Epoch 153  \tTraining Loss: 0.7648562469667574\tValidation Loss: 0.7657462774718778\n",
            "Epoch 154  \tTraining Loss: 0.7640103783773389\tValidation Loss: 0.7648967275041296\n",
            "Epoch 155  \tTraining Loss: 0.7631692512231105\tValidation Loss: 0.7640519160337217\n",
            "Epoch 156  \tTraining Loss: 0.7623328389264126\tValidation Loss: 0.763211816565625\n",
            "Epoch 157  \tTraining Loss: 0.7615011150585639\tValidation Loss: 0.762376402753139\n",
            "Epoch 158  \tTraining Loss: 0.7606740533390258\tValidation Loss: 0.7615456483970623\n",
            "Epoch 159  \tTraining Loss: 0.759851627634574\tValidation Loss: 0.7607195274448659\n",
            "Epoch 160  \tTraining Loss: 0.7590338119584707\tValidation Loss: 0.759898013989872\n",
            "Epoch 161  \tTraining Loss: 0.7582205804696448\tValidation Loss: 0.7590810822704361\n",
            "Epoch 162  \tTraining Loss: 0.757411907471874\tValidation Loss: 0.7582687066691349\n",
            "Epoch 163  \tTraining Loss: 0.7566077674129748\tValidation Loss: 0.7574608617119589\n",
            "Epoch 164  \tTraining Loss: 0.7558081348839929\tValidation Loss: 0.7566575220675086\n",
            "Epoch 165  \tTraining Loss: 0.7550129846184022\tValidation Loss: 0.7558586625461955\n",
            "Epoch 166  \tTraining Loss: 0.7542222914913054\tValidation Loss: 0.755064258099448\n",
            "Epoch 167  \tTraining Loss: 0.7534360305186407\tValidation Loss: 0.7542742838189213\n",
            "Epoch 168  \tTraining Loss: 0.7526541768563914\tValidation Loss: 0.7534887149357117\n",
            "Epoch 169  \tTraining Loss: 0.7518767057998027\tValidation Loss: 0.7527075268195758\n",
            "Epoch 170  \tTraining Loss: 0.7511035927825985\tValidation Loss: 0.751930694978154\n",
            "Epoch 171  \tTraining Loss: 0.7503348133762077\tValidation Loss: 0.751158195056197\n",
            "Epoch 172  \tTraining Loss: 0.7495703432889907\tValidation Loss: 0.7503900028347992\n",
            "Epoch 173  \tTraining Loss: 0.7488101583654727\tValidation Loss: 0.7496260942306338\n",
            "Epoch 174  \tTraining Loss: 0.7480542345855795\tValidation Loss: 0.7488664452951935\n",
            "Epoch 175  \tTraining Loss: 0.7473025480638799\tValidation Loss: 0.7481110322140365\n",
            "Epoch 176  \tTraining Loss: 0.7465550750488295\tValidation Loss: 0.7473598313060339\n",
            "Epoch 177  \tTraining Loss: 0.7458117919220212\tValidation Loss: 0.746612819022623\n",
            "Epoch 178  \tTraining Loss: 0.7450726751974388\tValidation Loss: 0.7458699719470663\n",
            "Epoch 179  \tTraining Loss: 0.7443377015207148\tValidation Loss: 0.7451312667937113\n",
            "Epoch 180  \tTraining Loss: 0.7436068476683912\tValidation Loss: 0.7443966804072575\n",
            "Epoch 181  \tTraining Loss: 0.7428800905471885\tValidation Loss: 0.743666189762025\n",
            "Epoch 182  \tTraining Loss: 0.7421574071932736\tValidation Loss: 0.7429397719612294\n",
            "Epoch 183  \tTraining Loss: 0.7414387747715345\tValidation Loss: 0.7422174042362593\n",
            "Epoch 184  \tTraining Loss: 0.7407241705748601\tValidation Loss: 0.7414990639459584\n",
            "Epoch 185  \tTraining Loss: 0.7400135720234212\tValidation Loss: 0.7407847285759122\n",
            "Epoch 186  \tTraining Loss: 0.739306956663958\tValidation Loss: 0.7400743757377364\n",
            "Epoch 187  \tTraining Loss: 0.7386043021690701\tValidation Loss: 0.7393679831683727\n",
            "Epoch 188  \tTraining Loss: 0.7379055863365116\tValidation Loss: 0.738665528729386\n",
            "Epoch 189  \tTraining Loss: 0.7372107870884886\tValidation Loss: 0.7379669904062657\n",
            "Epoch 190  \tTraining Loss: 0.7365198824709625\tValidation Loss: 0.7372723463077331\n",
            "Epoch 191  \tTraining Loss: 0.7358328506529558\tValidation Loss: 0.7365815746650495\n",
            "Epoch 192  \tTraining Loss: 0.7351496699258625\tValidation Loss: 0.7358946538313306\n",
            "Epoch 193  \tTraining Loss: 0.734470318702762\tValidation Loss: 0.7352115622808643\n",
            "Epoch 194  \tTraining Loss: 0.7337947755177374\tValidation Loss: 0.7345322786084308\n",
            "Epoch 195  \tTraining Loss: 0.7331230190251969\tValidation Loss: 0.7338567815286289\n",
            "Epoch 196  \tTraining Loss: 0.732455027999199\tValidation Loss: 0.7331850498752037\n",
            "Epoch 197  \tTraining Loss: 0.7317907813327826\tValidation Loss: 0.7325170626003801\n",
            "Epoch 198  \tTraining Loss: 0.7311302580372991\tValidation Loss: 0.7318527987741987\n",
            "Epoch 199  \tTraining Loss: 0.7304734372417505\tValidation Loss: 0.7311922375838563\n",
            "Epoch 200  \tTraining Loss: 0.7298202981921288\tValidation Loss: 0.7305353583330486\n",
            "Epoch 201  \tTraining Loss: 0.7291708202507602\tValidation Loss: 0.7298821404413189\n",
            "Epoch 202  \tTraining Loss: 0.7285249828956543\tValidation Loss: 0.7292325634434087\n",
            "Epoch 203  \tTraining Loss: 0.7278827657198533\tValidation Loss: 0.7285866069886118\n",
            "Epoch 204  \tTraining Loss: 0.7272441484307909\tValidation Loss: 0.7279442508401339\n",
            "Epoch 205  \tTraining Loss: 0.7266091108496463\tValidation Loss: 0.7273054748744536\n",
            "Epoch 206  \tTraining Loss: 0.7259776329107114\tValidation Loss: 0.7266702590806877\n",
            "Epoch 207  \tTraining Loss: 0.7253496946607525\tValidation Loss: 0.7260385835599616\n",
            "Epoch 208  \tTraining Loss: 0.7247252762583835\tValidation Loss: 0.7254104285247804\n",
            "Epoch 209  \tTraining Loss: 0.7241043579734362\tValidation Loss: 0.7247857742984057\n",
            "Epoch 210  \tTraining Loss: 0.7234869201863384\tValidation Loss: 0.7241646013142357\n",
            "Epoch 211  \tTraining Loss: 0.7228729433874937\tValidation Loss: 0.7235468901151866\n",
            "Epoch 212  \tTraining Loss: 0.7222624081766642\tValidation Loss: 0.7229326213530821\n",
            "Epoch 213  \tTraining Loss: 0.7216552952623588\tValidation Loss: 0.7223217757880399\n",
            "Epoch 214  \tTraining Loss: 0.7210515854612232\tValidation Loss: 0.7217143342878675\n",
            "Epoch 215  \tTraining Loss: 0.7204512596974327\tValidation Loss: 0.7211102778274588\n",
            "Epoch 216  \tTraining Loss: 0.719854299002091\tValidation Loss: 0.7205095874881928\n",
            "Epoch 217  \tTraining Loss: 0.7192606845126297\tValidation Loss: 0.7199122444573389\n",
            "Epoch 218  \tTraining Loss: 0.7186703974722128\tValidation Loss: 0.7193182300274626\n",
            "Epoch 219  \tTraining Loss: 0.7180834192291445\tValidation Loss: 0.7187275255958364\n",
            "Epoch 220  \tTraining Loss: 0.7174997312362785\tValidation Loss: 0.7181401126638527\n",
            "Epoch 221  \tTraining Loss: 0.7169193150504335\tValidation Loss: 0.7175559728364416\n",
            "Epoch 222  \tTraining Loss: 0.7163421523318086\tValidation Loss: 0.7169750878214894\n",
            "Epoch 223  \tTraining Loss: 0.715768224843406\tValidation Loss: 0.7163974394292636\n",
            "Epoch 224  \tTraining Loss: 0.7151975144504533\tValidation Loss: 0.7158230095718382\n",
            "Epoch 225  \tTraining Loss: 0.7146300031198307\tValidation Loss: 0.7152517802625243\n",
            "Epoch 226  \tTraining Loss: 0.7140656729195011\tValidation Loss: 0.7146837336153026\n",
            "Epoch 227  \tTraining Loss: 0.7135045060179442\tValidation Loss: 0.7141188518442593\n",
            "Epoch 228  \tTraining Loss: 0.7129464846835921\tValidation Loss: 0.7135571172630262\n",
            "Epoch 229  \tTraining Loss: 0.7123915912842701\tValidation Loss: 0.7129985122842224\n",
            "Epoch 230  \tTraining Loss: 0.7118398082866383\tValidation Loss: 0.7124430194189001\n",
            "Epoch 231  \tTraining Loss: 0.711291118255639\tValidation Loss: 0.7118906212759937\n",
            "Epoch 232  \tTraining Loss: 0.7107455038539441\tValidation Loss: 0.7113413005617703\n",
            "Epoch 233  \tTraining Loss: 0.710202947841409\tValidation Loss: 0.7107950400792865\n",
            "Epoch 234  \tTraining Loss: 0.7096634330745268\tValidation Loss: 0.7102518227278444\n",
            "Epoch 235  \tTraining Loss: 0.7091269425058868\tValidation Loss: 0.7097116315024531\n",
            "Epoch 236  \tTraining Loss: 0.7085934591836361\tValidation Loss: 0.7091744494932937\n",
            "Epoch 237  \tTraining Loss: 0.7080629662509438\tValidation Loss: 0.7086402598851844\n",
            "Epoch 238  \tTraining Loss: 0.7075354469454681\tValidation Loss: 0.708109045957052\n",
            "Epoch 239  \tTraining Loss: 0.707010884598827\tValidation Loss: 0.7075807910814044\n",
            "Epoch 240  \tTraining Loss: 0.7064892626360717\tValidation Loss: 0.7070554787238054\n",
            "Epoch 241  \tTraining Loss: 0.7059705645751627\tValidation Loss: 0.7065330924423551\n",
            "Epoch 242  \tTraining Loss: 0.7054547740264484\tValidation Loss: 0.7060136158871706\n",
            "Epoch 243  \tTraining Loss: 0.7049418746921488\tValidation Loss: 0.7054970327998707\n",
            "Epoch 244  \tTraining Loss: 0.7044318503658383\tValidation Loss: 0.704983327013064\n",
            "Epoch 245  \tTraining Loss: 0.7039246849319365\tValidation Loss: 0.7044724824498386\n",
            "Epoch 246  \tTraining Loss: 0.7034203623651955\tValidation Loss: 0.7039644831232557\n",
            "Epoch 247  \tTraining Loss: 0.7029188667301968\tValidation Loss: 0.7034593131358451\n",
            "Epoch 248  \tTraining Loss: 0.7024201821808459\tValidation Loss: 0.7029569566791054\n",
            "Epoch 249  \tTraining Loss: 0.7019242929598715\tValidation Loss: 0.702457398033004\n",
            "Epoch 250  \tTraining Loss: 0.7014311833983293\tValidation Loss: 0.701960621565483\n",
            "Epoch 251  \tTraining Loss: 0.7009408379151045\tValidation Loss: 0.7014666117319659\n",
            "Epoch 252  \tTraining Loss: 0.7004532410164221\tValidation Loss: 0.7009753530748672\n",
            "Epoch 253  \tTraining Loss: 0.6999683772953552\tValidation Loss: 0.7004868302231062\n",
            "Epoch 254  \tTraining Loss: 0.6994862314313384\tValidation Loss: 0.7000010278916217\n",
            "Epoch 255  \tTraining Loss: 0.6990067881896852\tValidation Loss: 0.6995179308808901\n",
            "Epoch 256  \tTraining Loss: 0.6985300324211055\tValidation Loss: 0.6990375240764474\n",
            "Epoch 257  \tTraining Loss: 0.6980559490612267\tValidation Loss: 0.6985597924484115\n",
            "Epoch 258  \tTraining Loss: 0.6975845231301184\tValidation Loss: 0.6980847210510093\n",
            "Epoch 259  \tTraining Loss: 0.6971157397318187\tValidation Loss: 0.6976122950221054\n",
            "Epoch 260  \tTraining Loss: 0.6966495840538637\tValidation Loss: 0.6971424995827332\n",
            "Epoch 261  \tTraining Loss: 0.696186041366819\tValidation Loss: 0.6966753200366306\n",
            "Epoch 262  \tTraining Loss: 0.6957250970238149\tValidation Loss: 0.6962107417697746\n",
            "Epoch 263  \tTraining Loss: 0.6952667364600835\tValidation Loss: 0.6957487502499226\n",
            "Epoch 264  \tTraining Loss: 0.6948109451924979\tValidation Loss: 0.6952893310261533\n",
            "Epoch 265  \tTraining Loss: 0.6943577088191151\tValidation Loss: 0.6948324697284115\n",
            "Epoch 266  \tTraining Loss: 0.6939070130187208\tValidation Loss: 0.6943781520670557\n",
            "Epoch 267  \tTraining Loss: 0.6934588435503775\tValidation Loss: 0.6939263638324069\n",
            "Epoch 268  \tTraining Loss: 0.6930131862529728\tValidation Loss: 0.6934770908943011\n",
            "Epoch 269  \tTraining Loss: 0.6925700270447738\tValidation Loss: 0.6930303192016444\n",
            "Epoch 270  \tTraining Loss: 0.6921293519229811\tValidation Loss: 0.6925860347819697\n",
            "Epoch 271  \tTraining Loss: 0.691691146963287\tValidation Loss: 0.6921442237409962\n",
            "Epoch 272  \tTraining Loss: 0.6912553983194348\tValidation Loss: 0.6917048722621918\n",
            "Epoch 273  \tTraining Loss: 0.690822092222782\tValidation Loss: 0.6912679666063379\n",
            "Epoch 274  \tTraining Loss: 0.6903912149818645\tValidation Loss: 0.6908334931110958\n",
            "Epoch 275  \tTraining Loss: 0.6899627529819647\tValidation Loss: 0.6904014381905768\n",
            "Epoch 276  \tTraining Loss: 0.6895366926846808\tValidation Loss: 0.689971788334913\n",
            "Epoch 277  \tTraining Loss: 0.6891130206274995\tValidation Loss: 0.6895445301098332\n",
            "Epoch 278  \tTraining Loss: 0.6886917234233699\tValidation Loss: 0.6891196501562378\n",
            "Epoch 279  \tTraining Loss: 0.6882727877602813\tValidation Loss: 0.6886971351897789\n",
            "Epoch 280  \tTraining Loss: 0.687856200400842\tValidation Loss: 0.6882769720004412\n",
            "Epoch 281  \tTraining Loss: 0.6874419481818614\tValidation Loss: 0.687859147452126\n",
            "Epoch 282  \tTraining Loss: 0.6870300180139336\tValidation Loss: 0.6874436484822367\n",
            "Epoch 283  \tTraining Loss: 0.6866203968810245\tValidation Loss: 0.6870304621012676\n",
            "Epoch 284  \tTraining Loss: 0.6862130718400598\tValidation Loss: 0.686619575392395\n",
            "Epoch 285  \tTraining Loss: 0.6858080300205166\tValidation Loss: 0.6862109755110689\n",
            "Epoch 286  \tTraining Loss: 0.6854052586240162\tValidation Loss: 0.6858046496846097\n",
            "Epoch 287  \tTraining Loss: 0.6850047449239203\tValidation Loss: 0.6854005852118042\n",
            "Epoch 288  \tTraining Loss: 0.6846064762649281\tValidation Loss: 0.6849987694625068\n",
            "Epoch 289  \tTraining Loss: 0.6842104400626776\tValidation Loss: 0.6845991898772401\n",
            "Epoch 290  \tTraining Loss: 0.6838166238033466\tValidation Loss: 0.6842018339668008\n",
            "Epoch 291  \tTraining Loss: 0.6834250150432583\tValidation Loss: 0.6838066893118644\n",
            "Epoch 292  \tTraining Loss: 0.6830356014084872\tValidation Loss: 0.6834137435625952\n",
            "Epoch 293  \tTraining Loss: 0.6826483705944696\tValidation Loss: 0.6830229844382563\n",
            "Epoch 294  \tTraining Loss: 0.6822633103656127\tValidation Loss: 0.682634399726823\n",
            "Epoch 295  \tTraining Loss: 0.68188040855491\tValidation Loss: 0.6822479772845982\n",
            "Epoch 296  \tTraining Loss: 0.6814996530635556\tValidation Loss: 0.6818637050358289\n",
            "Epoch 297  \tTraining Loss: 0.6811210318605625\tValidation Loss: 0.6814815709723271\n",
            "Epoch 298  \tTraining Loss: 0.6807445329823821\tValidation Loss: 0.6811015631530898\n",
            "Epoch 299  \tTraining Loss: 0.6803701445325265\tValidation Loss: 0.6807236697039241\n",
            "Epoch 300  \tTraining Loss: 0.679997854681192\tValidation Loss: 0.6803478788170725\n",
            "Epoch 301  \tTraining Loss: 0.6796276516648864\tValidation Loss: 0.6799741787508403\n",
            "Epoch 302  \tTraining Loss: 0.6792595237860559\tValidation Loss: 0.679602557829227\n",
            "Epoch 303  \tTraining Loss: 0.6788934594127166\tValidation Loss: 0.6792330044415569\n",
            "Epoch 304  \tTraining Loss: 0.6785294469780867\tValidation Loss: 0.6788655070421146\n",
            "Epoch 305  \tTraining Loss: 0.6781674749802206\tValidation Loss: 0.6785000541497801\n",
            "Epoch 306  \tTraining Loss: 0.6778075319816459\tValidation Loss: 0.6781366343476678\n",
            "Epoch 307  \tTraining Loss: 0.6774496066090019\tValidation Loss: 0.6777752362827666\n",
            "Epoch 308  \tTraining Loss: 0.67709368755268\tValidation Loss: 0.6774158486655818\n",
            "Epoch 309  \tTraining Loss: 0.6767397635664664\tValidation Loss: 0.6770584602697801\n",
            "Epoch 310  \tTraining Loss: 0.6763878234671876\tValidation Loss: 0.6767030599318354\n",
            "Epoch 311  \tTraining Loss: 0.6760378561343553\tValidation Loss: 0.6763496365506774\n",
            "Epoch 312  \tTraining Loss: 0.6756898505098168\tValidation Loss: 0.6759981790873414\n",
            "Epoch 313  \tTraining Loss: 0.6753437955974047\tValidation Loss: 0.6756486765646206\n",
            "Epoch 314  \tTraining Loss: 0.6749996804625891\tValidation Loss: 0.6753011180667213\n",
            "Epoch 315  \tTraining Loss: 0.6746574942321331\tValidation Loss: 0.674955492738917\n",
            "Epoch 316  \tTraining Loss: 0.674317226093748\tValidation Loss: 0.6746117897872084\n",
            "Epoch 317  \tTraining Loss: 0.6739788652957535\tValidation Loss: 0.674269998477982\n",
            "Epoch 318  \tTraining Loss: 0.6736424011467356\tValidation Loss: 0.673930108137673\n",
            "Epoch 319  \tTraining Loss: 0.6733078230152107\tValidation Loss: 0.6735921081524278\n",
            "Epoch 320  \tTraining Loss: 0.6729751203292886\tValidation Loss: 0.6732559879677704\n",
            "Epoch 321  \tTraining Loss: 0.6726442825763391\tValidation Loss: 0.6729217370882703\n",
            "Epoch 322  \tTraining Loss: 0.6723152993026597\tValidation Loss: 0.672589345077211\n",
            "Epoch 323  \tTraining Loss: 0.6719881601131443\tValidation Loss: 0.6722588015562612\n",
            "Epoch 324  \tTraining Loss: 0.6716628546709565\tValidation Loss: 0.6719300962051491\n",
            "Epoch 325  \tTraining Loss: 0.6713393726972015\tValidation Loss: 0.6716032187613354\n",
            "Epoch 326  \tTraining Loss: 0.6710177039706021\tValidation Loss: 0.6712781590196918\n",
            "Epoch 327  \tTraining Loss: 0.6706978383271756\tValidation Loss: 0.6709549068321782\n",
            "Epoch 328  \tTraining Loss: 0.6703797656599118\tValidation Loss: 0.6706334521075237\n",
            "Epoch 329  \tTraining Loss: 0.6700634759184552\tValidation Loss: 0.6703137848109084\n",
            "Epoch 330  \tTraining Loss: 0.6697489591087865\tValidation Loss: 0.6699958949636479\n",
            "Epoch 331  \tTraining Loss: 0.6694362052929063\tValidation Loss: 0.6696797726428779\n",
            "Epoch 332  \tTraining Loss: 0.6691252045885226\tValidation Loss: 0.6693654079812424\n",
            "Epoch 333  \tTraining Loss: 0.6688159471687372\tValidation Loss: 0.6690527911665832\n",
            "Epoch 334  \tTraining Loss: 0.6685084232617351\tValidation Loss: 0.6687419124416294\n",
            "Epoch 335  \tTraining Loss: 0.6682026231504772\tValidation Loss: 0.6684327621036914\n",
            "Epoch 336  \tTraining Loss: 0.6678985371723917\tValidation Loss: 0.6681253305043551\n",
            "Epoch 337  \tTraining Loss: 0.6675961557190697\tValidation Loss: 0.6678196080491772\n",
            "Epoch 338  \tTraining Loss: 0.6672954692359614\tValidation Loss: 0.6675155851973836\n",
            "Epoch 339  \tTraining Loss: 0.6669964682220735\tValidation Loss: 0.667213252461569\n",
            "Epoch 340  \tTraining Loss: 0.6666991432296705\tValidation Loss: 0.6669126004073979\n",
            "Epoch 341  \tTraining Loss: 0.6664034848639747\tValidation Loss: 0.6666136196533066\n",
            "Epoch 342  \tTraining Loss: 0.6661094837828702\tValidation Loss: 0.6663163008702092\n",
            "Epoch 343  \tTraining Loss: 0.6658171306966069\tValidation Loss: 0.6660206347812029\n",
            "Epoch 344  \tTraining Loss: 0.6655264163675081\tValidation Loss: 0.6657266121612753\n",
            "Epoch 345  \tTraining Loss: 0.665237331609678\tValidation Loss: 0.6654342238370154\n",
            "Epoch 346  \tTraining Loss: 0.6649498672887111\tValidation Loss: 0.665143460686323\n",
            "Epoch 347  \tTraining Loss: 0.6646640143214041\tValidation Loss: 0.6648543136381224\n",
            "Epoch 348  \tTraining Loss: 0.6643797636754685\tValidation Loss: 0.6645667736720761\n",
            "Epoch 349  \tTraining Loss: 0.6640971063692461\tValidation Loss: 0.6642808318183014\n",
            "Epoch 350  \tTraining Loss: 0.6638160334714236\tValidation Loss: 0.663996479157087\n",
            "Epoch 351  \tTraining Loss: 0.6635365361007524\tValidation Loss: 0.6637137068186131\n",
            "Epoch 352  \tTraining Loss: 0.6632586054257654\tValidation Loss: 0.6634325059826708\n",
            "Epoch 353  \tTraining Loss: 0.6629822326645012\tValidation Loss: 0.6631528678783857\n",
            "Epoch 354  \tTraining Loss: 0.6627074090842233\tValidation Loss: 0.6628747837839403\n",
            "Epoch 355  \tTraining Loss: 0.6624341260011469\tValidation Loss: 0.6625982450263006\n",
            "Epoch 356  \tTraining Loss: 0.6621623747801625\tValidation Loss: 0.6623232429809418\n",
            "Epoch 357  \tTraining Loss: 0.6618921468345642\tValidation Loss: 0.6620497690715773\n",
            "Epoch 358  \tTraining Loss: 0.661623433625778\tValidation Loss: 0.6617778147698888\n",
            "Epoch 359  \tTraining Loss: 0.6613562266630919\tValidation Loss: 0.6615073715952572\n",
            "Epoch 360  \tTraining Loss: 0.6610905175033884\tValidation Loss: 0.6612384311144957\n",
            "Epoch 361  \tTraining Loss: 0.6608262977508764\tValidation Loss: 0.6609709849415841\n",
            "Epoch 362  \tTraining Loss: 0.6605635590568267\tValidation Loss: 0.6607050247374056\n",
            "Epoch 363  \tTraining Loss: 0.6603022931193088\tValidation Loss: 0.6604405422094823\n",
            "Epoch 364  \tTraining Loss: 0.6600424916829268\tValidation Loss: 0.6601775291117163\n",
            "Epoch 365  \tTraining Loss: 0.6597841465385601\tValidation Loss: 0.6599159772441281\n",
            "Epoch 366  \tTraining Loss: 0.6595272495231039\tValidation Loss: 0.6596558784525999\n",
            "Epoch 367  \tTraining Loss: 0.6592717925192105\tValidation Loss: 0.6593972246286174\n",
            "Epoch 368  \tTraining Loss: 0.6590177674550332\tValidation Loss: 0.6591400077090157\n",
            "Epoch 369  \tTraining Loss: 0.6587651663039714\tValidation Loss: 0.6588842196757246\n",
            "Epoch 370  \tTraining Loss: 0.6585139810844168\tValidation Loss: 0.6586298525555168\n",
            "Epoch 371  \tTraining Loss: 0.6582642038595012\tValidation Loss: 0.6583768984197556\n",
            "Epoch 372  \tTraining Loss: 0.658015826736846\tValidation Loss: 0.6581253493841467\n",
            "Epoch 373  \tTraining Loss: 0.6577688418683121\tValidation Loss: 0.657875197608489\n",
            "Epoch 374  \tTraining Loss: 0.657523241449753\tValidation Loss: 0.6576264352964281\n",
            "Epoch 375  \tTraining Loss: 0.6572790177207667\tValidation Loss: 0.6573790546952107\n",
            "Epoch 376  \tTraining Loss: 0.6570361629644523\tValidation Loss: 0.6571330480954405\n",
            "Epoch 377  \tTraining Loss: 0.6567946695071648\tValidation Loss: 0.656888407830835\n",
            "Epoch 378  \tTraining Loss: 0.6565545297182733\tValidation Loss: 0.656645126277985\n",
            "Epoch 379  \tTraining Loss: 0.6563157360099194\tValidation Loss: 0.6564031958561141\n",
            "Epoch 380  \tTraining Loss: 0.6560782808367781\tValidation Loss: 0.6561626090268398\n",
            "Epoch 381  \tTraining Loss: 0.6558421566958181\tValidation Loss: 0.6559233582939363\n",
            "Epoch 382  \tTraining Loss: 0.6556073561260669\tValidation Loss: 0.6556854362030986\n",
            "Epoch 383  \tTraining Loss: 0.655373871708373\tValidation Loss: 0.6554488353417078\n",
            "Epoch 384  \tTraining Loss: 0.6551416960651723\tValidation Loss: 0.6552135483385975\n",
            "Epoch 385  \tTraining Loss: 0.6549108218602547\tValidation Loss: 0.6549795678638219\n",
            "Epoch 386  \tTraining Loss: 0.6546812417985333\tValidation Loss: 0.6547468866284246\n",
            "Epoch 387  \tTraining Loss: 0.6544529486258123\tValidation Loss: 0.6545154973842102\n",
            "Epoch 388  \tTraining Loss: 0.654225935128559\tValidation Loss: 0.6542853929235146\n",
            "Epoch 389  \tTraining Loss: 0.6540001941336755\tValidation Loss: 0.6540565660789792\n",
            "Epoch 390  \tTraining Loss: 0.653775718508272\tValidation Loss: 0.6538290097233248\n",
            "Epoch 391  \tTraining Loss: 0.6535525011594411\tValidation Loss: 0.6536027167691272\n",
            "Epoch 392  \tTraining Loss: 0.6533305350340342\tValidation Loss: 0.6533776801685944\n",
            "Epoch 393  \tTraining Loss: 0.6531098131184386\tValidation Loss: 0.6531538929133442\n",
            "Epoch 394  \tTraining Loss: 0.6528903284383556\tValidation Loss: 0.6529313480341841\n",
            "Epoch 395  \tTraining Loss: 0.6526720740585804\tValidation Loss: 0.652710038600891\n",
            "Epoch 396  \tTraining Loss: 0.6524550430827827\tValidation Loss: 0.6524899577219946\n",
            "Epoch 397  \tTraining Loss: 0.6522392286532889\tValidation Loss: 0.6522710985445583\n",
            "Epoch 398  \tTraining Loss: 0.6520246239508652\tValidation Loss: 0.652053454253966\n",
            "Epoch 399  \tTraining Loss: 0.6518112221945029\tValidation Loss: 0.6518370180737053\n",
            "Epoch 400  \tTraining Loss: 0.6515990166412032\tValidation Loss: 0.6516217832651551\n",
            "lr, batch_size: (0.001, 400)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.6243534526955732\n",
            "Epoch 2  \tTraining Loss: 0.6211419364869208\tValidation Loss: 0.8706964125768494\n",
            "Epoch 3  \tTraining Loss: 0.8652956414574802\tValidation Loss: 0.6080005424691387\n",
            "Epoch 4  \tTraining Loss: 0.6057258164438752\tValidation Loss: 0.6340736540992257\n",
            "Epoch 5  \tTraining Loss: 0.6252237430173914\tValidation Loss: 0.8651971395052276\n",
            "Epoch 6  \tTraining Loss: 0.8631873101975883\tValidation Loss: 0.5740676554933143\n",
            "Epoch 7  \tTraining Loss: 0.5681604855792008\tValidation Loss: 0.5709372474182237\n",
            "Epoch 8  \tTraining Loss: 0.5578663203610648\tValidation Loss: 0.6604647027530745\n",
            "Epoch 9  \tTraining Loss: 0.6453815262912882\tValidation Loss: 8.510887002784191\n",
            "Epoch 10  \tTraining Loss: 8.554905179409724\tValidation Loss: 0.9342282716618441\n",
            "Epoch 11  \tTraining Loss: 0.9311494197660802\tValidation Loss: 0.9321786454884309\n",
            "Epoch 12  \tTraining Loss: 0.9291057032192446\tValidation Loss: 0.9282034960205785\n",
            "Epoch 13  \tTraining Loss: 0.9249627130767325\tValidation Loss: 0.9063776912725549\n",
            "Epoch 14  \tTraining Loss: 0.9025341770683879\tValidation Loss: 0.7597925493872184\n",
            "Epoch 15  \tTraining Loss: 0.7561155980652636\tValidation Loss: 0.8569698807397439\n",
            "Epoch 16  \tTraining Loss: 0.8620547787858478\tValidation Loss: 0.9301583208540838\n",
            "Epoch 17  \tTraining Loss: 0.9270153094757775\tValidation Loss: 0.9281158515036863\n",
            "Epoch 18  \tTraining Loss: 0.9250051223871008\tValidation Loss: 0.9245802837732365\n",
            "Epoch 19  \tTraining Loss: 0.9214338045315198\tValidation Loss: 0.9101549180178962\n",
            "Epoch 20  \tTraining Loss: 0.9068119994847061\tValidation Loss: 0.765848939127548\n",
            "Epoch 21  \tTraining Loss: 0.7638876288693668\tValidation Loss: 1.2008148593786083\n",
            "Epoch 22  \tTraining Loss: 1.20506386086261\tValidation Loss: 0.9238496145517081\n",
            "Epoch 23  \tTraining Loss: 0.9208215042376782\tValidation Loss: 0.9073228248763365\n",
            "Epoch 24  \tTraining Loss: 0.9041775590340153\tValidation Loss: 0.7271733403852984\n",
            "Epoch 25  \tTraining Loss: 0.7237622127170513\tValidation Loss: 1.3020207933124877\n",
            "Epoch 26  \tTraining Loss: 1.301939602883523\tValidation Loss: 0.923379957497202\n",
            "Epoch 27  \tTraining Loss: 0.9203567467873853\tValidation Loss: 0.9216504099658089\n",
            "Epoch 28  \tTraining Loss: 0.9186392350714089\tValidation Loss: 0.9199305235425486\n",
            "Epoch 29  \tTraining Loss: 0.9169313507025068\tValidation Loss: 0.9182202441866457\n",
            "Epoch 30  \tTraining Loss: 0.9152330397155088\tValidation Loss: 0.9165195181408842\n",
            "Epoch 31  \tTraining Loss: 0.9135442484477425\tValidation Loss: 0.9148282919491134\n",
            "Epoch 32  \tTraining Loss: 0.9118649235373344\tValidation Loss: 0.9131465124545632\n",
            "Epoch 33  \tTraining Loss: 0.9101950119215273\tValidation Loss: 0.9114741267981648\n",
            "Epoch 34  \tTraining Loss: 0.9085344608350017\tValidation Loss: 0.9098110824168839\n",
            "Epoch 35  \tTraining Loss: 0.9068832178082096\tValidation Loss: 0.9081573270420613\n",
            "Epoch 36  \tTraining Loss: 0.905241230665715\tValidation Loss: 0.9065128086977641\n",
            "Epoch 37  \tTraining Loss: 0.9036084475245472\tValidation Loss: 0.904877475699145\n",
            "Epoch 38  \tTraining Loss: 0.9019848167925602\tValidation Loss: 0.9032512766508122\n",
            "Epoch 39  \tTraining Loss: 0.9003702871668022\tValidation Loss: 0.9016341604452063\n",
            "Epoch 40  \tTraining Loss: 0.8987648076318966\tValidation Loss: 0.9000260762609895\n",
            "Epoch 41  \tTraining Loss: 0.8971683274584276\tValidation Loss: 0.89842697356144\n",
            "Epoch 42  \tTraining Loss: 0.895580796201338\tValidation Loss: 0.8968368020928583\n",
            "Epoch 43  \tTraining Loss: 0.8940021636983363\tValidation Loss: 0.8952555118829819\n",
            "Epoch 44  \tTraining Loss: 0.8924323800683104\tValidation Loss: 0.8936830532394076\n",
            "Epoch 45  \tTraining Loss: 0.8908713957097525\tValidation Loss: 0.8921193767480233\n",
            "Epoch 46  \tTraining Loss: 0.8893191612991906\tValidation Loss: 0.8905644332714502\n",
            "Epoch 47  \tTraining Loss: 0.8877756277896311\tValidation Loss: 0.8890181739474907\n",
            "Epoch 48  \tTraining Loss: 0.8862407464090087\tValidation Loss: 0.8874805501875878\n",
            "Epoch 49  \tTraining Loss: 0.884714468658645\tValidation Loss: 0.8859515136752909\n",
            "Epoch 50  \tTraining Loss: 0.8831967463117163\tValidation Loss: 0.884431016364732\n",
            "Epoch 51  \tTraining Loss: 0.8816875314117302\tValidation Loss: 0.8829190104791098\n",
            "Epoch 52  \tTraining Loss: 0.8801867762710086\tValidation Loss: 0.8814154485091813\n",
            "Epoch 53  \tTraining Loss: 0.8786944334691841\tValidation Loss: 0.8799202832117639\n",
            "Epoch 54  \tTraining Loss: 0.8772104558516973\tValidation Loss: 0.8784334676082435\n",
            "Epoch 55  \tTraining Loss: 0.8757347965283108\tValidation Loss: 0.8769549549830928\n",
            "Epoch 56  \tTraining Loss: 0.8742674088716254\tValidation Loss: 0.8754846988823977\n",
            "Epoch 57  \tTraining Loss: 0.8728082465156076\tValidation Loss: 0.8740226531123906\n",
            "Epoch 58  \tTraining Loss: 0.8713572633541237\tValidation Loss: 0.872568771737994\n",
            "Epoch 59  \tTraining Loss: 0.8699144135394834\tValidation Loss: 0.8711230090813695\n",
            "Epoch 60  \tTraining Loss: 0.8684796514809922\tValidation Loss: 0.8696853197204786\n",
            "Epoch 61  \tTraining Loss: 0.8670529318435095\tValidation Loss: 0.868255658487648\n",
            "Epoch 62  \tTraining Loss: 0.8656342095460153\tValidation Loss: 0.8668339804681445\n",
            "Epoch 63  \tTraining Loss: 0.864223439760189\tValidation Loss: 0.8654202409987594\n",
            "Epoch 64  \tTraining Loss: 0.8628205779089888\tValidation Loss: 0.8640143956663967\n",
            "Epoch 65  \tTraining Loss: 0.8614255796652468\tValidation Loss: 0.8626164003066747\n",
            "Epoch 66  \tTraining Loss: 0.860038400950266\tValidation Loss: 0.8612262110025302\n",
            "Epoch 67  \tTraining Loss: 0.8586589979324282\tValidation Loss: 0.8598437840828338\n",
            "Epoch 68  \tTraining Loss: 0.8572873270258098\tValidation Loss: 0.8584690761210124\n",
            "Epoch 69  \tTraining Loss: 0.8559233448888031\tValidation Loss: 0.8571020439336782\n",
            "Epoch 70  \tTraining Loss: 0.8545670084227482\tValidation Loss: 0.8557426445792667\n",
            "Epoch 71  \tTraining Loss: 0.8532182747705707\tValidation Loss: 0.8543908353566817\n",
            "Epoch 72  \tTraining Loss: 0.8518771013154265\tValidation Loss: 0.8530465738039484\n",
            "Epoch 73  \tTraining Loss: 0.8505434456793572\tValidation Loss: 0.8517098176968728\n",
            "Epoch 74  \tTraining Loss: 0.8492172657219497\tValidation Loss: 0.8503805250477108\n",
            "Epoch 75  \tTraining Loss: 0.8478985195390043\tValidation Loss: 0.8490586541038424\n",
            "Epoch 76  \tTraining Loss: 0.8465871654612124\tValidation Loss: 0.8477441633464543\n",
            "Epoch 77  \tTraining Loss: 0.8452831620528375\tValidation Loss: 0.846437011489231\n",
            "Epoch 78  \tTraining Loss: 0.843986468110408\tValidation Loss: 0.8451371574770516\n",
            "Epoch 79  \tTraining Loss: 0.8426970426614138\tValidation Loss: 0.8438445604846941\n",
            "Epoch 80  \tTraining Loss: 0.841414844963013\tValidation Loss: 0.8425591799155485\n",
            "Epoch 81  \tTraining Loss: 0.840139834500743\tValidation Loss: 0.8412809754003345\n",
            "Epoch 82  \tTraining Loss: 0.8388719709872421\tValidation Loss: 0.8400099067958291\n",
            "Epoch 83  \tTraining Loss: 0.8376112143609743\tValidation Loss: 0.8387459341835997\n",
            "Epoch 84  \tTraining Loss: 0.8363575247849656\tValidation Loss: 0.8374890178687442\n",
            "Epoch 85  \tTraining Loss: 0.8351108626455442\tValidation Loss: 0.83623911837864\n",
            "Epoch 86  \tTraining Loss: 0.8338711885510885\tValidation Loss: 0.8349961964616968\n",
            "Epoch 87  \tTraining Loss: 0.8326384633307837\tValidation Loss: 0.8337602130861199\n",
            "Epoch 88  \tTraining Loss: 0.8314126480333824\tValidation Loss: 0.8325311294386779\n",
            "Epoch 89  \tTraining Loss: 0.8301937039259745\tValidation Loss: 0.8313089069234786\n",
            "Epoch 90  \tTraining Loss: 0.8289815924927648\tValidation Loss: 0.8300935071607509\n",
            "Epoch 91  \tTraining Loss: 0.8277762754338537\tValidation Loss: 0.8288848919856346\n",
            "Epoch 92  \tTraining Loss: 0.8265777146640277\tValidation Loss: 0.8276830234469758\n",
            "Epoch 93  \tTraining Loss: 0.8253858723115574\tValidation Loss: 0.8264878638061303\n",
            "Epoch 94  \tTraining Loss: 0.8242007107169989\tValidation Loss: 0.8252993755357718\n",
            "Epoch 95  \tTraining Loss: 0.823022192432005\tValidation Loss: 0.8241175213187097\n",
            "Epoch 96  \tTraining Loss: 0.8218502802181415\tValidation Loss: 0.8229422640467103\n",
            "Epoch 97  \tTraining Loss: 0.8206849370457109\tValidation Loss: 0.8217735668193266\n",
            "Epoch 98  \tTraining Loss: 0.819526126092582\tValidation Loss: 0.8206113929427351\n",
            "Epoch 99  \tTraining Loss: 0.818373810743026\tValidation Loss: 0.8194557059285766\n",
            "Epoch 100  \tTraining Loss: 0.8172279545865611\tValidation Loss: 0.8183064694928064\n",
            "Epoch 101  \tTraining Loss: 0.8160885214168001\tValidation Loss: 0.8171636475545492\n",
            "Epoch 102  \tTraining Loss: 0.8149554752303074\tValidation Loss: 0.8160272042349606\n",
            "Epoch 103  \tTraining Loss: 0.8138287802254619\tValidation Loss: 0.8148971038560955\n",
            "Epoch 104  \tTraining Loss: 0.8127084008013236\tValidation Loss: 0.8137733109397829\n",
            "Epoch 105  \tTraining Loss: 0.8115943015565118\tValidation Loss: 0.812655790206506\n",
            "Epoch 106  \tTraining Loss: 0.8104864472880837\tValidation Loss: 0.8115445065742901\n",
            "Epoch 107  \tTraining Loss: 0.8093848029904237\tValidation Loss: 0.8104394251575948\n",
            "Epoch 108  \tTraining Loss: 0.8082893338541367\tValidation Loss: 0.8093405112662146\n",
            "Epoch 109  \tTraining Loss: 0.8072000052649481\tValidation Loss: 0.8082477304041839\n",
            "Epoch 110  \tTraining Loss: 0.8061167828026107\tValidation Loss: 0.8071610482686888\n",
            "Epoch 111  \tTraining Loss: 0.8050396322398167\tValidation Loss: 0.806080430748986\n",
            "Epoch 112  \tTraining Loss: 0.8039685195411157\tValidation Loss: 0.8050058439253254\n",
            "Epoch 113  \tTraining Loss: 0.8029034108618404\tValidation Loss: 0.8039372540678803\n",
            "Epoch 114  \tTraining Loss: 0.8018442725470356\tValidation Loss: 0.8028746276356844\n",
            "Epoch 115  \tTraining Loss: 0.8007910711303967\tValidation Loss: 0.801817931275573\n",
            "Epoch 116  \tTraining Loss: 0.7997437733332109\tValidation Loss: 0.8007671318211305\n",
            "Epoch 117  \tTraining Loss: 0.7987023460633059\tValidation Loss: 0.7997221962916454\n",
            "Epoch 118  \tTraining Loss: 0.7976667564140045\tValidation Loss: 0.7986830918910687\n",
            "Epoch 119  \tTraining Loss: 0.7966369716630846\tValidation Loss: 0.79764978600698\n",
            "Epoch 120  \tTraining Loss: 0.7956129592717458\tValidation Loss: 0.7966222462095588\n",
            "Epoch 121  \tTraining Loss: 0.7945946868835801\tValidation Loss: 0.7956004402505611\n",
            "Epoch 122  \tTraining Loss: 0.793582122323551\tValidation Loss: 0.7945843360623027\n",
            "Epoch 123  \tTraining Loss: 0.7925752335969755\tValidation Loss: 0.7935739017566471\n",
            "Epoch 124  \tTraining Loss: 0.7915739888885143\tValidation Loss: 0.7925691056240004\n",
            "Epoch 125  \tTraining Loss: 0.7905783565611648\tValidation Loss: 0.7915699161323105\n",
            "Epoch 126  \tTraining Loss: 0.7895883051552642\tValidation Loss: 0.7905763019260721\n",
            "Epoch 127  \tTraining Loss: 0.7886038033874927\tValidation Loss: 0.7895882318253388\n",
            "Epoch 128  \tTraining Loss: 0.7876248201498862\tValidation Loss: 0.7886056748247384\n",
            "Epoch 129  \tTraining Loss: 0.7866513245088539\tValidation Loss: 0.7876286000924959\n",
            "Epoch 130  \tTraining Loss: 0.7856832857041998\tValidation Loss: 0.7866569769694604\n",
            "Epoch 131  \tTraining Loss: 0.784720673148151\tValidation Loss: 0.7856907749681376\n",
            "Epoch 132  \tTraining Loss: 0.7837634564243922\tValidation Loss: 0.7847299637717298\n",
            "Epoch 133  \tTraining Loss: 0.782811605287103\tValidation Loss: 0.7837745132331773\n",
            "Epoch 134  \tTraining Loss: 0.7818650896600042\tValidation Loss: 0.78282439337421\n",
            "Epoch 135  \tTraining Loss: 0.7809238796354045\tValidation Loss: 0.7818795743843996\n",
            "Epoch 136  \tTraining Loss: 0.7799879454732592\tValidation Loss: 0.7809400266202207\n",
            "Epoch 137  \tTraining Loss: 0.7790572576002271\tValidation Loss: 0.7800057206041152\n",
            "Epoch 138  \tTraining Loss: 0.7781317866087385\tValidation Loss: 0.7790766270235635\n",
            "Epoch 139  \tTraining Loss: 0.7772115032560645\tValidation Loss: 0.7781527167301581\n",
            "Epoch 140  \tTraining Loss: 0.7762963784633937\tValidation Loss: 0.7772339607386859\n",
            "Epoch 141  \tTraining Loss: 0.775386383314913\tValidation Loss: 0.7763203302262134\n",
            "Epoch 142  \tTraining Loss: 0.7744814890568942\tValidation Loss: 0.7754117965311776\n",
            "Epoch 143  \tTraining Loss: 0.7735816670967858\tValidation Loss: 0.7745083311524816\n",
            "Epoch 144  \tTraining Loss: 0.7726868890023083\tValidation Loss: 0.773609905748597\n",
            "Epoch 145  \tTraining Loss: 0.7717971265005573\tValidation Loss: 0.7727164921366677\n",
            "Epoch 146  \tTraining Loss: 0.7709123514771089\tValidation Loss: 0.7718280622916236\n",
            "Epoch 147  \tTraining Loss: 0.7700325359751322\tValidation Loss: 0.7709445883452947\n",
            "Epoch 148  \tTraining Loss: 0.7691576521945059\tValidation Loss: 0.7700660425855332\n",
            "Epoch 149  \tTraining Loss: 0.7682876724909391\tValidation Loss: 0.769192397455339\n",
            "Epoch 150  \tTraining Loss: 0.7674225693750985\tValidation Loss: 0.7683236255519912\n",
            "Epoch 151  \tTraining Loss: 0.7665623155117401\tValidation Loss: 0.7674596996261828\n",
            "Epoch 152  \tTraining Loss: 0.7657068837188438\tValidation Loss: 0.7666005925811621\n",
            "Epoch 153  \tTraining Loss: 0.7648562469667574\tValidation Loss: 0.7657462774718778\n",
            "Epoch 154  \tTraining Loss: 0.7640103783773389\tValidation Loss: 0.7648967275041296\n",
            "Epoch 155  \tTraining Loss: 0.7631692512231105\tValidation Loss: 0.7640519160337217\n",
            "Epoch 156  \tTraining Loss: 0.7623328389264126\tValidation Loss: 0.763211816565625\n",
            "Epoch 157  \tTraining Loss: 0.7615011150585639\tValidation Loss: 0.762376402753139\n",
            "Epoch 158  \tTraining Loss: 0.7606740533390258\tValidation Loss: 0.7615456483970623\n",
            "Epoch 159  \tTraining Loss: 0.759851627634574\tValidation Loss: 0.7607195274448659\n",
            "Epoch 160  \tTraining Loss: 0.7590338119584707\tValidation Loss: 0.759898013989872\n",
            "Epoch 161  \tTraining Loss: 0.7582205804696448\tValidation Loss: 0.7590810822704361\n",
            "Epoch 162  \tTraining Loss: 0.757411907471874\tValidation Loss: 0.7582687066691349\n",
            "Epoch 163  \tTraining Loss: 0.7566077674129748\tValidation Loss: 0.7574608617119589\n",
            "Epoch 164  \tTraining Loss: 0.7558081348839929\tValidation Loss: 0.7566575220675086\n",
            "Epoch 165  \tTraining Loss: 0.7550129846184022\tValidation Loss: 0.7558586625461955\n",
            "Epoch 166  \tTraining Loss: 0.7542222914913054\tValidation Loss: 0.755064258099448\n",
            "Epoch 167  \tTraining Loss: 0.7534360305186407\tValidation Loss: 0.7542742838189213\n",
            "Epoch 168  \tTraining Loss: 0.7526541768563914\tValidation Loss: 0.7534887149357117\n",
            "Epoch 169  \tTraining Loss: 0.7518767057998027\tValidation Loss: 0.7527075268195758\n",
            "Epoch 170  \tTraining Loss: 0.7511035927825985\tValidation Loss: 0.751930694978154\n",
            "Epoch 171  \tTraining Loss: 0.7503348133762077\tValidation Loss: 0.751158195056197\n",
            "Epoch 172  \tTraining Loss: 0.7495703432889907\tValidation Loss: 0.7503900028347992\n",
            "Epoch 173  \tTraining Loss: 0.7488101583654727\tValidation Loss: 0.7496260942306338\n",
            "Epoch 174  \tTraining Loss: 0.7480542345855795\tValidation Loss: 0.7488664452951935\n",
            "Epoch 175  \tTraining Loss: 0.7473025480638799\tValidation Loss: 0.7481110322140365\n",
            "Epoch 176  \tTraining Loss: 0.7465550750488295\tValidation Loss: 0.7473598313060339\n",
            "Epoch 177  \tTraining Loss: 0.7458117919220212\tValidation Loss: 0.746612819022623\n",
            "Epoch 178  \tTraining Loss: 0.7450726751974388\tValidation Loss: 0.7458699719470663\n",
            "Epoch 179  \tTraining Loss: 0.7443377015207148\tValidation Loss: 0.7451312667937113\n",
            "Epoch 180  \tTraining Loss: 0.7436068476683912\tValidation Loss: 0.7443966804072575\n",
            "Epoch 181  \tTraining Loss: 0.7428800905471885\tValidation Loss: 0.743666189762025\n",
            "Epoch 182  \tTraining Loss: 0.7421574071932736\tValidation Loss: 0.7429397719612294\n",
            "Epoch 183  \tTraining Loss: 0.7414387747715345\tValidation Loss: 0.7422174042362593\n",
            "Epoch 184  \tTraining Loss: 0.7407241705748601\tValidation Loss: 0.7414990639459584\n",
            "Epoch 185  \tTraining Loss: 0.7400135720234212\tValidation Loss: 0.7407847285759122\n",
            "Epoch 186  \tTraining Loss: 0.739306956663958\tValidation Loss: 0.7400743757377364\n",
            "Epoch 187  \tTraining Loss: 0.7386043021690701\tValidation Loss: 0.7393679831683727\n",
            "Epoch 188  \tTraining Loss: 0.7379055863365116\tValidation Loss: 0.738665528729386\n",
            "Epoch 189  \tTraining Loss: 0.7372107870884886\tValidation Loss: 0.7379669904062657\n",
            "Epoch 190  \tTraining Loss: 0.7365198824709625\tValidation Loss: 0.7372723463077331\n",
            "Epoch 191  \tTraining Loss: 0.7358328506529558\tValidation Loss: 0.7365815746650495\n",
            "Epoch 192  \tTraining Loss: 0.7351496699258625\tValidation Loss: 0.7358946538313306\n",
            "Epoch 193  \tTraining Loss: 0.734470318702762\tValidation Loss: 0.7352115622808643\n",
            "Epoch 194  \tTraining Loss: 0.7337947755177374\tValidation Loss: 0.7345322786084308\n",
            "Epoch 195  \tTraining Loss: 0.7331230190251969\tValidation Loss: 0.7338567815286289\n",
            "Epoch 196  \tTraining Loss: 0.732455027999199\tValidation Loss: 0.7331850498752037\n",
            "Epoch 197  \tTraining Loss: 0.7317907813327826\tValidation Loss: 0.7325170626003801\n",
            "Epoch 198  \tTraining Loss: 0.7311302580372991\tValidation Loss: 0.7318527987741987\n",
            "Epoch 199  \tTraining Loss: 0.7304734372417505\tValidation Loss: 0.7311922375838563\n",
            "Epoch 200  \tTraining Loss: 0.7298202981921288\tValidation Loss: 0.7305353583330486\n",
            "Epoch 201  \tTraining Loss: 0.7291708202507602\tValidation Loss: 0.7298821404413189\n",
            "Epoch 202  \tTraining Loss: 0.7285249828956543\tValidation Loss: 0.7292325634434087\n",
            "Epoch 203  \tTraining Loss: 0.7278827657198533\tValidation Loss: 0.7285866069886118\n",
            "Epoch 204  \tTraining Loss: 0.7272441484307909\tValidation Loss: 0.7279442508401339\n",
            "Epoch 205  \tTraining Loss: 0.7266091108496463\tValidation Loss: 0.7273054748744536\n",
            "Epoch 206  \tTraining Loss: 0.7259776329107114\tValidation Loss: 0.7266702590806877\n",
            "Epoch 207  \tTraining Loss: 0.7253496946607525\tValidation Loss: 0.7260385835599616\n",
            "Epoch 208  \tTraining Loss: 0.7247252762583835\tValidation Loss: 0.7254104285247804\n",
            "Epoch 209  \tTraining Loss: 0.7241043579734362\tValidation Loss: 0.7247857742984057\n",
            "Epoch 210  \tTraining Loss: 0.7234869201863384\tValidation Loss: 0.7241646013142357\n",
            "Epoch 211  \tTraining Loss: 0.7228729433874937\tValidation Loss: 0.7235468901151866\n",
            "Epoch 212  \tTraining Loss: 0.7222624081766642\tValidation Loss: 0.7229326213530821\n",
            "Epoch 213  \tTraining Loss: 0.7216552952623588\tValidation Loss: 0.7223217757880399\n",
            "Epoch 214  \tTraining Loss: 0.7210515854612232\tValidation Loss: 0.7217143342878675\n",
            "Epoch 215  \tTraining Loss: 0.7204512596974327\tValidation Loss: 0.7211102778274588\n",
            "Epoch 216  \tTraining Loss: 0.719854299002091\tValidation Loss: 0.7205095874881928\n",
            "Epoch 217  \tTraining Loss: 0.7192606845126297\tValidation Loss: 0.7199122444573389\n",
            "Epoch 218  \tTraining Loss: 0.7186703974722128\tValidation Loss: 0.7193182300274626\n",
            "Epoch 219  \tTraining Loss: 0.7180834192291445\tValidation Loss: 0.7187275255958364\n",
            "Epoch 220  \tTraining Loss: 0.7174997312362785\tValidation Loss: 0.7181401126638527\n",
            "Epoch 221  \tTraining Loss: 0.7169193150504335\tValidation Loss: 0.7175559728364416\n",
            "Epoch 222  \tTraining Loss: 0.7163421523318086\tValidation Loss: 0.7169750878214894\n",
            "Epoch 223  \tTraining Loss: 0.715768224843406\tValidation Loss: 0.7163974394292636\n",
            "Epoch 224  \tTraining Loss: 0.7151975144504533\tValidation Loss: 0.7158230095718382\n",
            "Epoch 225  \tTraining Loss: 0.7146300031198307\tValidation Loss: 0.7152517802625243\n",
            "Epoch 226  \tTraining Loss: 0.7140656729195011\tValidation Loss: 0.7146837336153026\n",
            "Epoch 227  \tTraining Loss: 0.7135045060179442\tValidation Loss: 0.7141188518442593\n",
            "Epoch 228  \tTraining Loss: 0.7129464846835921\tValidation Loss: 0.7135571172630262\n",
            "Epoch 229  \tTraining Loss: 0.7123915912842701\tValidation Loss: 0.7129985122842224\n",
            "Epoch 230  \tTraining Loss: 0.7118398082866383\tValidation Loss: 0.7124430194189001\n",
            "Epoch 231  \tTraining Loss: 0.711291118255639\tValidation Loss: 0.7118906212759937\n",
            "Epoch 232  \tTraining Loss: 0.7107455038539441\tValidation Loss: 0.7113413005617703\n",
            "Epoch 233  \tTraining Loss: 0.710202947841409\tValidation Loss: 0.7107950400792865\n",
            "Epoch 234  \tTraining Loss: 0.7096634330745268\tValidation Loss: 0.7102518227278444\n",
            "Epoch 235  \tTraining Loss: 0.7091269425058868\tValidation Loss: 0.7097116315024531\n",
            "Epoch 236  \tTraining Loss: 0.7085934591836361\tValidation Loss: 0.7091744494932937\n",
            "Epoch 237  \tTraining Loss: 0.7080629662509438\tValidation Loss: 0.7086402598851844\n",
            "Epoch 238  \tTraining Loss: 0.7075354469454681\tValidation Loss: 0.708109045957052\n",
            "Epoch 239  \tTraining Loss: 0.707010884598827\tValidation Loss: 0.7075807910814044\n",
            "Epoch 240  \tTraining Loss: 0.7064892626360717\tValidation Loss: 0.7070554787238054\n",
            "Epoch 241  \tTraining Loss: 0.7059705645751627\tValidation Loss: 0.7065330924423551\n",
            "Epoch 242  \tTraining Loss: 0.7054547740264484\tValidation Loss: 0.7060136158871706\n",
            "Epoch 243  \tTraining Loss: 0.7049418746921488\tValidation Loss: 0.7054970327998707\n",
            "Epoch 244  \tTraining Loss: 0.7044318503658383\tValidation Loss: 0.704983327013064\n",
            "Epoch 245  \tTraining Loss: 0.7039246849319365\tValidation Loss: 0.7044724824498386\n",
            "Epoch 246  \tTraining Loss: 0.7034203623651955\tValidation Loss: 0.7039644831232557\n",
            "Epoch 247  \tTraining Loss: 0.7029188667301968\tValidation Loss: 0.7034593131358451\n",
            "Epoch 248  \tTraining Loss: 0.7024201821808459\tValidation Loss: 0.7029569566791054\n",
            "Epoch 249  \tTraining Loss: 0.7019242929598715\tValidation Loss: 0.702457398033004\n",
            "Epoch 250  \tTraining Loss: 0.7014311833983293\tValidation Loss: 0.701960621565483\n",
            "Epoch 251  \tTraining Loss: 0.7009408379151045\tValidation Loss: 0.7014666117319659\n",
            "Epoch 252  \tTraining Loss: 0.7004532410164221\tValidation Loss: 0.7009753530748672\n",
            "Epoch 253  \tTraining Loss: 0.6999683772953552\tValidation Loss: 0.7004868302231062\n",
            "Epoch 254  \tTraining Loss: 0.6994862314313384\tValidation Loss: 0.7000010278916217\n",
            "Epoch 255  \tTraining Loss: 0.6990067881896852\tValidation Loss: 0.6995179308808901\n",
            "Epoch 256  \tTraining Loss: 0.6985300324211055\tValidation Loss: 0.6990375240764474\n",
            "Epoch 257  \tTraining Loss: 0.6980559490612267\tValidation Loss: 0.6985597924484115\n",
            "Epoch 258  \tTraining Loss: 0.6975845231301184\tValidation Loss: 0.6980847210510093\n",
            "Epoch 259  \tTraining Loss: 0.6971157397318187\tValidation Loss: 0.6976122950221054\n",
            "Epoch 260  \tTraining Loss: 0.6966495840538637\tValidation Loss: 0.6971424995827332\n",
            "Epoch 261  \tTraining Loss: 0.696186041366819\tValidation Loss: 0.6966753200366306\n",
            "Epoch 262  \tTraining Loss: 0.6957250970238149\tValidation Loss: 0.6962107417697746\n",
            "Epoch 263  \tTraining Loss: 0.6952667364600835\tValidation Loss: 0.6957487502499226\n",
            "Epoch 264  \tTraining Loss: 0.6948109451924979\tValidation Loss: 0.6952893310261533\n",
            "Epoch 265  \tTraining Loss: 0.6943577088191151\tValidation Loss: 0.6948324697284115\n",
            "Epoch 266  \tTraining Loss: 0.6939070130187208\tValidation Loss: 0.6943781520670557\n",
            "Epoch 267  \tTraining Loss: 0.6934588435503775\tValidation Loss: 0.6939263638324069\n",
            "Epoch 268  \tTraining Loss: 0.6930131862529728\tValidation Loss: 0.6934770908943011\n",
            "Epoch 269  \tTraining Loss: 0.6925700270447738\tValidation Loss: 0.6930303192016444\n",
            "Epoch 270  \tTraining Loss: 0.6921293519229811\tValidation Loss: 0.6925860347819697\n",
            "Epoch 271  \tTraining Loss: 0.691691146963287\tValidation Loss: 0.6921442237409962\n",
            "Epoch 272  \tTraining Loss: 0.6912553983194348\tValidation Loss: 0.6917048722621918\n",
            "Epoch 273  \tTraining Loss: 0.690822092222782\tValidation Loss: 0.6912679666063379\n",
            "Epoch 274  \tTraining Loss: 0.6903912149818645\tValidation Loss: 0.6908334931110958\n",
            "Epoch 275  \tTraining Loss: 0.6899627529819647\tValidation Loss: 0.6904014381905768\n",
            "Epoch 276  \tTraining Loss: 0.6895366926846808\tValidation Loss: 0.689971788334913\n",
            "Epoch 277  \tTraining Loss: 0.6891130206274995\tValidation Loss: 0.6895445301098332\n",
            "Epoch 278  \tTraining Loss: 0.6886917234233699\tValidation Loss: 0.6891196501562378\n",
            "Epoch 279  \tTraining Loss: 0.6882727877602813\tValidation Loss: 0.6886971351897789\n",
            "Epoch 280  \tTraining Loss: 0.687856200400842\tValidation Loss: 0.6882769720004412\n",
            "Epoch 281  \tTraining Loss: 0.6874419481818614\tValidation Loss: 0.687859147452126\n",
            "Epoch 282  \tTraining Loss: 0.6870300180139336\tValidation Loss: 0.6874436484822367\n",
            "Epoch 283  \tTraining Loss: 0.6866203968810245\tValidation Loss: 0.6870304621012676\n",
            "Epoch 284  \tTraining Loss: 0.6862130718400598\tValidation Loss: 0.686619575392395\n",
            "Epoch 285  \tTraining Loss: 0.6858080300205166\tValidation Loss: 0.6862109755110689\n",
            "Epoch 286  \tTraining Loss: 0.6854052586240162\tValidation Loss: 0.6858046496846097\n",
            "Epoch 287  \tTraining Loss: 0.6850047449239203\tValidation Loss: 0.6854005852118042\n",
            "Epoch 288  \tTraining Loss: 0.6846064762649281\tValidation Loss: 0.6849987694625068\n",
            "Epoch 289  \tTraining Loss: 0.6842104400626776\tValidation Loss: 0.6845991898772401\n",
            "Epoch 290  \tTraining Loss: 0.6838166238033466\tValidation Loss: 0.6842018339668008\n",
            "Epoch 291  \tTraining Loss: 0.6834250150432583\tValidation Loss: 0.6838066893118644\n",
            "Epoch 292  \tTraining Loss: 0.6830356014084872\tValidation Loss: 0.6834137435625952\n",
            "Epoch 293  \tTraining Loss: 0.6826483705944696\tValidation Loss: 0.6830229844382563\n",
            "Epoch 294  \tTraining Loss: 0.6822633103656127\tValidation Loss: 0.682634399726823\n",
            "Epoch 295  \tTraining Loss: 0.68188040855491\tValidation Loss: 0.6822479772845982\n",
            "Epoch 296  \tTraining Loss: 0.6814996530635556\tValidation Loss: 0.6818637050358289\n",
            "Epoch 297  \tTraining Loss: 0.6811210318605625\tValidation Loss: 0.6814815709723271\n",
            "Epoch 298  \tTraining Loss: 0.6807445329823821\tValidation Loss: 0.6811015631530898\n",
            "Epoch 299  \tTraining Loss: 0.6803701445325265\tValidation Loss: 0.6807236697039241\n",
            "Epoch 300  \tTraining Loss: 0.679997854681192\tValidation Loss: 0.6803478788170725\n",
            "Epoch 301  \tTraining Loss: 0.6796276516648864\tValidation Loss: 0.6799741787508403\n",
            "Epoch 302  \tTraining Loss: 0.6792595237860559\tValidation Loss: 0.679602557829227\n",
            "Epoch 303  \tTraining Loss: 0.6788934594127166\tValidation Loss: 0.6792330044415569\n",
            "Epoch 304  \tTraining Loss: 0.6785294469780867\tValidation Loss: 0.6788655070421146\n",
            "Epoch 305  \tTraining Loss: 0.6781674749802206\tValidation Loss: 0.6785000541497801\n",
            "Epoch 306  \tTraining Loss: 0.6778075319816459\tValidation Loss: 0.6781366343476678\n",
            "Epoch 307  \tTraining Loss: 0.6774496066090019\tValidation Loss: 0.6777752362827666\n",
            "Epoch 308  \tTraining Loss: 0.67709368755268\tValidation Loss: 0.6774158486655818\n",
            "Epoch 309  \tTraining Loss: 0.6767397635664664\tValidation Loss: 0.6770584602697801\n",
            "Epoch 310  \tTraining Loss: 0.6763878234671876\tValidation Loss: 0.6767030599318354\n",
            "Epoch 311  \tTraining Loss: 0.6760378561343553\tValidation Loss: 0.6763496365506774\n",
            "Epoch 312  \tTraining Loss: 0.6756898505098168\tValidation Loss: 0.6759981790873414\n",
            "Epoch 313  \tTraining Loss: 0.6753437955974047\tValidation Loss: 0.6756486765646206\n",
            "Epoch 314  \tTraining Loss: 0.6749996804625891\tValidation Loss: 0.6753011180667213\n",
            "Epoch 315  \tTraining Loss: 0.6746574942321331\tValidation Loss: 0.674955492738917\n",
            "Epoch 316  \tTraining Loss: 0.674317226093748\tValidation Loss: 0.6746117897872084\n",
            "Epoch 317  \tTraining Loss: 0.6739788652957535\tValidation Loss: 0.674269998477982\n",
            "Epoch 318  \tTraining Loss: 0.6736424011467356\tValidation Loss: 0.673930108137673\n",
            "Epoch 319  \tTraining Loss: 0.6733078230152107\tValidation Loss: 0.6735921081524278\n",
            "Epoch 320  \tTraining Loss: 0.6729751203292886\tValidation Loss: 0.6732559879677704\n",
            "Epoch 321  \tTraining Loss: 0.6726442825763391\tValidation Loss: 0.6729217370882703\n",
            "Epoch 322  \tTraining Loss: 0.6723152993026597\tValidation Loss: 0.672589345077211\n",
            "Epoch 323  \tTraining Loss: 0.6719881601131443\tValidation Loss: 0.6722588015562612\n",
            "Epoch 324  \tTraining Loss: 0.6716628546709565\tValidation Loss: 0.6719300962051491\n",
            "Epoch 325  \tTraining Loss: 0.6713393726972015\tValidation Loss: 0.6716032187613354\n",
            "Epoch 326  \tTraining Loss: 0.6710177039706021\tValidation Loss: 0.6712781590196918\n",
            "Epoch 327  \tTraining Loss: 0.6706978383271756\tValidation Loss: 0.6709549068321782\n",
            "Epoch 328  \tTraining Loss: 0.6703797656599118\tValidation Loss: 0.6706334521075237\n",
            "Epoch 329  \tTraining Loss: 0.6700634759184552\tValidation Loss: 0.6703137848109084\n",
            "Epoch 330  \tTraining Loss: 0.6697489591087865\tValidation Loss: 0.6699958949636479\n",
            "Epoch 331  \tTraining Loss: 0.6694362052929063\tValidation Loss: 0.6696797726428779\n",
            "Epoch 332  \tTraining Loss: 0.6691252045885226\tValidation Loss: 0.6693654079812424\n",
            "Epoch 333  \tTraining Loss: 0.6688159471687372\tValidation Loss: 0.6690527911665832\n",
            "Epoch 334  \tTraining Loss: 0.6685084232617351\tValidation Loss: 0.6687419124416294\n",
            "Epoch 335  \tTraining Loss: 0.6682026231504772\tValidation Loss: 0.6684327621036914\n",
            "Epoch 336  \tTraining Loss: 0.6678985371723917\tValidation Loss: 0.6681253305043551\n",
            "Epoch 337  \tTraining Loss: 0.6675961557190697\tValidation Loss: 0.6678196080491772\n",
            "Epoch 338  \tTraining Loss: 0.6672954692359614\tValidation Loss: 0.6675155851973836\n",
            "Epoch 339  \tTraining Loss: 0.6669964682220735\tValidation Loss: 0.667213252461569\n",
            "Epoch 340  \tTraining Loss: 0.6666991432296705\tValidation Loss: 0.6669126004073979\n",
            "Epoch 341  \tTraining Loss: 0.6664034848639747\tValidation Loss: 0.6666136196533066\n",
            "Epoch 342  \tTraining Loss: 0.6661094837828702\tValidation Loss: 0.6663163008702092\n",
            "Epoch 343  \tTraining Loss: 0.6658171306966069\tValidation Loss: 0.6660206347812029\n",
            "Epoch 344  \tTraining Loss: 0.6655264163675081\tValidation Loss: 0.6657266121612753\n",
            "Epoch 345  \tTraining Loss: 0.665237331609678\tValidation Loss: 0.6654342238370154\n",
            "Epoch 346  \tTraining Loss: 0.6649498672887111\tValidation Loss: 0.665143460686323\n",
            "Epoch 347  \tTraining Loss: 0.6646640143214041\tValidation Loss: 0.6648543136381224\n",
            "Epoch 348  \tTraining Loss: 0.6643797636754685\tValidation Loss: 0.6645667736720761\n",
            "Epoch 349  \tTraining Loss: 0.6640971063692461\tValidation Loss: 0.6642808318183014\n",
            "Epoch 350  \tTraining Loss: 0.6638160334714236\tValidation Loss: 0.663996479157087\n",
            "Epoch 351  \tTraining Loss: 0.6635365361007524\tValidation Loss: 0.6637137068186131\n",
            "Epoch 352  \tTraining Loss: 0.6632586054257654\tValidation Loss: 0.6634325059826708\n",
            "Epoch 353  \tTraining Loss: 0.6629822326645012\tValidation Loss: 0.6631528678783857\n",
            "Epoch 354  \tTraining Loss: 0.6627074090842233\tValidation Loss: 0.6628747837839403\n",
            "Epoch 355  \tTraining Loss: 0.6624341260011469\tValidation Loss: 0.6625982450263006\n",
            "Epoch 356  \tTraining Loss: 0.6621623747801625\tValidation Loss: 0.6623232429809418\n",
            "Epoch 357  \tTraining Loss: 0.6618921468345642\tValidation Loss: 0.6620497690715773\n",
            "Epoch 358  \tTraining Loss: 0.661623433625778\tValidation Loss: 0.6617778147698888\n",
            "Epoch 359  \tTraining Loss: 0.6613562266630919\tValidation Loss: 0.6615073715952572\n",
            "Epoch 360  \tTraining Loss: 0.6610905175033884\tValidation Loss: 0.6612384311144957\n",
            "Epoch 361  \tTraining Loss: 0.6608262977508764\tValidation Loss: 0.6609709849415841\n",
            "Epoch 362  \tTraining Loss: 0.6605635590568267\tValidation Loss: 0.6607050247374056\n",
            "Epoch 363  \tTraining Loss: 0.6603022931193088\tValidation Loss: 0.6604405422094823\n",
            "Epoch 364  \tTraining Loss: 0.6600424916829268\tValidation Loss: 0.6601775291117163\n",
            "Epoch 365  \tTraining Loss: 0.6597841465385601\tValidation Loss: 0.6599159772441281\n",
            "Epoch 366  \tTraining Loss: 0.6595272495231039\tValidation Loss: 0.6596558784525999\n",
            "Epoch 367  \tTraining Loss: 0.6592717925192105\tValidation Loss: 0.6593972246286174\n",
            "Epoch 368  \tTraining Loss: 0.6590177674550332\tValidation Loss: 0.6591400077090157\n",
            "Epoch 369  \tTraining Loss: 0.6587651663039714\tValidation Loss: 0.6588842196757246\n",
            "Epoch 370  \tTraining Loss: 0.6585139810844168\tValidation Loss: 0.6586298525555168\n",
            "Epoch 371  \tTraining Loss: 0.6582642038595012\tValidation Loss: 0.6583768984197556\n",
            "Epoch 372  \tTraining Loss: 0.658015826736846\tValidation Loss: 0.6581253493841467\n",
            "Epoch 373  \tTraining Loss: 0.6577688418683121\tValidation Loss: 0.657875197608489\n",
            "Epoch 374  \tTraining Loss: 0.657523241449753\tValidation Loss: 0.6576264352964281\n",
            "Epoch 375  \tTraining Loss: 0.6572790177207667\tValidation Loss: 0.6573790546952107\n",
            "Epoch 376  \tTraining Loss: 0.6570361629644523\tValidation Loss: 0.6571330480954405\n",
            "Epoch 377  \tTraining Loss: 0.6567946695071648\tValidation Loss: 0.656888407830835\n",
            "Epoch 378  \tTraining Loss: 0.6565545297182733\tValidation Loss: 0.656645126277985\n",
            "Epoch 379  \tTraining Loss: 0.6563157360099194\tValidation Loss: 0.6564031958561141\n",
            "Epoch 380  \tTraining Loss: 0.6560782808367781\tValidation Loss: 0.6561626090268398\n",
            "Epoch 381  \tTraining Loss: 0.6558421566958181\tValidation Loss: 0.6559233582939363\n",
            "Epoch 382  \tTraining Loss: 0.6556073561260669\tValidation Loss: 0.6556854362030986\n",
            "Epoch 383  \tTraining Loss: 0.655373871708373\tValidation Loss: 0.6554488353417078\n",
            "Epoch 384  \tTraining Loss: 0.6551416960651723\tValidation Loss: 0.6552135483385975\n",
            "Epoch 385  \tTraining Loss: 0.6549108218602547\tValidation Loss: 0.6549795678638219\n",
            "Epoch 386  \tTraining Loss: 0.6546812417985333\tValidation Loss: 0.6547468866284246\n",
            "Epoch 387  \tTraining Loss: 0.6544529486258123\tValidation Loss: 0.6545154973842102\n",
            "Epoch 388  \tTraining Loss: 0.654225935128559\tValidation Loss: 0.6542853929235146\n",
            "Epoch 389  \tTraining Loss: 0.6540001941336755\tValidation Loss: 0.6540565660789792\n",
            "Epoch 390  \tTraining Loss: 0.653775718508272\tValidation Loss: 0.6538290097233248\n",
            "Epoch 391  \tTraining Loss: 0.6535525011594411\tValidation Loss: 0.6536027167691272\n",
            "Epoch 392  \tTraining Loss: 0.6533305350340342\tValidation Loss: 0.6533776801685944\n",
            "Epoch 393  \tTraining Loss: 0.6531098131184386\tValidation Loss: 0.6531538929133442\n",
            "Epoch 394  \tTraining Loss: 0.6528903284383556\tValidation Loss: 0.6529313480341841\n",
            "Epoch 395  \tTraining Loss: 0.6526720740585804\tValidation Loss: 0.652710038600891\n",
            "Epoch 396  \tTraining Loss: 0.6524550430827827\tValidation Loss: 0.6524899577219946\n",
            "Epoch 397  \tTraining Loss: 0.6522392286532889\tValidation Loss: 0.6522710985445583\n",
            "Epoch 398  \tTraining Loss: 0.6520246239508652\tValidation Loss: 0.652053454253966\n",
            "Epoch 399  \tTraining Loss: 0.6518112221945029\tValidation Loss: 0.6518370180737053\n",
            "Epoch 400  \tTraining Loss: 0.6515990166412032\tValidation Loss: 0.6516217832651551\n",
            "lr, batch_size: (0.001, 500)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.6243534526955732\n",
            "Epoch 2  \tTraining Loss: 0.6211419364869208\tValidation Loss: 0.8706964125768494\n",
            "Epoch 3  \tTraining Loss: 0.8652956414574802\tValidation Loss: 0.6080005424691387\n",
            "Epoch 4  \tTraining Loss: 0.6057258164438752\tValidation Loss: 0.6340736540992257\n",
            "Epoch 5  \tTraining Loss: 0.6252237430173914\tValidation Loss: 0.8651971395052276\n",
            "Epoch 6  \tTraining Loss: 0.8631873101975883\tValidation Loss: 0.5740676554933143\n",
            "Epoch 7  \tTraining Loss: 0.5681604855792008\tValidation Loss: 0.5709372474182237\n",
            "Epoch 8  \tTraining Loss: 0.5578663203610648\tValidation Loss: 0.6604647027530745\n",
            "Epoch 9  \tTraining Loss: 0.6453815262912882\tValidation Loss: 8.510887002784191\n",
            "Epoch 10  \tTraining Loss: 8.554905179409724\tValidation Loss: 0.9342282716618441\n",
            "Epoch 11  \tTraining Loss: 0.9311494197660802\tValidation Loss: 0.9321786454884309\n",
            "Epoch 12  \tTraining Loss: 0.9291057032192446\tValidation Loss: 0.9282034960205785\n",
            "Epoch 13  \tTraining Loss: 0.9249627130767325\tValidation Loss: 0.9063776912725549\n",
            "Epoch 14  \tTraining Loss: 0.9025341770683879\tValidation Loss: 0.7597925493872184\n",
            "Epoch 15  \tTraining Loss: 0.7561155980652636\tValidation Loss: 0.8569698807397439\n",
            "Epoch 16  \tTraining Loss: 0.8620547787858478\tValidation Loss: 0.9301583208540838\n",
            "Epoch 17  \tTraining Loss: 0.9270153094757775\tValidation Loss: 0.9281158515036863\n",
            "Epoch 18  \tTraining Loss: 0.9250051223871008\tValidation Loss: 0.9245802837732365\n",
            "Epoch 19  \tTraining Loss: 0.9214338045315198\tValidation Loss: 0.9101549180178962\n",
            "Epoch 20  \tTraining Loss: 0.9068119994847061\tValidation Loss: 0.765848939127548\n",
            "Epoch 21  \tTraining Loss: 0.7638876288693668\tValidation Loss: 1.2008148593786083\n",
            "Epoch 22  \tTraining Loss: 1.20506386086261\tValidation Loss: 0.9238496145517081\n",
            "Epoch 23  \tTraining Loss: 0.9208215042376782\tValidation Loss: 0.9073228248763365\n",
            "Epoch 24  \tTraining Loss: 0.9041775590340153\tValidation Loss: 0.7271733403852984\n",
            "Epoch 25  \tTraining Loss: 0.7237622127170513\tValidation Loss: 1.3020207933124877\n",
            "Epoch 26  \tTraining Loss: 1.301939602883523\tValidation Loss: 0.923379957497202\n",
            "Epoch 27  \tTraining Loss: 0.9203567467873853\tValidation Loss: 0.9216504099658089\n",
            "Epoch 28  \tTraining Loss: 0.9186392350714089\tValidation Loss: 0.9199305235425486\n",
            "Epoch 29  \tTraining Loss: 0.9169313507025068\tValidation Loss: 0.9182202441866457\n",
            "Epoch 30  \tTraining Loss: 0.9152330397155088\tValidation Loss: 0.9165195181408842\n",
            "Epoch 31  \tTraining Loss: 0.9135442484477425\tValidation Loss: 0.9148282919491134\n",
            "Epoch 32  \tTraining Loss: 0.9118649235373344\tValidation Loss: 0.9131465124545632\n",
            "Epoch 33  \tTraining Loss: 0.9101950119215273\tValidation Loss: 0.9114741267981648\n",
            "Epoch 34  \tTraining Loss: 0.9085344608350017\tValidation Loss: 0.9098110824168839\n",
            "Epoch 35  \tTraining Loss: 0.9068832178082096\tValidation Loss: 0.9081573270420613\n",
            "Epoch 36  \tTraining Loss: 0.905241230665715\tValidation Loss: 0.9065128086977641\n",
            "Epoch 37  \tTraining Loss: 0.9036084475245472\tValidation Loss: 0.904877475699145\n",
            "Epoch 38  \tTraining Loss: 0.9019848167925602\tValidation Loss: 0.9032512766508122\n",
            "Epoch 39  \tTraining Loss: 0.9003702871668022\tValidation Loss: 0.9016341604452063\n",
            "Epoch 40  \tTraining Loss: 0.8987648076318966\tValidation Loss: 0.9000260762609895\n",
            "Epoch 41  \tTraining Loss: 0.8971683274584276\tValidation Loss: 0.89842697356144\n",
            "Epoch 42  \tTraining Loss: 0.895580796201338\tValidation Loss: 0.8968368020928583\n",
            "Epoch 43  \tTraining Loss: 0.8940021636983363\tValidation Loss: 0.8952555118829819\n",
            "Epoch 44  \tTraining Loss: 0.8924323800683104\tValidation Loss: 0.8936830532394076\n",
            "Epoch 45  \tTraining Loss: 0.8908713957097525\tValidation Loss: 0.8921193767480233\n",
            "Epoch 46  \tTraining Loss: 0.8893191612991906\tValidation Loss: 0.8905644332714502\n",
            "Epoch 47  \tTraining Loss: 0.8877756277896311\tValidation Loss: 0.8890181739474907\n",
            "Epoch 48  \tTraining Loss: 0.8862407464090087\tValidation Loss: 0.8874805501875878\n",
            "Epoch 49  \tTraining Loss: 0.884714468658645\tValidation Loss: 0.8859515136752909\n",
            "Epoch 50  \tTraining Loss: 0.8831967463117163\tValidation Loss: 0.884431016364732\n",
            "Epoch 51  \tTraining Loss: 0.8816875314117302\tValidation Loss: 0.8829190104791098\n",
            "Epoch 52  \tTraining Loss: 0.8801867762710086\tValidation Loss: 0.8814154485091813\n",
            "Epoch 53  \tTraining Loss: 0.8786944334691841\tValidation Loss: 0.8799202832117639\n",
            "Epoch 54  \tTraining Loss: 0.8772104558516973\tValidation Loss: 0.8784334676082435\n",
            "Epoch 55  \tTraining Loss: 0.8757347965283108\tValidation Loss: 0.8769549549830928\n",
            "Epoch 56  \tTraining Loss: 0.8742674088716254\tValidation Loss: 0.8754846988823977\n",
            "Epoch 57  \tTraining Loss: 0.8728082465156076\tValidation Loss: 0.8740226531123906\n",
            "Epoch 58  \tTraining Loss: 0.8713572633541237\tValidation Loss: 0.872568771737994\n",
            "Epoch 59  \tTraining Loss: 0.8699144135394834\tValidation Loss: 0.8711230090813695\n",
            "Epoch 60  \tTraining Loss: 0.8684796514809922\tValidation Loss: 0.8696853197204786\n",
            "Epoch 61  \tTraining Loss: 0.8670529318435095\tValidation Loss: 0.868255658487648\n",
            "Epoch 62  \tTraining Loss: 0.8656342095460153\tValidation Loss: 0.8668339804681445\n",
            "Epoch 63  \tTraining Loss: 0.864223439760189\tValidation Loss: 0.8654202409987594\n",
            "Epoch 64  \tTraining Loss: 0.8628205779089888\tValidation Loss: 0.8640143956663967\n",
            "Epoch 65  \tTraining Loss: 0.8614255796652468\tValidation Loss: 0.8626164003066747\n",
            "Epoch 66  \tTraining Loss: 0.860038400950266\tValidation Loss: 0.8612262110025302\n",
            "Epoch 67  \tTraining Loss: 0.8586589979324282\tValidation Loss: 0.8598437840828338\n",
            "Epoch 68  \tTraining Loss: 0.8572873270258098\tValidation Loss: 0.8584690761210124\n",
            "Epoch 69  \tTraining Loss: 0.8559233448888031\tValidation Loss: 0.8571020439336782\n",
            "Epoch 70  \tTraining Loss: 0.8545670084227482\tValidation Loss: 0.8557426445792667\n",
            "Epoch 71  \tTraining Loss: 0.8532182747705707\tValidation Loss: 0.8543908353566817\n",
            "Epoch 72  \tTraining Loss: 0.8518771013154265\tValidation Loss: 0.8530465738039484\n",
            "Epoch 73  \tTraining Loss: 0.8505434456793572\tValidation Loss: 0.8517098176968728\n",
            "Epoch 74  \tTraining Loss: 0.8492172657219497\tValidation Loss: 0.8503805250477108\n",
            "Epoch 75  \tTraining Loss: 0.8478985195390043\tValidation Loss: 0.8490586541038424\n",
            "Epoch 76  \tTraining Loss: 0.8465871654612124\tValidation Loss: 0.8477441633464543\n",
            "Epoch 77  \tTraining Loss: 0.8452831620528375\tValidation Loss: 0.846437011489231\n",
            "Epoch 78  \tTraining Loss: 0.843986468110408\tValidation Loss: 0.8451371574770516\n",
            "Epoch 79  \tTraining Loss: 0.8426970426614138\tValidation Loss: 0.8438445604846941\n",
            "Epoch 80  \tTraining Loss: 0.841414844963013\tValidation Loss: 0.8425591799155485\n",
            "Epoch 81  \tTraining Loss: 0.840139834500743\tValidation Loss: 0.8412809754003345\n",
            "Epoch 82  \tTraining Loss: 0.8388719709872421\tValidation Loss: 0.8400099067958291\n",
            "Epoch 83  \tTraining Loss: 0.8376112143609743\tValidation Loss: 0.8387459341835997\n",
            "Epoch 84  \tTraining Loss: 0.8363575247849656\tValidation Loss: 0.8374890178687442\n",
            "Epoch 85  \tTraining Loss: 0.8351108626455442\tValidation Loss: 0.83623911837864\n",
            "Epoch 86  \tTraining Loss: 0.8338711885510885\tValidation Loss: 0.8349961964616968\n",
            "Epoch 87  \tTraining Loss: 0.8326384633307837\tValidation Loss: 0.8337602130861199\n",
            "Epoch 88  \tTraining Loss: 0.8314126480333824\tValidation Loss: 0.8325311294386779\n",
            "Epoch 89  \tTraining Loss: 0.8301937039259745\tValidation Loss: 0.8313089069234786\n",
            "Epoch 90  \tTraining Loss: 0.8289815924927648\tValidation Loss: 0.8300935071607509\n",
            "Epoch 91  \tTraining Loss: 0.8277762754338537\tValidation Loss: 0.8288848919856346\n",
            "Epoch 92  \tTraining Loss: 0.8265777146640277\tValidation Loss: 0.8276830234469758\n",
            "Epoch 93  \tTraining Loss: 0.8253858723115574\tValidation Loss: 0.8264878638061303\n",
            "Epoch 94  \tTraining Loss: 0.8242007107169989\tValidation Loss: 0.8252993755357718\n",
            "Epoch 95  \tTraining Loss: 0.823022192432005\tValidation Loss: 0.8241175213187097\n",
            "Epoch 96  \tTraining Loss: 0.8218502802181415\tValidation Loss: 0.8229422640467103\n",
            "Epoch 97  \tTraining Loss: 0.8206849370457109\tValidation Loss: 0.8217735668193266\n",
            "Epoch 98  \tTraining Loss: 0.819526126092582\tValidation Loss: 0.8206113929427351\n",
            "Epoch 99  \tTraining Loss: 0.818373810743026\tValidation Loss: 0.8194557059285766\n",
            "Epoch 100  \tTraining Loss: 0.8172279545865611\tValidation Loss: 0.8183064694928064\n",
            "Epoch 101  \tTraining Loss: 0.8160885214168001\tValidation Loss: 0.8171636475545492\n",
            "Epoch 102  \tTraining Loss: 0.8149554752303074\tValidation Loss: 0.8160272042349606\n",
            "Epoch 103  \tTraining Loss: 0.8138287802254619\tValidation Loss: 0.8148971038560955\n",
            "Epoch 104  \tTraining Loss: 0.8127084008013236\tValidation Loss: 0.8137733109397829\n",
            "Epoch 105  \tTraining Loss: 0.8115943015565118\tValidation Loss: 0.812655790206506\n",
            "Epoch 106  \tTraining Loss: 0.8104864472880837\tValidation Loss: 0.8115445065742901\n",
            "Epoch 107  \tTraining Loss: 0.8093848029904237\tValidation Loss: 0.8104394251575948\n",
            "Epoch 108  \tTraining Loss: 0.8082893338541367\tValidation Loss: 0.8093405112662146\n",
            "Epoch 109  \tTraining Loss: 0.8072000052649481\tValidation Loss: 0.8082477304041839\n",
            "Epoch 110  \tTraining Loss: 0.8061167828026107\tValidation Loss: 0.8071610482686888\n",
            "Epoch 111  \tTraining Loss: 0.8050396322398167\tValidation Loss: 0.806080430748986\n",
            "Epoch 112  \tTraining Loss: 0.8039685195411157\tValidation Loss: 0.8050058439253254\n",
            "Epoch 113  \tTraining Loss: 0.8029034108618404\tValidation Loss: 0.8039372540678803\n",
            "Epoch 114  \tTraining Loss: 0.8018442725470356\tValidation Loss: 0.8028746276356844\n",
            "Epoch 115  \tTraining Loss: 0.8007910711303967\tValidation Loss: 0.801817931275573\n",
            "Epoch 116  \tTraining Loss: 0.7997437733332109\tValidation Loss: 0.8007671318211305\n",
            "Epoch 117  \tTraining Loss: 0.7987023460633059\tValidation Loss: 0.7997221962916454\n",
            "Epoch 118  \tTraining Loss: 0.7976667564140045\tValidation Loss: 0.7986830918910687\n",
            "Epoch 119  \tTraining Loss: 0.7966369716630846\tValidation Loss: 0.79764978600698\n",
            "Epoch 120  \tTraining Loss: 0.7956129592717458\tValidation Loss: 0.7966222462095588\n",
            "Epoch 121  \tTraining Loss: 0.7945946868835801\tValidation Loss: 0.7956004402505611\n",
            "Epoch 122  \tTraining Loss: 0.793582122323551\tValidation Loss: 0.7945843360623027\n",
            "Epoch 123  \tTraining Loss: 0.7925752335969755\tValidation Loss: 0.7935739017566471\n",
            "Epoch 124  \tTraining Loss: 0.7915739888885143\tValidation Loss: 0.7925691056240004\n",
            "Epoch 125  \tTraining Loss: 0.7905783565611648\tValidation Loss: 0.7915699161323105\n",
            "Epoch 126  \tTraining Loss: 0.7895883051552642\tValidation Loss: 0.7905763019260721\n",
            "Epoch 127  \tTraining Loss: 0.7886038033874927\tValidation Loss: 0.7895882318253388\n",
            "Epoch 128  \tTraining Loss: 0.7876248201498862\tValidation Loss: 0.7886056748247384\n",
            "Epoch 129  \tTraining Loss: 0.7866513245088539\tValidation Loss: 0.7876286000924959\n",
            "Epoch 130  \tTraining Loss: 0.7856832857041998\tValidation Loss: 0.7866569769694604\n",
            "Epoch 131  \tTraining Loss: 0.784720673148151\tValidation Loss: 0.7856907749681376\n",
            "Epoch 132  \tTraining Loss: 0.7837634564243922\tValidation Loss: 0.7847299637717298\n",
            "Epoch 133  \tTraining Loss: 0.782811605287103\tValidation Loss: 0.7837745132331773\n",
            "Epoch 134  \tTraining Loss: 0.7818650896600042\tValidation Loss: 0.78282439337421\n",
            "Epoch 135  \tTraining Loss: 0.7809238796354045\tValidation Loss: 0.7818795743843996\n",
            "Epoch 136  \tTraining Loss: 0.7799879454732592\tValidation Loss: 0.7809400266202207\n",
            "Epoch 137  \tTraining Loss: 0.7790572576002271\tValidation Loss: 0.7800057206041152\n",
            "Epoch 138  \tTraining Loss: 0.7781317866087385\tValidation Loss: 0.7790766270235635\n",
            "Epoch 139  \tTraining Loss: 0.7772115032560645\tValidation Loss: 0.7781527167301581\n",
            "Epoch 140  \tTraining Loss: 0.7762963784633937\tValidation Loss: 0.7772339607386859\n",
            "Epoch 141  \tTraining Loss: 0.775386383314913\tValidation Loss: 0.7763203302262134\n",
            "Epoch 142  \tTraining Loss: 0.7744814890568942\tValidation Loss: 0.7754117965311776\n",
            "Epoch 143  \tTraining Loss: 0.7735816670967858\tValidation Loss: 0.7745083311524816\n",
            "Epoch 144  \tTraining Loss: 0.7726868890023083\tValidation Loss: 0.773609905748597\n",
            "Epoch 145  \tTraining Loss: 0.7717971265005573\tValidation Loss: 0.7727164921366677\n",
            "Epoch 146  \tTraining Loss: 0.7709123514771089\tValidation Loss: 0.7718280622916236\n",
            "Epoch 147  \tTraining Loss: 0.7700325359751322\tValidation Loss: 0.7709445883452947\n",
            "Epoch 148  \tTraining Loss: 0.7691576521945059\tValidation Loss: 0.7700660425855332\n",
            "Epoch 149  \tTraining Loss: 0.7682876724909391\tValidation Loss: 0.769192397455339\n",
            "Epoch 150  \tTraining Loss: 0.7674225693750985\tValidation Loss: 0.7683236255519912\n",
            "Epoch 151  \tTraining Loss: 0.7665623155117401\tValidation Loss: 0.7674596996261828\n",
            "Epoch 152  \tTraining Loss: 0.7657068837188438\tValidation Loss: 0.7666005925811621\n",
            "Epoch 153  \tTraining Loss: 0.7648562469667574\tValidation Loss: 0.7657462774718778\n",
            "Epoch 154  \tTraining Loss: 0.7640103783773389\tValidation Loss: 0.7648967275041296\n",
            "Epoch 155  \tTraining Loss: 0.7631692512231105\tValidation Loss: 0.7640519160337217\n",
            "Epoch 156  \tTraining Loss: 0.7623328389264126\tValidation Loss: 0.763211816565625\n",
            "Epoch 157  \tTraining Loss: 0.7615011150585639\tValidation Loss: 0.762376402753139\n",
            "Epoch 158  \tTraining Loss: 0.7606740533390258\tValidation Loss: 0.7615456483970623\n",
            "Epoch 159  \tTraining Loss: 0.759851627634574\tValidation Loss: 0.7607195274448659\n",
            "Epoch 160  \tTraining Loss: 0.7590338119584707\tValidation Loss: 0.759898013989872\n",
            "Epoch 161  \tTraining Loss: 0.7582205804696448\tValidation Loss: 0.7590810822704361\n",
            "Epoch 162  \tTraining Loss: 0.757411907471874\tValidation Loss: 0.7582687066691349\n",
            "Epoch 163  \tTraining Loss: 0.7566077674129748\tValidation Loss: 0.7574608617119589\n",
            "Epoch 164  \tTraining Loss: 0.7558081348839929\tValidation Loss: 0.7566575220675086\n",
            "Epoch 165  \tTraining Loss: 0.7550129846184022\tValidation Loss: 0.7558586625461955\n",
            "Epoch 166  \tTraining Loss: 0.7542222914913054\tValidation Loss: 0.755064258099448\n",
            "Epoch 167  \tTraining Loss: 0.7534360305186407\tValidation Loss: 0.7542742838189213\n",
            "Epoch 168  \tTraining Loss: 0.7526541768563914\tValidation Loss: 0.7534887149357117\n",
            "Epoch 169  \tTraining Loss: 0.7518767057998027\tValidation Loss: 0.7527075268195758\n",
            "Epoch 170  \tTraining Loss: 0.7511035927825985\tValidation Loss: 0.751930694978154\n",
            "Epoch 171  \tTraining Loss: 0.7503348133762077\tValidation Loss: 0.751158195056197\n",
            "Epoch 172  \tTraining Loss: 0.7495703432889907\tValidation Loss: 0.7503900028347992\n",
            "Epoch 173  \tTraining Loss: 0.7488101583654727\tValidation Loss: 0.7496260942306338\n",
            "Epoch 174  \tTraining Loss: 0.7480542345855795\tValidation Loss: 0.7488664452951935\n",
            "Epoch 175  \tTraining Loss: 0.7473025480638799\tValidation Loss: 0.7481110322140365\n",
            "Epoch 176  \tTraining Loss: 0.7465550750488295\tValidation Loss: 0.7473598313060339\n",
            "Epoch 177  \tTraining Loss: 0.7458117919220212\tValidation Loss: 0.746612819022623\n",
            "Epoch 178  \tTraining Loss: 0.7450726751974388\tValidation Loss: 0.7458699719470663\n",
            "Epoch 179  \tTraining Loss: 0.7443377015207148\tValidation Loss: 0.7451312667937113\n",
            "Epoch 180  \tTraining Loss: 0.7436068476683912\tValidation Loss: 0.7443966804072575\n",
            "Epoch 181  \tTraining Loss: 0.7428800905471885\tValidation Loss: 0.743666189762025\n",
            "Epoch 182  \tTraining Loss: 0.7421574071932736\tValidation Loss: 0.7429397719612294\n",
            "Epoch 183  \tTraining Loss: 0.7414387747715345\tValidation Loss: 0.7422174042362593\n",
            "Epoch 184  \tTraining Loss: 0.7407241705748601\tValidation Loss: 0.7414990639459584\n",
            "Epoch 185  \tTraining Loss: 0.7400135720234212\tValidation Loss: 0.7407847285759122\n",
            "Epoch 186  \tTraining Loss: 0.739306956663958\tValidation Loss: 0.7400743757377364\n",
            "Epoch 187  \tTraining Loss: 0.7386043021690701\tValidation Loss: 0.7393679831683727\n",
            "Epoch 188  \tTraining Loss: 0.7379055863365116\tValidation Loss: 0.738665528729386\n",
            "Epoch 189  \tTraining Loss: 0.7372107870884886\tValidation Loss: 0.7379669904062657\n",
            "Epoch 190  \tTraining Loss: 0.7365198824709625\tValidation Loss: 0.7372723463077331\n",
            "Epoch 191  \tTraining Loss: 0.7358328506529558\tValidation Loss: 0.7365815746650495\n",
            "Epoch 192  \tTraining Loss: 0.7351496699258625\tValidation Loss: 0.7358946538313306\n",
            "Epoch 193  \tTraining Loss: 0.734470318702762\tValidation Loss: 0.7352115622808643\n",
            "Epoch 194  \tTraining Loss: 0.7337947755177374\tValidation Loss: 0.7345322786084308\n",
            "Epoch 195  \tTraining Loss: 0.7331230190251969\tValidation Loss: 0.7338567815286289\n",
            "Epoch 196  \tTraining Loss: 0.732455027999199\tValidation Loss: 0.7331850498752037\n",
            "Epoch 197  \tTraining Loss: 0.7317907813327826\tValidation Loss: 0.7325170626003801\n",
            "Epoch 198  \tTraining Loss: 0.7311302580372991\tValidation Loss: 0.7318527987741987\n",
            "Epoch 199  \tTraining Loss: 0.7304734372417505\tValidation Loss: 0.7311922375838563\n",
            "Epoch 200  \tTraining Loss: 0.7298202981921288\tValidation Loss: 0.7305353583330486\n",
            "Epoch 201  \tTraining Loss: 0.7291708202507602\tValidation Loss: 0.7298821404413189\n",
            "Epoch 202  \tTraining Loss: 0.7285249828956543\tValidation Loss: 0.7292325634434087\n",
            "Epoch 203  \tTraining Loss: 0.7278827657198533\tValidation Loss: 0.7285866069886118\n",
            "Epoch 204  \tTraining Loss: 0.7272441484307909\tValidation Loss: 0.7279442508401339\n",
            "Epoch 205  \tTraining Loss: 0.7266091108496463\tValidation Loss: 0.7273054748744536\n",
            "Epoch 206  \tTraining Loss: 0.7259776329107114\tValidation Loss: 0.7266702590806877\n",
            "Epoch 207  \tTraining Loss: 0.7253496946607525\tValidation Loss: 0.7260385835599616\n",
            "Epoch 208  \tTraining Loss: 0.7247252762583835\tValidation Loss: 0.7254104285247804\n",
            "Epoch 209  \tTraining Loss: 0.7241043579734362\tValidation Loss: 0.7247857742984057\n",
            "Epoch 210  \tTraining Loss: 0.7234869201863384\tValidation Loss: 0.7241646013142357\n",
            "Epoch 211  \tTraining Loss: 0.7228729433874937\tValidation Loss: 0.7235468901151866\n",
            "Epoch 212  \tTraining Loss: 0.7222624081766642\tValidation Loss: 0.7229326213530821\n",
            "Epoch 213  \tTraining Loss: 0.7216552952623588\tValidation Loss: 0.7223217757880399\n",
            "Epoch 214  \tTraining Loss: 0.7210515854612232\tValidation Loss: 0.7217143342878675\n",
            "Epoch 215  \tTraining Loss: 0.7204512596974327\tValidation Loss: 0.7211102778274588\n",
            "Epoch 216  \tTraining Loss: 0.719854299002091\tValidation Loss: 0.7205095874881928\n",
            "Epoch 217  \tTraining Loss: 0.7192606845126297\tValidation Loss: 0.7199122444573389\n",
            "Epoch 218  \tTraining Loss: 0.7186703974722128\tValidation Loss: 0.7193182300274626\n",
            "Epoch 219  \tTraining Loss: 0.7180834192291445\tValidation Loss: 0.7187275255958364\n",
            "Epoch 220  \tTraining Loss: 0.7174997312362785\tValidation Loss: 0.7181401126638527\n",
            "Epoch 221  \tTraining Loss: 0.7169193150504335\tValidation Loss: 0.7175559728364416\n",
            "Epoch 222  \tTraining Loss: 0.7163421523318086\tValidation Loss: 0.7169750878214894\n",
            "Epoch 223  \tTraining Loss: 0.715768224843406\tValidation Loss: 0.7163974394292636\n",
            "Epoch 224  \tTraining Loss: 0.7151975144504533\tValidation Loss: 0.7158230095718382\n",
            "Epoch 225  \tTraining Loss: 0.7146300031198307\tValidation Loss: 0.7152517802625243\n",
            "Epoch 226  \tTraining Loss: 0.7140656729195011\tValidation Loss: 0.7146837336153026\n",
            "Epoch 227  \tTraining Loss: 0.7135045060179442\tValidation Loss: 0.7141188518442593\n",
            "Epoch 228  \tTraining Loss: 0.7129464846835921\tValidation Loss: 0.7135571172630262\n",
            "Epoch 229  \tTraining Loss: 0.7123915912842701\tValidation Loss: 0.7129985122842224\n",
            "Epoch 230  \tTraining Loss: 0.7118398082866383\tValidation Loss: 0.7124430194189001\n",
            "Epoch 231  \tTraining Loss: 0.711291118255639\tValidation Loss: 0.7118906212759937\n",
            "Epoch 232  \tTraining Loss: 0.7107455038539441\tValidation Loss: 0.7113413005617703\n",
            "Epoch 233  \tTraining Loss: 0.710202947841409\tValidation Loss: 0.7107950400792865\n",
            "Epoch 234  \tTraining Loss: 0.7096634330745268\tValidation Loss: 0.7102518227278444\n",
            "Epoch 235  \tTraining Loss: 0.7091269425058868\tValidation Loss: 0.7097116315024531\n",
            "Epoch 236  \tTraining Loss: 0.7085934591836361\tValidation Loss: 0.7091744494932937\n",
            "Epoch 237  \tTraining Loss: 0.7080629662509438\tValidation Loss: 0.7086402598851844\n",
            "Epoch 238  \tTraining Loss: 0.7075354469454681\tValidation Loss: 0.708109045957052\n",
            "Epoch 239  \tTraining Loss: 0.707010884598827\tValidation Loss: 0.7075807910814044\n",
            "Epoch 240  \tTraining Loss: 0.7064892626360717\tValidation Loss: 0.7070554787238054\n",
            "Epoch 241  \tTraining Loss: 0.7059705645751627\tValidation Loss: 0.7065330924423551\n",
            "Epoch 242  \tTraining Loss: 0.7054547740264484\tValidation Loss: 0.7060136158871706\n",
            "Epoch 243  \tTraining Loss: 0.7049418746921488\tValidation Loss: 0.7054970327998707\n",
            "Epoch 244  \tTraining Loss: 0.7044318503658383\tValidation Loss: 0.704983327013064\n",
            "Epoch 245  \tTraining Loss: 0.7039246849319365\tValidation Loss: 0.7044724824498386\n",
            "Epoch 246  \tTraining Loss: 0.7034203623651955\tValidation Loss: 0.7039644831232557\n",
            "Epoch 247  \tTraining Loss: 0.7029188667301968\tValidation Loss: 0.7034593131358451\n",
            "Epoch 248  \tTraining Loss: 0.7024201821808459\tValidation Loss: 0.7029569566791054\n",
            "Epoch 249  \tTraining Loss: 0.7019242929598715\tValidation Loss: 0.702457398033004\n",
            "Epoch 250  \tTraining Loss: 0.7014311833983293\tValidation Loss: 0.701960621565483\n",
            "Epoch 251  \tTraining Loss: 0.7009408379151045\tValidation Loss: 0.7014666117319659\n",
            "Epoch 252  \tTraining Loss: 0.7004532410164221\tValidation Loss: 0.7009753530748672\n",
            "Epoch 253  \tTraining Loss: 0.6999683772953552\tValidation Loss: 0.7004868302231062\n",
            "Epoch 254  \tTraining Loss: 0.6994862314313384\tValidation Loss: 0.7000010278916217\n",
            "Epoch 255  \tTraining Loss: 0.6990067881896852\tValidation Loss: 0.6995179308808901\n",
            "Epoch 256  \tTraining Loss: 0.6985300324211055\tValidation Loss: 0.6990375240764474\n",
            "Epoch 257  \tTraining Loss: 0.6980559490612267\tValidation Loss: 0.6985597924484115\n",
            "Epoch 258  \tTraining Loss: 0.6975845231301184\tValidation Loss: 0.6980847210510093\n",
            "Epoch 259  \tTraining Loss: 0.6971157397318187\tValidation Loss: 0.6976122950221054\n",
            "Epoch 260  \tTraining Loss: 0.6966495840538637\tValidation Loss: 0.6971424995827332\n",
            "Epoch 261  \tTraining Loss: 0.696186041366819\tValidation Loss: 0.6966753200366306\n",
            "Epoch 262  \tTraining Loss: 0.6957250970238149\tValidation Loss: 0.6962107417697746\n",
            "Epoch 263  \tTraining Loss: 0.6952667364600835\tValidation Loss: 0.6957487502499226\n",
            "Epoch 264  \tTraining Loss: 0.6948109451924979\tValidation Loss: 0.6952893310261533\n",
            "Epoch 265  \tTraining Loss: 0.6943577088191151\tValidation Loss: 0.6948324697284115\n",
            "Epoch 266  \tTraining Loss: 0.6939070130187208\tValidation Loss: 0.6943781520670557\n",
            "Epoch 267  \tTraining Loss: 0.6934588435503775\tValidation Loss: 0.6939263638324069\n",
            "Epoch 268  \tTraining Loss: 0.6930131862529728\tValidation Loss: 0.6934770908943011\n",
            "Epoch 269  \tTraining Loss: 0.6925700270447738\tValidation Loss: 0.6930303192016444\n",
            "Epoch 270  \tTraining Loss: 0.6921293519229811\tValidation Loss: 0.6925860347819697\n",
            "Epoch 271  \tTraining Loss: 0.691691146963287\tValidation Loss: 0.6921442237409962\n",
            "Epoch 272  \tTraining Loss: 0.6912553983194348\tValidation Loss: 0.6917048722621918\n",
            "Epoch 273  \tTraining Loss: 0.690822092222782\tValidation Loss: 0.6912679666063379\n",
            "Epoch 274  \tTraining Loss: 0.6903912149818645\tValidation Loss: 0.6908334931110958\n",
            "Epoch 275  \tTraining Loss: 0.6899627529819647\tValidation Loss: 0.6904014381905768\n",
            "Epoch 276  \tTraining Loss: 0.6895366926846808\tValidation Loss: 0.689971788334913\n",
            "Epoch 277  \tTraining Loss: 0.6891130206274995\tValidation Loss: 0.6895445301098332\n",
            "Epoch 278  \tTraining Loss: 0.6886917234233699\tValidation Loss: 0.6891196501562378\n",
            "Epoch 279  \tTraining Loss: 0.6882727877602813\tValidation Loss: 0.6886971351897789\n",
            "Epoch 280  \tTraining Loss: 0.687856200400842\tValidation Loss: 0.6882769720004412\n",
            "Epoch 281  \tTraining Loss: 0.6874419481818614\tValidation Loss: 0.687859147452126\n",
            "Epoch 282  \tTraining Loss: 0.6870300180139336\tValidation Loss: 0.6874436484822367\n",
            "Epoch 283  \tTraining Loss: 0.6866203968810245\tValidation Loss: 0.6870304621012676\n",
            "Epoch 284  \tTraining Loss: 0.6862130718400598\tValidation Loss: 0.686619575392395\n",
            "Epoch 285  \tTraining Loss: 0.6858080300205166\tValidation Loss: 0.6862109755110689\n",
            "Epoch 286  \tTraining Loss: 0.6854052586240162\tValidation Loss: 0.6858046496846097\n",
            "Epoch 287  \tTraining Loss: 0.6850047449239203\tValidation Loss: 0.6854005852118042\n",
            "Epoch 288  \tTraining Loss: 0.6846064762649281\tValidation Loss: 0.6849987694625068\n",
            "Epoch 289  \tTraining Loss: 0.6842104400626776\tValidation Loss: 0.6845991898772401\n",
            "Epoch 290  \tTraining Loss: 0.6838166238033466\tValidation Loss: 0.6842018339668008\n",
            "Epoch 291  \tTraining Loss: 0.6834250150432583\tValidation Loss: 0.6838066893118644\n",
            "Epoch 292  \tTraining Loss: 0.6830356014084872\tValidation Loss: 0.6834137435625952\n",
            "Epoch 293  \tTraining Loss: 0.6826483705944696\tValidation Loss: 0.6830229844382563\n",
            "Epoch 294  \tTraining Loss: 0.6822633103656127\tValidation Loss: 0.682634399726823\n",
            "Epoch 295  \tTraining Loss: 0.68188040855491\tValidation Loss: 0.6822479772845982\n",
            "Epoch 296  \tTraining Loss: 0.6814996530635556\tValidation Loss: 0.6818637050358289\n",
            "Epoch 297  \tTraining Loss: 0.6811210318605625\tValidation Loss: 0.6814815709723271\n",
            "Epoch 298  \tTraining Loss: 0.6807445329823821\tValidation Loss: 0.6811015631530898\n",
            "Epoch 299  \tTraining Loss: 0.6803701445325265\tValidation Loss: 0.6807236697039241\n",
            "Epoch 300  \tTraining Loss: 0.679997854681192\tValidation Loss: 0.6803478788170725\n",
            "Epoch 301  \tTraining Loss: 0.6796276516648864\tValidation Loss: 0.6799741787508403\n",
            "Epoch 302  \tTraining Loss: 0.6792595237860559\tValidation Loss: 0.679602557829227\n",
            "Epoch 303  \tTraining Loss: 0.6788934594127166\tValidation Loss: 0.6792330044415569\n",
            "Epoch 304  \tTraining Loss: 0.6785294469780867\tValidation Loss: 0.6788655070421146\n",
            "Epoch 305  \tTraining Loss: 0.6781674749802206\tValidation Loss: 0.6785000541497801\n",
            "Epoch 306  \tTraining Loss: 0.6778075319816459\tValidation Loss: 0.6781366343476678\n",
            "Epoch 307  \tTraining Loss: 0.6774496066090019\tValidation Loss: 0.6777752362827666\n",
            "Epoch 308  \tTraining Loss: 0.67709368755268\tValidation Loss: 0.6774158486655818\n",
            "Epoch 309  \tTraining Loss: 0.6767397635664664\tValidation Loss: 0.6770584602697801\n",
            "Epoch 310  \tTraining Loss: 0.6763878234671876\tValidation Loss: 0.6767030599318354\n",
            "Epoch 311  \tTraining Loss: 0.6760378561343553\tValidation Loss: 0.6763496365506774\n",
            "Epoch 312  \tTraining Loss: 0.6756898505098168\tValidation Loss: 0.6759981790873414\n",
            "Epoch 313  \tTraining Loss: 0.6753437955974047\tValidation Loss: 0.6756486765646206\n",
            "Epoch 314  \tTraining Loss: 0.6749996804625891\tValidation Loss: 0.6753011180667213\n",
            "Epoch 315  \tTraining Loss: 0.6746574942321331\tValidation Loss: 0.674955492738917\n",
            "Epoch 316  \tTraining Loss: 0.674317226093748\tValidation Loss: 0.6746117897872084\n",
            "Epoch 317  \tTraining Loss: 0.6739788652957535\tValidation Loss: 0.674269998477982\n",
            "Epoch 318  \tTraining Loss: 0.6736424011467356\tValidation Loss: 0.673930108137673\n",
            "Epoch 319  \tTraining Loss: 0.6733078230152107\tValidation Loss: 0.6735921081524278\n",
            "Epoch 320  \tTraining Loss: 0.6729751203292886\tValidation Loss: 0.6732559879677704\n",
            "Epoch 321  \tTraining Loss: 0.6726442825763391\tValidation Loss: 0.6729217370882703\n",
            "Epoch 322  \tTraining Loss: 0.6723152993026597\tValidation Loss: 0.672589345077211\n",
            "Epoch 323  \tTraining Loss: 0.6719881601131443\tValidation Loss: 0.6722588015562612\n",
            "Epoch 324  \tTraining Loss: 0.6716628546709565\tValidation Loss: 0.6719300962051491\n",
            "Epoch 325  \tTraining Loss: 0.6713393726972015\tValidation Loss: 0.6716032187613354\n",
            "Epoch 326  \tTraining Loss: 0.6710177039706021\tValidation Loss: 0.6712781590196918\n",
            "Epoch 327  \tTraining Loss: 0.6706978383271756\tValidation Loss: 0.6709549068321782\n",
            "Epoch 328  \tTraining Loss: 0.6703797656599118\tValidation Loss: 0.6706334521075237\n",
            "Epoch 329  \tTraining Loss: 0.6700634759184552\tValidation Loss: 0.6703137848109084\n",
            "Epoch 330  \tTraining Loss: 0.6697489591087865\tValidation Loss: 0.6699958949636479\n",
            "Epoch 331  \tTraining Loss: 0.6694362052929063\tValidation Loss: 0.6696797726428779\n",
            "Epoch 332  \tTraining Loss: 0.6691252045885226\tValidation Loss: 0.6693654079812424\n",
            "Epoch 333  \tTraining Loss: 0.6688159471687372\tValidation Loss: 0.6690527911665832\n",
            "Epoch 334  \tTraining Loss: 0.6685084232617351\tValidation Loss: 0.6687419124416294\n",
            "Epoch 335  \tTraining Loss: 0.6682026231504772\tValidation Loss: 0.6684327621036914\n",
            "Epoch 336  \tTraining Loss: 0.6678985371723917\tValidation Loss: 0.6681253305043551\n",
            "Epoch 337  \tTraining Loss: 0.6675961557190697\tValidation Loss: 0.6678196080491772\n",
            "Epoch 338  \tTraining Loss: 0.6672954692359614\tValidation Loss: 0.6675155851973836\n",
            "Epoch 339  \tTraining Loss: 0.6669964682220735\tValidation Loss: 0.667213252461569\n",
            "Epoch 340  \tTraining Loss: 0.6666991432296705\tValidation Loss: 0.6669126004073979\n",
            "Epoch 341  \tTraining Loss: 0.6664034848639747\tValidation Loss: 0.6666136196533066\n",
            "Epoch 342  \tTraining Loss: 0.6661094837828702\tValidation Loss: 0.6663163008702092\n",
            "Epoch 343  \tTraining Loss: 0.6658171306966069\tValidation Loss: 0.6660206347812029\n",
            "Epoch 344  \tTraining Loss: 0.6655264163675081\tValidation Loss: 0.6657266121612753\n",
            "Epoch 345  \tTraining Loss: 0.665237331609678\tValidation Loss: 0.6654342238370154\n",
            "Epoch 346  \tTraining Loss: 0.6649498672887111\tValidation Loss: 0.665143460686323\n",
            "Epoch 347  \tTraining Loss: 0.6646640143214041\tValidation Loss: 0.6648543136381224\n",
            "Epoch 348  \tTraining Loss: 0.6643797636754685\tValidation Loss: 0.6645667736720761\n",
            "Epoch 349  \tTraining Loss: 0.6640971063692461\tValidation Loss: 0.6642808318183014\n",
            "Epoch 350  \tTraining Loss: 0.6638160334714236\tValidation Loss: 0.663996479157087\n",
            "Epoch 351  \tTraining Loss: 0.6635365361007524\tValidation Loss: 0.6637137068186131\n",
            "Epoch 352  \tTraining Loss: 0.6632586054257654\tValidation Loss: 0.6634325059826708\n",
            "Epoch 353  \tTraining Loss: 0.6629822326645012\tValidation Loss: 0.6631528678783857\n",
            "Epoch 354  \tTraining Loss: 0.6627074090842233\tValidation Loss: 0.6628747837839403\n",
            "Epoch 355  \tTraining Loss: 0.6624341260011469\tValidation Loss: 0.6625982450263006\n",
            "Epoch 356  \tTraining Loss: 0.6621623747801625\tValidation Loss: 0.6623232429809418\n",
            "Epoch 357  \tTraining Loss: 0.6618921468345642\tValidation Loss: 0.6620497690715773\n",
            "Epoch 358  \tTraining Loss: 0.661623433625778\tValidation Loss: 0.6617778147698888\n",
            "Epoch 359  \tTraining Loss: 0.6613562266630919\tValidation Loss: 0.6615073715952572\n",
            "Epoch 360  \tTraining Loss: 0.6610905175033884\tValidation Loss: 0.6612384311144957\n",
            "Epoch 361  \tTraining Loss: 0.6608262977508764\tValidation Loss: 0.6609709849415841\n",
            "Epoch 362  \tTraining Loss: 0.6605635590568267\tValidation Loss: 0.6607050247374056\n",
            "Epoch 363  \tTraining Loss: 0.6603022931193088\tValidation Loss: 0.6604405422094823\n",
            "Epoch 364  \tTraining Loss: 0.6600424916829268\tValidation Loss: 0.6601775291117163\n",
            "Epoch 365  \tTraining Loss: 0.6597841465385601\tValidation Loss: 0.6599159772441281\n",
            "Epoch 366  \tTraining Loss: 0.6595272495231039\tValidation Loss: 0.6596558784525999\n",
            "Epoch 367  \tTraining Loss: 0.6592717925192105\tValidation Loss: 0.6593972246286174\n",
            "Epoch 368  \tTraining Loss: 0.6590177674550332\tValidation Loss: 0.6591400077090157\n",
            "Epoch 369  \tTraining Loss: 0.6587651663039714\tValidation Loss: 0.6588842196757246\n",
            "Epoch 370  \tTraining Loss: 0.6585139810844168\tValidation Loss: 0.6586298525555168\n",
            "Epoch 371  \tTraining Loss: 0.6582642038595012\tValidation Loss: 0.6583768984197556\n",
            "Epoch 372  \tTraining Loss: 0.658015826736846\tValidation Loss: 0.6581253493841467\n",
            "Epoch 373  \tTraining Loss: 0.6577688418683121\tValidation Loss: 0.657875197608489\n",
            "Epoch 374  \tTraining Loss: 0.657523241449753\tValidation Loss: 0.6576264352964281\n",
            "Epoch 375  \tTraining Loss: 0.6572790177207667\tValidation Loss: 0.6573790546952107\n",
            "Epoch 376  \tTraining Loss: 0.6570361629644523\tValidation Loss: 0.6571330480954405\n",
            "Epoch 377  \tTraining Loss: 0.6567946695071648\tValidation Loss: 0.656888407830835\n",
            "Epoch 378  \tTraining Loss: 0.6565545297182733\tValidation Loss: 0.656645126277985\n",
            "Epoch 379  \tTraining Loss: 0.6563157360099194\tValidation Loss: 0.6564031958561141\n",
            "Epoch 380  \tTraining Loss: 0.6560782808367781\tValidation Loss: 0.6561626090268398\n",
            "Epoch 381  \tTraining Loss: 0.6558421566958181\tValidation Loss: 0.6559233582939363\n",
            "Epoch 382  \tTraining Loss: 0.6556073561260669\tValidation Loss: 0.6556854362030986\n",
            "Epoch 383  \tTraining Loss: 0.655373871708373\tValidation Loss: 0.6554488353417078\n",
            "Epoch 384  \tTraining Loss: 0.6551416960651723\tValidation Loss: 0.6552135483385975\n",
            "Epoch 385  \tTraining Loss: 0.6549108218602547\tValidation Loss: 0.6549795678638219\n",
            "Epoch 386  \tTraining Loss: 0.6546812417985333\tValidation Loss: 0.6547468866284246\n",
            "Epoch 387  \tTraining Loss: 0.6544529486258123\tValidation Loss: 0.6545154973842102\n",
            "Epoch 388  \tTraining Loss: 0.654225935128559\tValidation Loss: 0.6542853929235146\n",
            "Epoch 389  \tTraining Loss: 0.6540001941336755\tValidation Loss: 0.6540565660789792\n",
            "Epoch 390  \tTraining Loss: 0.653775718508272\tValidation Loss: 0.6538290097233248\n",
            "Epoch 391  \tTraining Loss: 0.6535525011594411\tValidation Loss: 0.6536027167691272\n",
            "Epoch 392  \tTraining Loss: 0.6533305350340342\tValidation Loss: 0.6533776801685944\n",
            "Epoch 393  \tTraining Loss: 0.6531098131184386\tValidation Loss: 0.6531538929133442\n",
            "Epoch 394  \tTraining Loss: 0.6528903284383556\tValidation Loss: 0.6529313480341841\n",
            "Epoch 395  \tTraining Loss: 0.6526720740585804\tValidation Loss: 0.652710038600891\n",
            "Epoch 396  \tTraining Loss: 0.6524550430827827\tValidation Loss: 0.6524899577219946\n",
            "Epoch 397  \tTraining Loss: 0.6522392286532889\tValidation Loss: 0.6522710985445583\n",
            "Epoch 398  \tTraining Loss: 0.6520246239508652\tValidation Loss: 0.652053454253966\n",
            "Epoch 399  \tTraining Loss: 0.6518112221945029\tValidation Loss: 0.6518370180737053\n",
            "Epoch 400  \tTraining Loss: 0.6515990166412032\tValidation Loss: 0.6516217832651551\n",
            "lr, batch_size: (0.0001, 200)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.7937891531497792\n",
            "Epoch 2  \tTraining Loss: 0.7903017837901265\tValidation Loss: 0.7396026472419468\n",
            "Epoch 3  \tTraining Loss: 0.7359201909958584\tValidation Loss: 0.675473217954124\n",
            "Epoch 4  \tTraining Loss: 0.6712661177190833\tValidation Loss: 0.6108661728883575\n",
            "Epoch 5  \tTraining Loss: 0.606227869917893\tValidation Loss: 0.5558117685758847\n",
            "Epoch 6  \tTraining Loss: 0.551030858571779\tValidation Loss: 0.5187382538776669\n",
            "Epoch 7  \tTraining Loss: 0.5139269378406597\tValidation Loss: 0.4972108725344907\n",
            "Epoch 8  \tTraining Loss: 0.49244450017491054\tValidation Loss: 0.48432523213891027\n",
            "Epoch 9  \tTraining Loss: 0.4797020446015512\tValidation Loss: 0.4773738634482261\n",
            "Epoch 10  \tTraining Loss: 0.47290961912875606\tValidation Loss: 0.4736185060113791\n",
            "Epoch 11  \tTraining Loss: 0.46926021319125616\tValidation Loss: 0.4713233593608902\n",
            "Epoch 12  \tTraining Loss: 0.4670303753036299\tValidation Loss: 0.46970762388687903\n",
            "Epoch 13  \tTraining Loss: 0.46544461208806204\tValidation Loss: 0.4684198117442504\n",
            "Epoch 14  \tTraining Loss: 0.4641654015678427\tValidation Loss: 0.4672966183346017\n",
            "Epoch 15  \tTraining Loss: 0.46303800712571347\tValidation Loss: 0.46626338537364975\n",
            "Epoch 16  \tTraining Loss: 0.4619919229554678\tValidation Loss: 0.4652792205581484\n",
            "Epoch 17  \tTraining Loss: 0.46099243968046144\tValidation Loss: 0.46432451352518544\n",
            "Epoch 18  \tTraining Loss: 0.46002063417970723\tValidation Loss: 0.4633874760150339\n",
            "Epoch 19  \tTraining Loss: 0.459067433278871\tValidation Loss: 0.46246451276390305\n",
            "Epoch 20  \tTraining Loss: 0.458127954812531\tValidation Loss: 0.4615526403955348\n",
            "Epoch 21  \tTraining Loss: 0.45719741263663033\tValidation Loss: 0.4606470049889237\n",
            "Epoch 22  \tTraining Loss: 0.4562721752260736\tValidation Loss: 0.45974690672242563\n",
            "Epoch 23  \tTraining Loss: 0.4553533368327994\tValidation Loss: 0.45885298383840567\n",
            "Epoch 24  \tTraining Loss: 0.45444194138137806\tValidation Loss: 0.4579642564831126\n",
            "Epoch 25  \tTraining Loss: 0.4535355234993162\tValidation Loss: 0.4570780289777831\n",
            "Epoch 26  \tTraining Loss: 0.4526326663956963\tValidation Loss: 0.45619294600287613\n",
            "Epoch 27  \tTraining Loss: 0.45173091406068294\tValidation Loss: 0.4553092818280986\n",
            "Epoch 28  \tTraining Loss: 0.4508295508974375\tValidation Loss: 0.4544281166304428\n",
            "Epoch 29  \tTraining Loss: 0.4499280641045611\tValidation Loss: 0.4535460810894125\n",
            "Epoch 30  \tTraining Loss: 0.4490266050027987\tValidation Loss: 0.4526627523871726\n",
            "Epoch 31  \tTraining Loss: 0.448125527230229\tValidation Loss: 0.45177785089323713\n",
            "Epoch 32  \tTraining Loss: 0.447223367716302\tValidation Loss: 0.4508931367513975\n",
            "Epoch 33  \tTraining Loss: 0.4463209541017042\tValidation Loss: 0.45000689094918694\n",
            "Epoch 34  \tTraining Loss: 0.44541803582112105\tValidation Loss: 0.4491199661315657\n",
            "Epoch 35  \tTraining Loss: 0.4445162676276918\tValidation Loss: 0.44823103305942563\n",
            "Epoch 36  \tTraining Loss: 0.4436145285252662\tValidation Loss: 0.4473405459754245\n",
            "Epoch 37  \tTraining Loss: 0.442712761510956\tValidation Loss: 0.44644912658120905\n",
            "Epoch 38  \tTraining Loss: 0.44181023781340545\tValidation Loss: 0.44555708468575717\n",
            "Epoch 39  \tTraining Loss: 0.4409063285477348\tValidation Loss: 0.4446622860276687\n",
            "Epoch 40  \tTraining Loss: 0.4400009766714702\tValidation Loss: 0.4437637195234951\n",
            "Epoch 41  \tTraining Loss: 0.4390935117968859\tValidation Loss: 0.4428622224727171\n",
            "Epoch 42  \tTraining Loss: 0.43818404115487314\tValidation Loss: 0.44196008741226256\n",
            "Epoch 43  \tTraining Loss: 0.43727251758489205\tValidation Loss: 0.4410549025980905\n",
            "Epoch 44  \tTraining Loss: 0.4363577298550211\tValidation Loss: 0.44014817672464374\n",
            "Epoch 45  \tTraining Loss: 0.4354408927441622\tValidation Loss: 0.4392394765795721\n",
            "Epoch 46  \tTraining Loss: 0.43452271408974724\tValidation Loss: 0.43832942279734805\n",
            "Epoch 47  \tTraining Loss: 0.4336034338246571\tValidation Loss: 0.43741781641838084\n",
            "Epoch 48  \tTraining Loss: 0.4326830516508903\tValidation Loss: 0.4365054167104606\n",
            "Epoch 49  \tTraining Loss: 0.43176187477250294\tValidation Loss: 0.43558941896999226\n",
            "Epoch 50  \tTraining Loss: 0.4308379506528068\tValidation Loss: 0.43467118108778274\n",
            "Epoch 51  \tTraining Loss: 0.42991166888939963\tValidation Loss: 0.4337509241495869\n",
            "Epoch 52  \tTraining Loss: 0.428983081615076\tValidation Loss: 0.432828364517572\n",
            "Epoch 53  \tTraining Loss: 0.4280524888701792\tValidation Loss: 0.4319035793437747\n",
            "Epoch 54  \tTraining Loss: 0.4271203463969871\tValidation Loss: 0.43097663765015837\n",
            "Epoch 55  \tTraining Loss: 0.42618638865120234\tValidation Loss: 0.4300480559391896\n",
            "Epoch 56  \tTraining Loss: 0.4252509474799466\tValidation Loss: 0.4291183386701177\n",
            "Epoch 57  \tTraining Loss: 0.424314598838022\tValidation Loss: 0.42818697978449116\n",
            "Epoch 58  \tTraining Loss: 0.42337771847203826\tValidation Loss: 0.4272541722273017\n",
            "Epoch 59  \tTraining Loss: 0.4224402268997953\tValidation Loss: 0.4263204541273726\n",
            "Epoch 60  \tTraining Loss: 0.4215014614127958\tValidation Loss: 0.4253850197519322\n",
            "Epoch 61  \tTraining Loss: 0.42056166830720654\tValidation Loss: 0.4244479583530851\n",
            "Epoch 62  \tTraining Loss: 0.419621525938314\tValidation Loss: 0.4235088817808493\n",
            "Epoch 63  \tTraining Loss: 0.41868041313554144\tValidation Loss: 0.42256926170653464\n",
            "Epoch 64  \tTraining Loss: 0.4177402197403607\tValidation Loss: 0.4216297366815058\n",
            "Epoch 65  \tTraining Loss: 0.4168000998141144\tValidation Loss: 0.4206892813926245\n",
            "Epoch 66  \tTraining Loss: 0.41585928458384297\tValidation Loss: 0.4197482707435665\n",
            "Epoch 67  \tTraining Loss: 0.41491855036513825\tValidation Loss: 0.41880596073056864\n",
            "Epoch 68  \tTraining Loss: 0.413978577361546\tValidation Loss: 0.41786206194199277\n",
            "Epoch 69  \tTraining Loss: 0.4130400450899515\tValidation Loss: 0.41691933687405114\n",
            "Epoch 70  \tTraining Loss: 0.41210227222878765\tValidation Loss: 0.41597396323368474\n",
            "Epoch 71  \tTraining Loss: 0.41116163754383495\tValidation Loss: 0.41502602601758426\n",
            "Epoch 72  \tTraining Loss: 0.41021941106156845\tValidation Loss: 0.4140780164116849\n",
            "Epoch 73  \tTraining Loss: 0.4092765633564972\tValidation Loss: 0.41312715347493956\n",
            "Epoch 74  \tTraining Loss: 0.4083268230266829\tValidation Loss: 0.4121638279758138\n",
            "Epoch 75  \tTraining Loss: 0.4073679729271208\tValidation Loss: 0.4111831821351704\n",
            "Epoch 76  \tTraining Loss: 0.4063939820249611\tValidation Loss: 0.410175280346891\n",
            "Epoch 77  \tTraining Loss: 0.4053928317175932\tValidation Loss: 0.40911316189709884\n",
            "Epoch 78  \tTraining Loss: 0.4043246329598917\tValidation Loss: 0.40795577453164183\n",
            "Epoch 79  \tTraining Loss: 0.40314475669915995\tValidation Loss: 0.4066988260329525\n",
            "Epoch 80  \tTraining Loss: 0.40183852644338475\tValidation Loss: 0.40528424180852307\n",
            "Epoch 81  \tTraining Loss: 0.40033127150317066\tValidation Loss: 0.4033127470782263\n",
            "Epoch 82  \tTraining Loss: 0.39831051316998156\tValidation Loss: 0.400257926193116\n",
            "Epoch 83  \tTraining Loss: 0.39511364824481876\tValidation Loss: 0.3957003898126781\n",
            "Epoch 84  \tTraining Loss: 0.3902521011241419\tValidation Loss: 0.391954731712515\n",
            "Epoch 85  \tTraining Loss: 0.3863132943699981\tValidation Loss: 0.3899705449331144\n",
            "Epoch 86  \tTraining Loss: 0.38421340357888406\tValidation Loss: 0.3886310300922752\n",
            "Epoch 87  \tTraining Loss: 0.38280890834540315\tValidation Loss: 0.3875141058473806\n",
            "Epoch 88  \tTraining Loss: 0.3816560226855519\tValidation Loss: 0.38647769186579767\n",
            "Epoch 89  \tTraining Loss: 0.3805986273853241\tValidation Loss: 0.38547265041057005\n",
            "Epoch 90  \tTraining Loss: 0.379581585861139\tValidation Loss: 0.3844815371383754\n",
            "Epoch 91  \tTraining Loss: 0.3785846789891427\tValidation Loss: 0.3835000776816758\n",
            "Epoch 92  \tTraining Loss: 0.37760138847171515\tValidation Loss: 0.38252661998758375\n",
            "Epoch 93  \tTraining Loss: 0.37662792022028835\tValidation Loss: 0.3815611341785969\n",
            "Epoch 94  \tTraining Loss: 0.3756629149509768\tValidation Loss: 0.3806037856234009\n",
            "Epoch 95  \tTraining Loss: 0.37470668189762063\tValidation Loss: 0.37965437927582263\n",
            "Epoch 96  \tTraining Loss: 0.37375969891191135\tValidation Loss: 0.37871238041477073\n",
            "Epoch 97  \tTraining Loss: 0.3728214167898011\tValidation Loss: 0.3777793962988748\n",
            "Epoch 98  \tTraining Loss: 0.37189256686400396\tValidation Loss: 0.3768553227785591\n",
            "Epoch 99  \tTraining Loss: 0.37097245620132974\tValidation Loss: 0.37593669274263697\n",
            "Epoch 100  \tTraining Loss: 0.37006037362677535\tValidation Loss: 0.37502619635204026\n",
            "Epoch 101  \tTraining Loss: 0.36915771393949237\tValidation Loss: 0.37412235158384555\n",
            "Epoch 102  \tTraining Loss: 0.36826183633322945\tValidation Loss: 0.37321941513811685\n",
            "Epoch 103  \tTraining Loss: 0.3673705113775454\tValidation Loss: 0.37232243423159084\n",
            "Epoch 104  \tTraining Loss: 0.3664864819199288\tValidation Loss: 0.37143004382535\n",
            "Epoch 105  \tTraining Loss: 0.3656073743154725\tValidation Loss: 0.37053228441930003\n",
            "Epoch 106  \tTraining Loss: 0.3647268672804672\tValidation Loss: 0.3696193174665022\n",
            "Epoch 107  \tTraining Loss: 0.36382633935339925\tValidation Loss: 0.36865453182117175\n",
            "Epoch 108  \tTraining Loss: 0.362861295500654\tValidation Loss: 0.3675364225314318\n",
            "Epoch 109  \tTraining Loss: 0.3617259905744314\tValidation Loss: 0.36598517033470696\n",
            "Epoch 110  \tTraining Loss: 0.3601286759791841\tValidation Loss: 0.3623225157604795\n",
            "Epoch 111  \tTraining Loss: 0.3563816414851839\tValidation Loss: 0.35400896276354543\n",
            "Epoch 112  \tTraining Loss: 0.34808402380264825\tValidation Loss: 0.3463954204948993\n",
            "Epoch 113  \tTraining Loss: 0.34043844735474493\tValidation Loss: 0.3431059831653356\n",
            "Epoch 114  \tTraining Loss: 0.33710340190212\tValidation Loss: 0.3414708530417665\n",
            "Epoch 115  \tTraining Loss: 0.33544001242899396\tValidation Loss: 0.340379028295548\n",
            "Epoch 116  \tTraining Loss: 0.3343320593182114\tValidation Loss: 0.3394673950873265\n",
            "Epoch 117  \tTraining Loss: 0.33341298084546617\tValidation Loss: 0.33862004123847683\n",
            "Epoch 118  \tTraining Loss: 0.33256359514592476\tValidation Loss: 0.33779998316860216\n",
            "Epoch 119  \tTraining Loss: 0.3317443711471905\tValidation Loss: 0.33699526951877706\n",
            "Epoch 120  \tTraining Loss: 0.33094274819252034\tValidation Loss: 0.33620224684133\n",
            "Epoch 121  \tTraining Loss: 0.33015418403850527\tValidation Loss: 0.33542050459792816\n",
            "Epoch 122  \tTraining Loss: 0.32937739430135404\tValidation Loss: 0.3346492088912953\n",
            "Epoch 123  \tTraining Loss: 0.32861181473718715\tValidation Loss: 0.33388837548704586\n",
            "Epoch 124  \tTraining Loss: 0.3278574258735916\tValidation Loss: 0.33313813163304345\n",
            "Epoch 125  \tTraining Loss: 0.3271139495743487\tValidation Loss: 0.33239908597681944\n",
            "Epoch 126  \tTraining Loss: 0.3263815752237541\tValidation Loss: 0.3316708038521047\n",
            "Epoch 127  \tTraining Loss: 0.3256601304957933\tValidation Loss: 0.3309531768453015\n",
            "Epoch 128  \tTraining Loss: 0.32494961273642664\tValidation Loss: 0.33024597792306065\n",
            "Epoch 129  \tTraining Loss: 0.32424995932294637\tValidation Loss: 0.3295485187354402\n",
            "Epoch 130  \tTraining Loss: 0.3235605244810727\tValidation Loss: 0.32886079677675617\n",
            "Epoch 131  \tTraining Loss: 0.3228812780143397\tValidation Loss: 0.3281829751392484\n",
            "Epoch 132  \tTraining Loss: 0.3222122356663404\tValidation Loss: 0.32751477524736167\n",
            "Epoch 133  \tTraining Loss: 0.3215530610881069\tValidation Loss: 0.3268560963983732\n",
            "Epoch 134  \tTraining Loss: 0.3209039021503009\tValidation Loss: 0.3262069361549692\n",
            "Epoch 135  \tTraining Loss: 0.3202642222331164\tValidation Loss: 0.3255674968587932\n",
            "Epoch 136  \tTraining Loss: 0.3196340453913359\tValidation Loss: 0.3249372978050929\n",
            "Epoch 137  \tTraining Loss: 0.31901324543471943\tValidation Loss: 0.32431586217390834\n",
            "Epoch 138  \tTraining Loss: 0.31840184695647233\tValidation Loss: 0.32370331061826363\n",
            "Epoch 139  \tTraining Loss: 0.3177995915334371\tValidation Loss: 0.32310027289091103\n",
            "Epoch 140  \tTraining Loss: 0.31720641342431233\tValidation Loss: 0.32250590628007875\n",
            "Epoch 141  \tTraining Loss: 0.3166219579076796\tValidation Loss: 0.32192012244198653\n",
            "Epoch 142  \tTraining Loss: 0.31604645171764384\tValidation Loss: 0.32134301869923565\n",
            "Epoch 143  \tTraining Loss: 0.3154796291990588\tValidation Loss: 0.3207744005954364\n",
            "Epoch 144  \tTraining Loss: 0.3149213442736238\tValidation Loss: 0.32021388493773756\n",
            "Epoch 145  \tTraining Loss: 0.31437141338284813\tValidation Loss: 0.3196617614483723\n",
            "Epoch 146  \tTraining Loss: 0.3138299827496432\tValidation Loss: 0.3191176703095907\n",
            "Epoch 147  \tTraining Loss: 0.31329645621691965\tValidation Loss: 0.31858133752763546\n",
            "Epoch 148  \tTraining Loss: 0.3127706292283547\tValidation Loss: 0.3180528176885997\n",
            "Epoch 149  \tTraining Loss: 0.3122526305009191\tValidation Loss: 0.3175318201326348\n",
            "Epoch 150  \tTraining Loss: 0.31174202744535734\tValidation Loss: 0.3170181424074684\n",
            "Epoch 151  \tTraining Loss: 0.31123885791867095\tValidation Loss: 0.31651177000220193\n",
            "Epoch 152  \tTraining Loss: 0.3107429688992671\tValidation Loss: 0.31601236148238426\n",
            "Epoch 153  \tTraining Loss: 0.31025438306970243\tValidation Loss: 0.3155196261628218\n",
            "Epoch 154  \tTraining Loss: 0.30977298544209964\tValidation Loss: 0.3150339730326214\n",
            "Epoch 155  \tTraining Loss: 0.3092986174664294\tValidation Loss: 0.31455521431246625\n",
            "Epoch 156  \tTraining Loss: 0.30883089934568775\tValidation Loss: 0.31408347475571813\n",
            "Epoch 157  \tTraining Loss: 0.30836993465785467\tValidation Loss: 0.3136182846987536\n",
            "Epoch 158  \tTraining Loss: 0.30791558127071333\tValidation Loss: 0.3131599836567367\n",
            "Epoch 159  \tTraining Loss: 0.3074677825867185\tValidation Loss: 0.3127082795439167\n",
            "Epoch 160  \tTraining Loss: 0.3070261044232248\tValidation Loss: 0.3122627418218909\n",
            "Epoch 161  \tTraining Loss: 0.3065905182845531\tValidation Loss: 0.31182313336908646\n",
            "Epoch 162  \tTraining Loss: 0.3061610470447732\tValidation Loss: 0.31138883825047237\n",
            "Epoch 163  \tTraining Loss: 0.3057369731480374\tValidation Loss: 0.31096036593733134\n",
            "Epoch 164  \tTraining Loss: 0.3053184531841\tValidation Loss: 0.31053683123016057\n",
            "Epoch 165  \tTraining Loss: 0.3049052128226577\tValidation Loss: 0.3101188330341531\n",
            "Epoch 166  \tTraining Loss: 0.3044973466744261\tValidation Loss: 0.30970624921007867\n",
            "Epoch 167  \tTraining Loss: 0.3040946847336912\tValidation Loss: 0.30929878153464185\n",
            "Epoch 168  \tTraining Loss: 0.30369663503731853\tValidation Loss: 0.3088957046102381\n",
            "Epoch 169  \tTraining Loss: 0.3033033526597804\tValidation Loss: 0.3084966605823968\n",
            "Epoch 170  \tTraining Loss: 0.30291483523060514\tValidation Loss: 0.30810225814732606\n",
            "Epoch 171  \tTraining Loss: 0.3025313901152937\tValidation Loss: 0.30771255084649085\n",
            "Epoch 172  \tTraining Loss: 0.3021524267735312\tValidation Loss: 0.30732771493884814\n",
            "Epoch 173  \tTraining Loss: 0.30177802131951537\tValidation Loss: 0.3069469827408685\n",
            "Epoch 174  \tTraining Loss: 0.30140753408986914\tValidation Loss: 0.3065707169511816\n",
            "Epoch 175  \tTraining Loss: 0.30104117147415377\tValidation Loss: 0.3061987895158401\n",
            "Epoch 176  \tTraining Loss: 0.3006785621997629\tValidation Loss: 0.30583114493903757\n",
            "Epoch 177  \tTraining Loss: 0.3003196629689193\tValidation Loss: 0.30546705930730295\n",
            "Epoch 178  \tTraining Loss: 0.2999635419936533\tValidation Loss: 0.3051046583306923\n",
            "Epoch 179  \tTraining Loss: 0.29961127322164727\tValidation Loss: 0.3047437464511171\n",
            "Epoch 180  \tTraining Loss: 0.29926137911989126\tValidation Loss: 0.304384713667403\n",
            "Epoch 181  \tTraining Loss: 0.2989128522975289\tValidation Loss: 0.3040253214282598\n",
            "Epoch 182  \tTraining Loss: 0.2985627816340139\tValidation Loss: 0.3036597170405859\n",
            "Epoch 183  \tTraining Loss: 0.29820389769326633\tValidation Loss: 0.3032892373471054\n",
            "Epoch 184  \tTraining Loss: 0.2978376578782676\tValidation Loss: 0.30290224312816816\n",
            "Epoch 185  \tTraining Loss: 0.29745247325957097\tValidation Loss: 0.30247418239203533\n",
            "Epoch 186  \tTraining Loss: 0.297026784567398\tValidation Loss: 0.30196910431825985\n",
            "Epoch 187  \tTraining Loss: 0.2965477723133139\tValidation Loss: 0.3013420497661846\n",
            "Epoch 188  \tTraining Loss: 0.2959794517872663\tValidation Loss: 0.30031168810243236\n",
            "Epoch 189  \tTraining Loss: 0.29512458892903914\tValidation Loss: 0.29794741252686535\n",
            "Epoch 190  \tTraining Loss: 0.2930947193476868\tValidation Loss: 0.2903655503375099\n",
            "Epoch 191  \tTraining Loss: 0.2857075063073514\tValidation Loss: 0.27499360013291374\n",
            "Epoch 192  \tTraining Loss: 0.2700004764750307\tValidation Loss: 0.2678783059340254\n",
            "Epoch 193  \tTraining Loss: 0.2627827819684379\tValidation Loss: 0.26594466387867055\n",
            "Epoch 194  \tTraining Loss: 0.2608043939273886\tValidation Loss: 0.26520792318403585\n",
            "Epoch 195  \tTraining Loss: 0.26005143821839993\tValidation Loss: 0.2647483536086536\n",
            "Epoch 196  \tTraining Loss: 0.259588624888154\tValidation Loss: 0.26435605396018974\n",
            "Epoch 197  \tTraining Loss: 0.2591988916911927\tValidation Loss: 0.26398322373623084\n",
            "Epoch 198  \tTraining Loss: 0.25883116965781516\tValidation Loss: 0.26361975104866586\n",
            "Epoch 199  \tTraining Loss: 0.2584735666258112\tValidation Loss: 0.263262880598646\n",
            "Epoch 200  \tTraining Loss: 0.25812287317101584\tValidation Loss: 0.262911966144481\n",
            "Epoch 201  \tTraining Loss: 0.25777819185068346\tValidation Loss: 0.2625670283165287\n",
            "Epoch 202  \tTraining Loss: 0.25743922241934813\tValidation Loss: 0.26222758032916504\n",
            "Epoch 203  \tTraining Loss: 0.25710572867557535\tValidation Loss: 0.2618938432603724\n",
            "Epoch 204  \tTraining Loss: 0.25677746466988954\tValidation Loss: 0.26156531050737974\n",
            "Epoch 205  \tTraining Loss: 0.25645422018305775\tValidation Loss: 0.2612419235256832\n",
            "Epoch 206  \tTraining Loss: 0.2561359027134254\tValidation Loss: 0.2609236061996937\n",
            "Epoch 207  \tTraining Loss: 0.2558224114898485\tValidation Loss: 0.2606102204444244\n",
            "Epoch 208  \tTraining Loss: 0.2555135552055597\tValidation Loss: 0.26030154772026987\n",
            "Epoch 209  \tTraining Loss: 0.2552090921919164\tValidation Loss: 0.2599971507334856\n",
            "Epoch 210  \tTraining Loss: 0.25490899039230336\tValidation Loss: 0.2596972989291461\n",
            "Epoch 211  \tTraining Loss: 0.25461323537867425\tValidation Loss: 0.2594020499199736\n",
            "Epoch 212  \tTraining Loss: 0.2543216207303062\tValidation Loss: 0.2591111106229058\n",
            "Epoch 213  \tTraining Loss: 0.2540341506392702\tValidation Loss: 0.25882439359413084\n",
            "Epoch 214  \tTraining Loss: 0.2537506605388366\tValidation Loss: 0.25854174264693214\n",
            "Epoch 215  \tTraining Loss: 0.25347100160131036\tValidation Loss: 0.2582631106284256\n",
            "Epoch 216  \tTraining Loss: 0.25319512753505063\tValidation Loss: 0.2579882318617196\n",
            "Epoch 217  \tTraining Loss: 0.25292292740621514\tValidation Loss: 0.2577170279508818\n",
            "Epoch 218  \tTraining Loss: 0.2526543181417047\tValidation Loss: 0.2574494160226696\n",
            "Epoch 219  \tTraining Loss: 0.252389177561973\tValidation Loss: 0.257185343059832\n",
            "Epoch 220  \tTraining Loss: 0.2521274161834666\tValidation Loss: 0.2569247315705698\n",
            "Epoch 221  \tTraining Loss: 0.25186897409037234\tValidation Loss: 0.2566674614626934\n",
            "Epoch 222  \tTraining Loss: 0.25161375961710675\tValidation Loss: 0.2564132345816694\n",
            "Epoch 223  \tTraining Loss: 0.2513616781119867\tValidation Loss: 0.2561620622421803\n",
            "Epoch 224  \tTraining Loss: 0.25111257827961314\tValidation Loss: 0.2559136683248965\n",
            "Epoch 225  \tTraining Loss: 0.25086635190708295\tValidation Loss: 0.2556682953301308\n",
            "Epoch 226  \tTraining Loss: 0.250622951022442\tValidation Loss: 0.25542609643681957\n",
            "Epoch 227  \tTraining Loss: 0.25038233352059663\tValidation Loss: 0.25518691764000334\n",
            "Epoch 228  \tTraining Loss: 0.2501445182205218\tValidation Loss: 0.25495069185954516\n",
            "Epoch 229  \tTraining Loss: 0.24990945344288684\tValidation Loss: 0.25471723838719645\n",
            "Epoch 230  \tTraining Loss: 0.24967707339342954\tValidation Loss: 0.254486439613132\n",
            "Epoch 231  \tTraining Loss: 0.24944734191645881\tValidation Loss: 0.2542584343325857\n",
            "Epoch 232  \tTraining Loss: 0.2492201835162948\tValidation Loss: 0.2540330844294091\n",
            "Epoch 233  \tTraining Loss: 0.24899555727521686\tValidation Loss: 0.25381027168818854\n",
            "Epoch 234  \tTraining Loss: 0.24877336807751516\tValidation Loss: 0.2535898703916114\n",
            "Epoch 235  \tTraining Loss: 0.2485535570598786\tValidation Loss: 0.25337202998409497\n",
            "Epoch 236  \tTraining Loss: 0.24833610836697553\tValidation Loss: 0.2531566216954869\n",
            "Epoch 237  \tTraining Loss: 0.24812099837684598\tValidation Loss: 0.2529435207393502\n",
            "Epoch 238  \tTraining Loss: 0.24790818792973934\tValidation Loss: 0.25273274805711543\n",
            "Epoch 239  \tTraining Loss: 0.24769756629475445\tValidation Loss: 0.2525242240920224\n",
            "Epoch 240  \tTraining Loss: 0.2474890540177743\tValidation Loss: 0.252317463814946\n",
            "Epoch 241  \tTraining Loss: 0.24728256080700609\tValidation Loss: 0.2521126251215407\n",
            "Epoch 242  \tTraining Loss: 0.247078093462745\tValidation Loss: 0.25190980895755455\n",
            "Epoch 243  \tTraining Loss: 0.24687560275384102\tValidation Loss: 0.2517088894240016\n",
            "Epoch 244  \tTraining Loss: 0.24667507902095925\tValidation Loss: 0.251510007700397\n",
            "Epoch 245  \tTraining Loss: 0.24647652225275662\tValidation Loss: 0.25131284475004056\n",
            "Epoch 246  \tTraining Loss: 0.24627991142028383\tValidation Loss: 0.25111769831348574\n",
            "Epoch 247  \tTraining Loss: 0.2460852595695485\tValidation Loss: 0.2509246985834862\n",
            "Epoch 248  \tTraining Loss: 0.24589246622668748\tValidation Loss: 0.25073361944076067\n",
            "Epoch 249  \tTraining Loss: 0.24570151478958496\tValidation Loss: 0.25054417906902265\n",
            "Epoch 250  \tTraining Loss: 0.24551234950175788\tValidation Loss: 0.25035671075792215\n",
            "Epoch 251  \tTraining Loss: 0.24532495470581606\tValidation Loss: 0.2501710016371513\n",
            "Epoch 252  \tTraining Loss: 0.24513927869417224\tValidation Loss: 0.24998690210579358\n",
            "Epoch 253  \tTraining Loss: 0.24495530401626914\tValidation Loss: 0.2498044652640512\n",
            "Epoch 254  \tTraining Loss: 0.2447730381608143\tValidation Loss: 0.24962381378385134\n",
            "Epoch 255  \tTraining Loss: 0.24459246165867918\tValidation Loss: 0.24944480703540833\n",
            "Epoch 256  \tTraining Loss: 0.2444134891000501\tValidation Loss: 0.24926746650105147\n",
            "Epoch 257  \tTraining Loss: 0.244236117078939\tValidation Loss: 0.24909176430771346\n",
            "Epoch 258  \tTraining Loss: 0.24406033279427053\tValidation Loss: 0.24891770593524992\n",
            "Epoch 259  \tTraining Loss: 0.2438860942888393\tValidation Loss: 0.24874523055407335\n",
            "Epoch 260  \tTraining Loss: 0.24371339523281757\tValidation Loss: 0.2485743420568226\n",
            "Epoch 261  \tTraining Loss: 0.24354225631243756\tValidation Loss: 0.24840495430379209\n",
            "Epoch 262  \tTraining Loss: 0.2433726788705292\tValidation Loss: 0.24823699032686422\n",
            "Epoch 263  \tTraining Loss: 0.24320460699878382\tValidation Loss: 0.24807048915265675\n",
            "Epoch 264  \tTraining Loss: 0.24303794122133568\tValidation Loss: 0.24790551202297526\n",
            "Epoch 265  \tTraining Loss: 0.242872615797319\tValidation Loss: 0.2477416022133903\n",
            "Epoch 266  \tTraining Loss: 0.24270854913709536\tValidation Loss: 0.2475791795602484\n",
            "Epoch 267  \tTraining Loss: 0.24254585232646195\tValidation Loss: 0.24741823334870353\n",
            "Epoch 268  \tTraining Loss: 0.2423844159060759\tValidation Loss: 0.24725860860150797\n",
            "Epoch 269  \tTraining Loss: 0.2422242713746643\tValidation Loss: 0.24710030580199627\n",
            "Epoch 270  \tTraining Loss: 0.24206542772452955\tValidation Loss: 0.24694303835339682\n",
            "Epoch 271  \tTraining Loss: 0.24190783262058257\tValidation Loss: 0.24678713601845995\n",
            "Epoch 272  \tTraining Loss: 0.24175150308485088\tValidation Loss: 0.24663239180166457\n",
            "Epoch 273  \tTraining Loss: 0.24159622098219538\tValidation Loss: 0.24647879428010896\n",
            "Epoch 274  \tTraining Loss: 0.2414420788202831\tValidation Loss: 0.2463263864142121\n",
            "Epoch 275  \tTraining Loss: 0.24128916776957676\tValidation Loss: 0.24617527040975923\n",
            "Epoch 276  \tTraining Loss: 0.24113747052225884\tValidation Loss: 0.24602524493132308\n",
            "Epoch 277  \tTraining Loss: 0.2409869329697373\tValidation Loss: 0.24587630350545342\n",
            "Epoch 278  \tTraining Loss: 0.24083760717861308\tValidation Loss: 0.24572844605901156\n",
            "Epoch 279  \tTraining Loss: 0.24068935292276392\tValidation Loss: 0.24558074193602977\n",
            "Epoch 280  \tTraining Loss: 0.24054190022581856\tValidation Loss: 0.24543324816969825\n",
            "Epoch 281  \tTraining Loss: 0.24039560972667856\tValidation Loss: 0.24528624209454936\n",
            "Epoch 282  \tTraining Loss: 0.2402504633096237\tValidation Loss: 0.24514020545829412\n",
            "Epoch 283  \tTraining Loss: 0.24010642984274008\tValidation Loss: 0.24499529033961176\n",
            "Epoch 284  \tTraining Loss: 0.23996339848682807\tValidation Loss: 0.2448507862973152\n",
            "Epoch 285  \tTraining Loss: 0.239821477630449\tValidation Loss: 0.24470742398884823\n",
            "Epoch 286  \tTraining Loss: 0.23968058208165507\tValidation Loss: 0.24456440985908512\n",
            "Epoch 287  \tTraining Loss: 0.239540284046024\tValidation Loss: 0.24442186149306613\n",
            "Epoch 288  \tTraining Loss: 0.23940079457013141\tValidation Loss: 0.2442801710951138\n",
            "Epoch 289  \tTraining Loss: 0.23926203769545779\tValidation Loss: 0.24413784213072023\n",
            "Epoch 290  \tTraining Loss: 0.23912323578080055\tValidation Loss: 0.2439954332963303\n",
            "Epoch 291  \tTraining Loss: 0.23898415414098453\tValidation Loss: 0.2438510329295777\n",
            "Epoch 292  \tTraining Loss: 0.2388435510810129\tValidation Loss: 0.24370154890191356\n",
            "Epoch 293  \tTraining Loss: 0.2387017320284666\tValidation Loss: 0.24354636810043767\n",
            "Epoch 294  \tTraining Loss: 0.23855588036980357\tValidation Loss: 0.24338331129288335\n",
            "Epoch 295  \tTraining Loss: 0.23840558702879777\tValidation Loss: 0.2432098149989956\n",
            "Epoch 296  \tTraining Loss: 0.23824738134798903\tValidation Loss: 0.24303150683763863\n",
            "Epoch 297  \tTraining Loss: 0.23807944800293082\tValidation Loss: 0.2428401665785304\n",
            "Epoch 298  \tTraining Loss: 0.23789480040450264\tValidation Loss: 0.24262279702839765\n",
            "Epoch 299  \tTraining Loss: 0.23768676557428456\tValidation Loss: 0.24236776955363837\n",
            "Epoch 300  \tTraining Loss: 0.2374441165977077\tValidation Loss: 0.24199911706769425\n",
            "Epoch 301  \tTraining Loss: 0.2371190360364851\tValidation Loss: 0.2414503203615154\n",
            "Epoch 302  \tTraining Loss: 0.23664635990797192\tValidation Loss: 0.240504087625262\n",
            "Epoch 303  \tTraining Loss: 0.23576932650994994\tValidation Loss: 0.23860790828836467\n",
            "Epoch 304  \tTraining Loss: 0.2339505231306391\tValidation Loss: 0.23562689985046156\n",
            "Epoch 305  \tTraining Loss: 0.23116349034681946\tValidation Loss: 0.23346759923443738\n",
            "Epoch 306  \tTraining Loss: 0.22918892519497983\tValidation Loss: 0.23202165285073756\n",
            "Epoch 307  \tTraining Loss: 0.22787025605719913\tValidation Loss: 0.22798800970002345\n",
            "Epoch 308  \tTraining Loss: 0.22406028835326122\tValidation Loss: 0.2195911544157873\n",
            "Epoch 309  \tTraining Loss: 0.21620636008387717\tValidation Loss: 0.21713669782950082\n",
            "Epoch 310  \tTraining Loss: 0.21397756602153664\tValidation Loss: 0.21634693061498922\n",
            "Epoch 311  \tTraining Loss: 0.2132478278776737\tValidation Loss: 0.2158441025143227\n",
            "Epoch 312  \tTraining Loss: 0.21274973883866777\tValidation Loss: 0.21540716880870467\n",
            "Epoch 313  \tTraining Loss: 0.21229978314888018\tValidation Loss: 0.21499604579210388\n",
            "Epoch 314  \tTraining Loss: 0.21186985340619077\tValidation Loss: 0.21460062100766555\n",
            "Epoch 315  \tTraining Loss: 0.2114548605089315\tValidation Loss: 0.2142184313974726\n",
            "Epoch 316  \tTraining Loss: 0.21105351571751635\tValidation Loss: 0.21384837432336584\n",
            "Epoch 317  \tTraining Loss: 0.2106649969213102\tValidation Loss: 0.21348956790000603\n",
            "Epoch 318  \tTraining Loss: 0.21028867687365096\tValidation Loss: 0.21314172695641756\n",
            "Epoch 319  \tTraining Loss: 0.20992395760168828\tValidation Loss: 0.21280376770224016\n",
            "Epoch 320  \tTraining Loss: 0.20957031887275399\tValidation Loss: 0.21247577921448285\n",
            "Epoch 321  \tTraining Loss: 0.20922728573673416\tValidation Loss: 0.21215707306720166\n",
            "Epoch 322  \tTraining Loss: 0.20889437270711247\tValidation Loss: 0.211847141706549\n",
            "Epoch 323  \tTraining Loss: 0.20857113514233377\tValidation Loss: 0.21154587205072456\n",
            "Epoch 324  \tTraining Loss: 0.2082572209603876\tValidation Loss: 0.21125267671709053\n",
            "Epoch 325  \tTraining Loss: 0.20795214628975675\tValidation Loss: 0.21096735315777373\n",
            "Epoch 326  \tTraining Loss: 0.20765558528940195\tValidation Loss: 0.2106895852741397\n",
            "Epoch 327  \tTraining Loss: 0.20736717396070006\tValidation Loss: 0.2104190807991803\n",
            "Epoch 328  \tTraining Loss: 0.2070865969005587\tValidation Loss: 0.21015555781570708\n",
            "Epoch 329  \tTraining Loss: 0.2068135360998984\tValidation Loss: 0.20989885186073526\n",
            "Epoch 330  \tTraining Loss: 0.20654766778897735\tValidation Loss: 0.20964867829599154\n",
            "Epoch 331  \tTraining Loss: 0.20628871772119928\tValidation Loss: 0.20940438974165226\n",
            "Epoch 332  \tTraining Loss: 0.20603635829412073\tValidation Loss: 0.20916627726920847\n",
            "Epoch 333  \tTraining Loss: 0.20579038680042874\tValidation Loss: 0.20893383376524655\n",
            "Epoch 334  \tTraining Loss: 0.20555053783617372\tValidation Loss: 0.2087070313701998\n",
            "Epoch 335  \tTraining Loss: 0.2053165203720626\tValidation Loss: 0.20848546413376287\n",
            "Epoch 336  \tTraining Loss: 0.20508813042358184\tValidation Loss: 0.2082689370852141\n",
            "Epoch 337  \tTraining Loss: 0.20486514468663272\tValidation Loss: 0.2080573012868868\n",
            "Epoch 338  \tTraining Loss: 0.20464738888930667\tValidation Loss: 0.20785031245499422\n",
            "Epoch 339  \tTraining Loss: 0.2044346599303532\tValidation Loss: 0.2076477720270024\n",
            "Epoch 340  \tTraining Loss: 0.2042267510949547\tValidation Loss: 0.20744967624539137\n",
            "Epoch 341  \tTraining Loss: 0.20402348388808322\tValidation Loss: 0.2072557638736306\n",
            "Epoch 342  \tTraining Loss: 0.20382466354558917\tValidation Loss: 0.20706570922945092\n",
            "Epoch 343  \tTraining Loss: 0.20363012687720086\tValidation Loss: 0.20687938865189678\n",
            "Epoch 344  \tTraining Loss: 0.2034397816180596\tValidation Loss: 0.20669696406379734\n",
            "Epoch 345  \tTraining Loss: 0.2032533591309524\tValidation Loss: 0.2065181988444592\n",
            "Epoch 346  \tTraining Loss: 0.2030707316186978\tValidation Loss: 0.20634257950336293\n",
            "Epoch 347  \tTraining Loss: 0.20289174844126948\tValidation Loss: 0.20617034994435185\n",
            "Epoch 348  \tTraining Loss: 0.20271622805164555\tValidation Loss: 0.20600144460504377\n",
            "Epoch 349  \tTraining Loss: 0.20254407883922032\tValidation Loss: 0.2058354274205832\n",
            "Epoch 350  \tTraining Loss: 0.20237508099259605\tValidation Loss: 0.20567218742548599\n",
            "Epoch 351  \tTraining Loss: 0.2022091795814626\tValidation Loss: 0.20551191668620356\n",
            "Epoch 352  \tTraining Loss: 0.2020462724801023\tValidation Loss: 0.2053545222797202\n",
            "Epoch 353  \tTraining Loss: 0.201886244858948\tValidation Loss: 0.20519975754314054\n",
            "Epoch 354  \tTraining Loss: 0.20172900861467843\tValidation Loss: 0.20504738180391038\n",
            "Epoch 355  \tTraining Loss: 0.20157442416578217\tValidation Loss: 0.2048975266158714\n",
            "Epoch 356  \tTraining Loss: 0.20142238691613054\tValidation Loss: 0.20475002146270083\n",
            "Epoch 357  \tTraining Loss: 0.20127283791004413\tValidation Loss: 0.204604772366269\n",
            "Epoch 358  \tTraining Loss: 0.20112566347637317\tValidation Loss: 0.20446172613537814\n",
            "Epoch 359  \tTraining Loss: 0.20098079532975197\tValidation Loss: 0.2043208491720846\n",
            "Epoch 360  \tTraining Loss: 0.20083813189002941\tValidation Loss: 0.20418206202291805\n",
            "Epoch 361  \tTraining Loss: 0.20069759728431652\tValidation Loss: 0.20404519705228705\n",
            "Epoch 362  \tTraining Loss: 0.200559116004986\tValidation Loss: 0.20391002863584956\n",
            "Epoch 363  \tTraining Loss: 0.200422619432688\tValidation Loss: 0.20377680757022085\n",
            "Epoch 364  \tTraining Loss: 0.2002880553938371\tValidation Loss: 0.20364532963344031\n",
            "Epoch 365  \tTraining Loss: 0.20015534756457531\tValidation Loss: 0.2035155673553647\n",
            "Epoch 366  \tTraining Loss: 0.2000244316043773\tValidation Loss: 0.2033874446417543\n",
            "Epoch 367  \tTraining Loss: 0.19989523875345022\tValidation Loss: 0.20326103992319805\n",
            "Epoch 368  \tTraining Loss: 0.19976770609728375\tValidation Loss: 0.20313611909957913\n",
            "Epoch 369  \tTraining Loss: 0.19964171479340123\tValidation Loss: 0.20301269013235593\n",
            "Epoch 370  \tTraining Loss: 0.19951725164512615\tValidation Loss: 0.20289072223320556\n",
            "Epoch 371  \tTraining Loss: 0.1993942912105269\tValidation Loss: 0.20277015016087666\n",
            "Epoch 372  \tTraining Loss: 0.19927277985340278\tValidation Loss: 0.20265098437495171\n",
            "Epoch 373  \tTraining Loss: 0.199152685500242\tValidation Loss: 0.2025331416200854\n",
            "Epoch 374  \tTraining Loss: 0.19903395694902992\tValidation Loss: 0.2024166387255176\n",
            "Epoch 375  \tTraining Loss: 0.19891650488585608\tValidation Loss: 0.2023013635138536\n",
            "Epoch 376  \tTraining Loss: 0.19880029812925312\tValidation Loss: 0.20218732502409817\n",
            "Epoch 377  \tTraining Loss: 0.19868530230603207\tValidation Loss: 0.20207436712364865\n",
            "Epoch 378  \tTraining Loss: 0.19857149512371708\tValidation Loss: 0.2019620350617955\n",
            "Epoch 379  \tTraining Loss: 0.19845880697611037\tValidation Loss: 0.2018510434837382\n",
            "Epoch 380  \tTraining Loss: 0.198347220659338\tValidation Loss: 0.20174114875980514\n",
            "Epoch 381  \tTraining Loss: 0.19823674459291962\tValidation Loss: 0.20163236876470722\n",
            "Epoch 382  \tTraining Loss: 0.19812735797951783\tValidation Loss: 0.2015244398727554\n",
            "Epoch 383  \tTraining Loss: 0.19801898732576606\tValidation Loss: 0.2014176276931776\n",
            "Epoch 384  \tTraining Loss: 0.19791161748355654\tValidation Loss: 0.201311941027459\n",
            "Epoch 385  \tTraining Loss: 0.19780519936559307\tValidation Loss: 0.20120717487137338\n",
            "Epoch 386  \tTraining Loss: 0.19769971949737275\tValidation Loss: 0.2011032577592311\n",
            "Epoch 387  \tTraining Loss: 0.1975951259942994\tValidation Loss: 0.2010004982254052\n",
            "Epoch 388  \tTraining Loss: 0.19749140968008716\tValidation Loss: 0.20089839241259497\n",
            "Epoch 389  \tTraining Loss: 0.19738856446295\tValidation Loss: 0.20079730523532544\n",
            "Epoch 390  \tTraining Loss: 0.19728652671329502\tValidation Loss: 0.20069704134010152\n",
            "Epoch 391  \tTraining Loss: 0.1971852902743452\tValidation Loss: 0.20059751849853003\n",
            "Epoch 392  \tTraining Loss: 0.19708486025636754\tValidation Loss: 0.2004986964664126\n",
            "Epoch 393  \tTraining Loss: 0.19698521072076774\tValidation Loss: 0.20040064952953768\n",
            "Epoch 394  \tTraining Loss: 0.1968863190152946\tValidation Loss: 0.2003033699514176\n",
            "Epoch 395  \tTraining Loss: 0.19678814179607673\tValidation Loss: 0.2002067991199133\n",
            "Epoch 396  \tTraining Loss: 0.1966907031614056\tValidation Loss: 0.20011087648354658\n",
            "Epoch 397  \tTraining Loss: 0.19659397258887643\tValidation Loss: 0.20001591732506116\n",
            "Epoch 398  \tTraining Loss: 0.19649792200102106\tValidation Loss: 0.19992154592302774\n",
            "Epoch 399  \tTraining Loss: 0.19640251327804076\tValidation Loss: 0.199827758237317\n",
            "Epoch 400  \tTraining Loss: 0.1963077304055683\tValidation Loss: 0.19973459556253859\n",
            "lr, batch_size: (0.0001, 300)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.7937891531497792\n",
            "Epoch 2  \tTraining Loss: 0.7903017837901265\tValidation Loss: 0.7396026472419468\n",
            "Epoch 3  \tTraining Loss: 0.7359201909958584\tValidation Loss: 0.675473217954124\n",
            "Epoch 4  \tTraining Loss: 0.6712661177190833\tValidation Loss: 0.6108661728883575\n",
            "Epoch 5  \tTraining Loss: 0.606227869917893\tValidation Loss: 0.5558117685758847\n",
            "Epoch 6  \tTraining Loss: 0.551030858571779\tValidation Loss: 0.5187382538776669\n",
            "Epoch 7  \tTraining Loss: 0.5139269378406597\tValidation Loss: 0.4972108725344907\n",
            "Epoch 8  \tTraining Loss: 0.49244450017491054\tValidation Loss: 0.48432523213891027\n",
            "Epoch 9  \tTraining Loss: 0.4797020446015512\tValidation Loss: 0.4773738634482261\n",
            "Epoch 10  \tTraining Loss: 0.47290961912875606\tValidation Loss: 0.4736185060113791\n",
            "Epoch 11  \tTraining Loss: 0.46926021319125616\tValidation Loss: 0.4713233593608902\n",
            "Epoch 12  \tTraining Loss: 0.4670303753036299\tValidation Loss: 0.46970762388687903\n",
            "Epoch 13  \tTraining Loss: 0.46544461208806204\tValidation Loss: 0.4684198117442504\n",
            "Epoch 14  \tTraining Loss: 0.4641654015678427\tValidation Loss: 0.4672966183346017\n",
            "Epoch 15  \tTraining Loss: 0.46303800712571347\tValidation Loss: 0.46626338537364975\n",
            "Epoch 16  \tTraining Loss: 0.4619919229554678\tValidation Loss: 0.4652792205581484\n",
            "Epoch 17  \tTraining Loss: 0.46099243968046144\tValidation Loss: 0.46432451352518544\n",
            "Epoch 18  \tTraining Loss: 0.46002063417970723\tValidation Loss: 0.4633874760150339\n",
            "Epoch 19  \tTraining Loss: 0.459067433278871\tValidation Loss: 0.46246451276390305\n",
            "Epoch 20  \tTraining Loss: 0.458127954812531\tValidation Loss: 0.4615526403955348\n",
            "Epoch 21  \tTraining Loss: 0.45719741263663033\tValidation Loss: 0.4606470049889237\n",
            "Epoch 22  \tTraining Loss: 0.4562721752260736\tValidation Loss: 0.45974690672242563\n",
            "Epoch 23  \tTraining Loss: 0.4553533368327994\tValidation Loss: 0.45885298383840567\n",
            "Epoch 24  \tTraining Loss: 0.45444194138137806\tValidation Loss: 0.4579642564831126\n",
            "Epoch 25  \tTraining Loss: 0.4535355234993162\tValidation Loss: 0.4570780289777831\n",
            "Epoch 26  \tTraining Loss: 0.4526326663956963\tValidation Loss: 0.45619294600287613\n",
            "Epoch 27  \tTraining Loss: 0.45173091406068294\tValidation Loss: 0.4553092818280986\n",
            "Epoch 28  \tTraining Loss: 0.4508295508974375\tValidation Loss: 0.4544281166304428\n",
            "Epoch 29  \tTraining Loss: 0.4499280641045611\tValidation Loss: 0.4535460810894125\n",
            "Epoch 30  \tTraining Loss: 0.4490266050027987\tValidation Loss: 0.4526627523871726\n",
            "Epoch 31  \tTraining Loss: 0.448125527230229\tValidation Loss: 0.45177785089323713\n",
            "Epoch 32  \tTraining Loss: 0.447223367716302\tValidation Loss: 0.4508931367513975\n",
            "Epoch 33  \tTraining Loss: 0.4463209541017042\tValidation Loss: 0.45000689094918694\n",
            "Epoch 34  \tTraining Loss: 0.44541803582112105\tValidation Loss: 0.4491199661315657\n",
            "Epoch 35  \tTraining Loss: 0.4445162676276918\tValidation Loss: 0.44823103305942563\n",
            "Epoch 36  \tTraining Loss: 0.4436145285252662\tValidation Loss: 0.4473405459754245\n",
            "Epoch 37  \tTraining Loss: 0.442712761510956\tValidation Loss: 0.44644912658120905\n",
            "Epoch 38  \tTraining Loss: 0.44181023781340545\tValidation Loss: 0.44555708468575717\n",
            "Epoch 39  \tTraining Loss: 0.4409063285477348\tValidation Loss: 0.4446622860276687\n",
            "Epoch 40  \tTraining Loss: 0.4400009766714702\tValidation Loss: 0.4437637195234951\n",
            "Epoch 41  \tTraining Loss: 0.4390935117968859\tValidation Loss: 0.4428622224727171\n",
            "Epoch 42  \tTraining Loss: 0.43818404115487314\tValidation Loss: 0.44196008741226256\n",
            "Epoch 43  \tTraining Loss: 0.43727251758489205\tValidation Loss: 0.4410549025980905\n",
            "Epoch 44  \tTraining Loss: 0.4363577298550211\tValidation Loss: 0.44014817672464374\n",
            "Epoch 45  \tTraining Loss: 0.4354408927441622\tValidation Loss: 0.4392394765795721\n",
            "Epoch 46  \tTraining Loss: 0.43452271408974724\tValidation Loss: 0.43832942279734805\n",
            "Epoch 47  \tTraining Loss: 0.4336034338246571\tValidation Loss: 0.43741781641838084\n",
            "Epoch 48  \tTraining Loss: 0.4326830516508903\tValidation Loss: 0.4365054167104606\n",
            "Epoch 49  \tTraining Loss: 0.43176187477250294\tValidation Loss: 0.43558941896999226\n",
            "Epoch 50  \tTraining Loss: 0.4308379506528068\tValidation Loss: 0.43467118108778274\n",
            "Epoch 51  \tTraining Loss: 0.42991166888939963\tValidation Loss: 0.4337509241495869\n",
            "Epoch 52  \tTraining Loss: 0.428983081615076\tValidation Loss: 0.432828364517572\n",
            "Epoch 53  \tTraining Loss: 0.4280524888701792\tValidation Loss: 0.4319035793437747\n",
            "Epoch 54  \tTraining Loss: 0.4271203463969871\tValidation Loss: 0.43097663765015837\n",
            "Epoch 55  \tTraining Loss: 0.42618638865120234\tValidation Loss: 0.4300480559391896\n",
            "Epoch 56  \tTraining Loss: 0.4252509474799466\tValidation Loss: 0.4291183386701177\n",
            "Epoch 57  \tTraining Loss: 0.424314598838022\tValidation Loss: 0.42818697978449116\n",
            "Epoch 58  \tTraining Loss: 0.42337771847203826\tValidation Loss: 0.4272541722273017\n",
            "Epoch 59  \tTraining Loss: 0.4224402268997953\tValidation Loss: 0.4263204541273726\n",
            "Epoch 60  \tTraining Loss: 0.4215014614127958\tValidation Loss: 0.4253850197519322\n",
            "Epoch 61  \tTraining Loss: 0.42056166830720654\tValidation Loss: 0.4244479583530851\n",
            "Epoch 62  \tTraining Loss: 0.419621525938314\tValidation Loss: 0.4235088817808493\n",
            "Epoch 63  \tTraining Loss: 0.41868041313554144\tValidation Loss: 0.42256926170653464\n",
            "Epoch 64  \tTraining Loss: 0.4177402197403607\tValidation Loss: 0.4216297366815058\n",
            "Epoch 65  \tTraining Loss: 0.4168000998141144\tValidation Loss: 0.4206892813926245\n",
            "Epoch 66  \tTraining Loss: 0.41585928458384297\tValidation Loss: 0.4197482707435665\n",
            "Epoch 67  \tTraining Loss: 0.41491855036513825\tValidation Loss: 0.41880596073056864\n",
            "Epoch 68  \tTraining Loss: 0.413978577361546\tValidation Loss: 0.41786206194199277\n",
            "Epoch 69  \tTraining Loss: 0.4130400450899515\tValidation Loss: 0.41691933687405114\n",
            "Epoch 70  \tTraining Loss: 0.41210227222878765\tValidation Loss: 0.41597396323368474\n",
            "Epoch 71  \tTraining Loss: 0.41116163754383495\tValidation Loss: 0.41502602601758426\n",
            "Epoch 72  \tTraining Loss: 0.41021941106156845\tValidation Loss: 0.4140780164116849\n",
            "Epoch 73  \tTraining Loss: 0.4092765633564972\tValidation Loss: 0.41312715347493956\n",
            "Epoch 74  \tTraining Loss: 0.4083268230266829\tValidation Loss: 0.4121638279758138\n",
            "Epoch 75  \tTraining Loss: 0.4073679729271208\tValidation Loss: 0.4111831821351704\n",
            "Epoch 76  \tTraining Loss: 0.4063939820249611\tValidation Loss: 0.410175280346891\n",
            "Epoch 77  \tTraining Loss: 0.4053928317175932\tValidation Loss: 0.40911316189709884\n",
            "Epoch 78  \tTraining Loss: 0.4043246329598917\tValidation Loss: 0.40795577453164183\n",
            "Epoch 79  \tTraining Loss: 0.40314475669915995\tValidation Loss: 0.4066988260329525\n",
            "Epoch 80  \tTraining Loss: 0.40183852644338475\tValidation Loss: 0.40528424180852307\n",
            "Epoch 81  \tTraining Loss: 0.40033127150317066\tValidation Loss: 0.4033127470782263\n",
            "Epoch 82  \tTraining Loss: 0.39831051316998156\tValidation Loss: 0.400257926193116\n",
            "Epoch 83  \tTraining Loss: 0.39511364824481876\tValidation Loss: 0.3957003898126781\n",
            "Epoch 84  \tTraining Loss: 0.3902521011241419\tValidation Loss: 0.391954731712515\n",
            "Epoch 85  \tTraining Loss: 0.3863132943699981\tValidation Loss: 0.3899705449331144\n",
            "Epoch 86  \tTraining Loss: 0.38421340357888406\tValidation Loss: 0.3886310300922752\n",
            "Epoch 87  \tTraining Loss: 0.38280890834540315\tValidation Loss: 0.3875141058473806\n",
            "Epoch 88  \tTraining Loss: 0.3816560226855519\tValidation Loss: 0.38647769186579767\n",
            "Epoch 89  \tTraining Loss: 0.3805986273853241\tValidation Loss: 0.38547265041057005\n",
            "Epoch 90  \tTraining Loss: 0.379581585861139\tValidation Loss: 0.3844815371383754\n",
            "Epoch 91  \tTraining Loss: 0.3785846789891427\tValidation Loss: 0.3835000776816758\n",
            "Epoch 92  \tTraining Loss: 0.37760138847171515\tValidation Loss: 0.38252661998758375\n",
            "Epoch 93  \tTraining Loss: 0.37662792022028835\tValidation Loss: 0.3815611341785969\n",
            "Epoch 94  \tTraining Loss: 0.3756629149509768\tValidation Loss: 0.3806037856234009\n",
            "Epoch 95  \tTraining Loss: 0.37470668189762063\tValidation Loss: 0.37965437927582263\n",
            "Epoch 96  \tTraining Loss: 0.37375969891191135\tValidation Loss: 0.37871238041477073\n",
            "Epoch 97  \tTraining Loss: 0.3728214167898011\tValidation Loss: 0.3777793962988748\n",
            "Epoch 98  \tTraining Loss: 0.37189256686400396\tValidation Loss: 0.3768553227785591\n",
            "Epoch 99  \tTraining Loss: 0.37097245620132974\tValidation Loss: 0.37593669274263697\n",
            "Epoch 100  \tTraining Loss: 0.37006037362677535\tValidation Loss: 0.37502619635204026\n",
            "Epoch 101  \tTraining Loss: 0.36915771393949237\tValidation Loss: 0.37412235158384555\n",
            "Epoch 102  \tTraining Loss: 0.36826183633322945\tValidation Loss: 0.37321941513811685\n",
            "Epoch 103  \tTraining Loss: 0.3673705113775454\tValidation Loss: 0.37232243423159084\n",
            "Epoch 104  \tTraining Loss: 0.3664864819199288\tValidation Loss: 0.37143004382535\n",
            "Epoch 105  \tTraining Loss: 0.3656073743154725\tValidation Loss: 0.37053228441930003\n",
            "Epoch 106  \tTraining Loss: 0.3647268672804672\tValidation Loss: 0.3696193174665022\n",
            "Epoch 107  \tTraining Loss: 0.36382633935339925\tValidation Loss: 0.36865453182117175\n",
            "Epoch 108  \tTraining Loss: 0.362861295500654\tValidation Loss: 0.3675364225314318\n",
            "Epoch 109  \tTraining Loss: 0.3617259905744314\tValidation Loss: 0.36598517033470696\n",
            "Epoch 110  \tTraining Loss: 0.3601286759791841\tValidation Loss: 0.3623225157604795\n",
            "Epoch 111  \tTraining Loss: 0.3563816414851839\tValidation Loss: 0.35400896276354543\n",
            "Epoch 112  \tTraining Loss: 0.34808402380264825\tValidation Loss: 0.3463954204948993\n",
            "Epoch 113  \tTraining Loss: 0.34043844735474493\tValidation Loss: 0.3431059831653356\n",
            "Epoch 114  \tTraining Loss: 0.33710340190212\tValidation Loss: 0.3414708530417665\n",
            "Epoch 115  \tTraining Loss: 0.33544001242899396\tValidation Loss: 0.340379028295548\n",
            "Epoch 116  \tTraining Loss: 0.3343320593182114\tValidation Loss: 0.3394673950873265\n",
            "Epoch 117  \tTraining Loss: 0.33341298084546617\tValidation Loss: 0.33862004123847683\n",
            "Epoch 118  \tTraining Loss: 0.33256359514592476\tValidation Loss: 0.33779998316860216\n",
            "Epoch 119  \tTraining Loss: 0.3317443711471905\tValidation Loss: 0.33699526951877706\n",
            "Epoch 120  \tTraining Loss: 0.33094274819252034\tValidation Loss: 0.33620224684133\n",
            "Epoch 121  \tTraining Loss: 0.33015418403850527\tValidation Loss: 0.33542050459792816\n",
            "Epoch 122  \tTraining Loss: 0.32937739430135404\tValidation Loss: 0.3346492088912953\n",
            "Epoch 123  \tTraining Loss: 0.32861181473718715\tValidation Loss: 0.33388837548704586\n",
            "Epoch 124  \tTraining Loss: 0.3278574258735916\tValidation Loss: 0.33313813163304345\n",
            "Epoch 125  \tTraining Loss: 0.3271139495743487\tValidation Loss: 0.33239908597681944\n",
            "Epoch 126  \tTraining Loss: 0.3263815752237541\tValidation Loss: 0.3316708038521047\n",
            "Epoch 127  \tTraining Loss: 0.3256601304957933\tValidation Loss: 0.3309531768453015\n",
            "Epoch 128  \tTraining Loss: 0.32494961273642664\tValidation Loss: 0.33024597792306065\n",
            "Epoch 129  \tTraining Loss: 0.32424995932294637\tValidation Loss: 0.3295485187354402\n",
            "Epoch 130  \tTraining Loss: 0.3235605244810727\tValidation Loss: 0.32886079677675617\n",
            "Epoch 131  \tTraining Loss: 0.3228812780143397\tValidation Loss: 0.3281829751392484\n",
            "Epoch 132  \tTraining Loss: 0.3222122356663404\tValidation Loss: 0.32751477524736167\n",
            "Epoch 133  \tTraining Loss: 0.3215530610881069\tValidation Loss: 0.3268560963983732\n",
            "Epoch 134  \tTraining Loss: 0.3209039021503009\tValidation Loss: 0.3262069361549692\n",
            "Epoch 135  \tTraining Loss: 0.3202642222331164\tValidation Loss: 0.3255674968587932\n",
            "Epoch 136  \tTraining Loss: 0.3196340453913359\tValidation Loss: 0.3249372978050929\n",
            "Epoch 137  \tTraining Loss: 0.31901324543471943\tValidation Loss: 0.32431586217390834\n",
            "Epoch 138  \tTraining Loss: 0.31840184695647233\tValidation Loss: 0.32370331061826363\n",
            "Epoch 139  \tTraining Loss: 0.3177995915334371\tValidation Loss: 0.32310027289091103\n",
            "Epoch 140  \tTraining Loss: 0.31720641342431233\tValidation Loss: 0.32250590628007875\n",
            "Epoch 141  \tTraining Loss: 0.3166219579076796\tValidation Loss: 0.32192012244198653\n",
            "Epoch 142  \tTraining Loss: 0.31604645171764384\tValidation Loss: 0.32134301869923565\n",
            "Epoch 143  \tTraining Loss: 0.3154796291990588\tValidation Loss: 0.3207744005954364\n",
            "Epoch 144  \tTraining Loss: 0.3149213442736238\tValidation Loss: 0.32021388493773756\n",
            "Epoch 145  \tTraining Loss: 0.31437141338284813\tValidation Loss: 0.3196617614483723\n",
            "Epoch 146  \tTraining Loss: 0.3138299827496432\tValidation Loss: 0.3191176703095907\n",
            "Epoch 147  \tTraining Loss: 0.31329645621691965\tValidation Loss: 0.31858133752763546\n",
            "Epoch 148  \tTraining Loss: 0.3127706292283547\tValidation Loss: 0.3180528176885997\n",
            "Epoch 149  \tTraining Loss: 0.3122526305009191\tValidation Loss: 0.3175318201326348\n",
            "Epoch 150  \tTraining Loss: 0.31174202744535734\tValidation Loss: 0.3170181424074684\n",
            "Epoch 151  \tTraining Loss: 0.31123885791867095\tValidation Loss: 0.31651177000220193\n",
            "Epoch 152  \tTraining Loss: 0.3107429688992671\tValidation Loss: 0.31601236148238426\n",
            "Epoch 153  \tTraining Loss: 0.31025438306970243\tValidation Loss: 0.3155196261628218\n",
            "Epoch 154  \tTraining Loss: 0.30977298544209964\tValidation Loss: 0.3150339730326214\n",
            "Epoch 155  \tTraining Loss: 0.3092986174664294\tValidation Loss: 0.31455521431246625\n",
            "Epoch 156  \tTraining Loss: 0.30883089934568775\tValidation Loss: 0.31408347475571813\n",
            "Epoch 157  \tTraining Loss: 0.30836993465785467\tValidation Loss: 0.3136182846987536\n",
            "Epoch 158  \tTraining Loss: 0.30791558127071333\tValidation Loss: 0.3131599836567367\n",
            "Epoch 159  \tTraining Loss: 0.3074677825867185\tValidation Loss: 0.3127082795439167\n",
            "Epoch 160  \tTraining Loss: 0.3070261044232248\tValidation Loss: 0.3122627418218909\n",
            "Epoch 161  \tTraining Loss: 0.3065905182845531\tValidation Loss: 0.31182313336908646\n",
            "Epoch 162  \tTraining Loss: 0.3061610470447732\tValidation Loss: 0.31138883825047237\n",
            "Epoch 163  \tTraining Loss: 0.3057369731480374\tValidation Loss: 0.31096036593733134\n",
            "Epoch 164  \tTraining Loss: 0.3053184531841\tValidation Loss: 0.31053683123016057\n",
            "Epoch 165  \tTraining Loss: 0.3049052128226577\tValidation Loss: 0.3101188330341531\n",
            "Epoch 166  \tTraining Loss: 0.3044973466744261\tValidation Loss: 0.30970624921007867\n",
            "Epoch 167  \tTraining Loss: 0.3040946847336912\tValidation Loss: 0.30929878153464185\n",
            "Epoch 168  \tTraining Loss: 0.30369663503731853\tValidation Loss: 0.3088957046102381\n",
            "Epoch 169  \tTraining Loss: 0.3033033526597804\tValidation Loss: 0.3084966605823968\n",
            "Epoch 170  \tTraining Loss: 0.30291483523060514\tValidation Loss: 0.30810225814732606\n",
            "Epoch 171  \tTraining Loss: 0.3025313901152937\tValidation Loss: 0.30771255084649085\n",
            "Epoch 172  \tTraining Loss: 0.3021524267735312\tValidation Loss: 0.30732771493884814\n",
            "Epoch 173  \tTraining Loss: 0.30177802131951537\tValidation Loss: 0.3069469827408685\n",
            "Epoch 174  \tTraining Loss: 0.30140753408986914\tValidation Loss: 0.3065707169511816\n",
            "Epoch 175  \tTraining Loss: 0.30104117147415377\tValidation Loss: 0.3061987895158401\n",
            "Epoch 176  \tTraining Loss: 0.3006785621997629\tValidation Loss: 0.30583114493903757\n",
            "Epoch 177  \tTraining Loss: 0.3003196629689193\tValidation Loss: 0.30546705930730295\n",
            "Epoch 178  \tTraining Loss: 0.2999635419936533\tValidation Loss: 0.3051046583306923\n",
            "Epoch 179  \tTraining Loss: 0.29961127322164727\tValidation Loss: 0.3047437464511171\n",
            "Epoch 180  \tTraining Loss: 0.29926137911989126\tValidation Loss: 0.304384713667403\n",
            "Epoch 181  \tTraining Loss: 0.2989128522975289\tValidation Loss: 0.3040253214282598\n",
            "Epoch 182  \tTraining Loss: 0.2985627816340139\tValidation Loss: 0.3036597170405859\n",
            "Epoch 183  \tTraining Loss: 0.29820389769326633\tValidation Loss: 0.3032892373471054\n",
            "Epoch 184  \tTraining Loss: 0.2978376578782676\tValidation Loss: 0.30290224312816816\n",
            "Epoch 185  \tTraining Loss: 0.29745247325957097\tValidation Loss: 0.30247418239203533\n",
            "Epoch 186  \tTraining Loss: 0.297026784567398\tValidation Loss: 0.30196910431825985\n",
            "Epoch 187  \tTraining Loss: 0.2965477723133139\tValidation Loss: 0.3013420497661846\n",
            "Epoch 188  \tTraining Loss: 0.2959794517872663\tValidation Loss: 0.30031168810243236\n",
            "Epoch 189  \tTraining Loss: 0.29512458892903914\tValidation Loss: 0.29794741252686535\n",
            "Epoch 190  \tTraining Loss: 0.2930947193476868\tValidation Loss: 0.2903655503375099\n",
            "Epoch 191  \tTraining Loss: 0.2857075063073514\tValidation Loss: 0.27499360013291374\n",
            "Epoch 192  \tTraining Loss: 0.2700004764750307\tValidation Loss: 0.2678783059340254\n",
            "Epoch 193  \tTraining Loss: 0.2627827819684379\tValidation Loss: 0.26594466387867055\n",
            "Epoch 194  \tTraining Loss: 0.2608043939273886\tValidation Loss: 0.26520792318403585\n",
            "Epoch 195  \tTraining Loss: 0.26005143821839993\tValidation Loss: 0.2647483536086536\n",
            "Epoch 196  \tTraining Loss: 0.259588624888154\tValidation Loss: 0.26435605396018974\n",
            "Epoch 197  \tTraining Loss: 0.2591988916911927\tValidation Loss: 0.26398322373623084\n",
            "Epoch 198  \tTraining Loss: 0.25883116965781516\tValidation Loss: 0.26361975104866586\n",
            "Epoch 199  \tTraining Loss: 0.2584735666258112\tValidation Loss: 0.263262880598646\n",
            "Epoch 200  \tTraining Loss: 0.25812287317101584\tValidation Loss: 0.262911966144481\n",
            "Epoch 201  \tTraining Loss: 0.25777819185068346\tValidation Loss: 0.2625670283165287\n",
            "Epoch 202  \tTraining Loss: 0.25743922241934813\tValidation Loss: 0.26222758032916504\n",
            "Epoch 203  \tTraining Loss: 0.25710572867557535\tValidation Loss: 0.2618938432603724\n",
            "Epoch 204  \tTraining Loss: 0.25677746466988954\tValidation Loss: 0.26156531050737974\n",
            "Epoch 205  \tTraining Loss: 0.25645422018305775\tValidation Loss: 0.2612419235256832\n",
            "Epoch 206  \tTraining Loss: 0.2561359027134254\tValidation Loss: 0.2609236061996937\n",
            "Epoch 207  \tTraining Loss: 0.2558224114898485\tValidation Loss: 0.2606102204444244\n",
            "Epoch 208  \tTraining Loss: 0.2555135552055597\tValidation Loss: 0.26030154772026987\n",
            "Epoch 209  \tTraining Loss: 0.2552090921919164\tValidation Loss: 0.2599971507334856\n",
            "Epoch 210  \tTraining Loss: 0.25490899039230336\tValidation Loss: 0.2596972989291461\n",
            "Epoch 211  \tTraining Loss: 0.25461323537867425\tValidation Loss: 0.2594020499199736\n",
            "Epoch 212  \tTraining Loss: 0.2543216207303062\tValidation Loss: 0.2591111106229058\n",
            "Epoch 213  \tTraining Loss: 0.2540341506392702\tValidation Loss: 0.25882439359413084\n",
            "Epoch 214  \tTraining Loss: 0.2537506605388366\tValidation Loss: 0.25854174264693214\n",
            "Epoch 215  \tTraining Loss: 0.25347100160131036\tValidation Loss: 0.2582631106284256\n",
            "Epoch 216  \tTraining Loss: 0.25319512753505063\tValidation Loss: 0.2579882318617196\n",
            "Epoch 217  \tTraining Loss: 0.25292292740621514\tValidation Loss: 0.2577170279508818\n",
            "Epoch 218  \tTraining Loss: 0.2526543181417047\tValidation Loss: 0.2574494160226696\n",
            "Epoch 219  \tTraining Loss: 0.252389177561973\tValidation Loss: 0.257185343059832\n",
            "Epoch 220  \tTraining Loss: 0.2521274161834666\tValidation Loss: 0.2569247315705698\n",
            "Epoch 221  \tTraining Loss: 0.25186897409037234\tValidation Loss: 0.2566674614626934\n",
            "Epoch 222  \tTraining Loss: 0.25161375961710675\tValidation Loss: 0.2564132345816694\n",
            "Epoch 223  \tTraining Loss: 0.2513616781119867\tValidation Loss: 0.2561620622421803\n",
            "Epoch 224  \tTraining Loss: 0.25111257827961314\tValidation Loss: 0.2559136683248965\n",
            "Epoch 225  \tTraining Loss: 0.25086635190708295\tValidation Loss: 0.2556682953301308\n",
            "Epoch 226  \tTraining Loss: 0.250622951022442\tValidation Loss: 0.25542609643681957\n",
            "Epoch 227  \tTraining Loss: 0.25038233352059663\tValidation Loss: 0.25518691764000334\n",
            "Epoch 228  \tTraining Loss: 0.2501445182205218\tValidation Loss: 0.25495069185954516\n",
            "Epoch 229  \tTraining Loss: 0.24990945344288684\tValidation Loss: 0.25471723838719645\n",
            "Epoch 230  \tTraining Loss: 0.24967707339342954\tValidation Loss: 0.254486439613132\n",
            "Epoch 231  \tTraining Loss: 0.24944734191645881\tValidation Loss: 0.2542584343325857\n",
            "Epoch 232  \tTraining Loss: 0.2492201835162948\tValidation Loss: 0.2540330844294091\n",
            "Epoch 233  \tTraining Loss: 0.24899555727521686\tValidation Loss: 0.25381027168818854\n",
            "Epoch 234  \tTraining Loss: 0.24877336807751516\tValidation Loss: 0.2535898703916114\n",
            "Epoch 235  \tTraining Loss: 0.2485535570598786\tValidation Loss: 0.25337202998409497\n",
            "Epoch 236  \tTraining Loss: 0.24833610836697553\tValidation Loss: 0.2531566216954869\n",
            "Epoch 237  \tTraining Loss: 0.24812099837684598\tValidation Loss: 0.2529435207393502\n",
            "Epoch 238  \tTraining Loss: 0.24790818792973934\tValidation Loss: 0.25273274805711543\n",
            "Epoch 239  \tTraining Loss: 0.24769756629475445\tValidation Loss: 0.2525242240920224\n",
            "Epoch 240  \tTraining Loss: 0.2474890540177743\tValidation Loss: 0.252317463814946\n",
            "Epoch 241  \tTraining Loss: 0.24728256080700609\tValidation Loss: 0.2521126251215407\n",
            "Epoch 242  \tTraining Loss: 0.247078093462745\tValidation Loss: 0.25190980895755455\n",
            "Epoch 243  \tTraining Loss: 0.24687560275384102\tValidation Loss: 0.2517088894240016\n",
            "Epoch 244  \tTraining Loss: 0.24667507902095925\tValidation Loss: 0.251510007700397\n",
            "Epoch 245  \tTraining Loss: 0.24647652225275662\tValidation Loss: 0.25131284475004056\n",
            "Epoch 246  \tTraining Loss: 0.24627991142028383\tValidation Loss: 0.25111769831348574\n",
            "Epoch 247  \tTraining Loss: 0.2460852595695485\tValidation Loss: 0.2509246985834862\n",
            "Epoch 248  \tTraining Loss: 0.24589246622668748\tValidation Loss: 0.25073361944076067\n",
            "Epoch 249  \tTraining Loss: 0.24570151478958496\tValidation Loss: 0.25054417906902265\n",
            "Epoch 250  \tTraining Loss: 0.24551234950175788\tValidation Loss: 0.25035671075792215\n",
            "Epoch 251  \tTraining Loss: 0.24532495470581606\tValidation Loss: 0.2501710016371513\n",
            "Epoch 252  \tTraining Loss: 0.24513927869417224\tValidation Loss: 0.24998690210579358\n",
            "Epoch 253  \tTraining Loss: 0.24495530401626914\tValidation Loss: 0.2498044652640512\n",
            "Epoch 254  \tTraining Loss: 0.2447730381608143\tValidation Loss: 0.24962381378385134\n",
            "Epoch 255  \tTraining Loss: 0.24459246165867918\tValidation Loss: 0.24944480703540833\n",
            "Epoch 256  \tTraining Loss: 0.2444134891000501\tValidation Loss: 0.24926746650105147\n",
            "Epoch 257  \tTraining Loss: 0.244236117078939\tValidation Loss: 0.24909176430771346\n",
            "Epoch 258  \tTraining Loss: 0.24406033279427053\tValidation Loss: 0.24891770593524992\n",
            "Epoch 259  \tTraining Loss: 0.2438860942888393\tValidation Loss: 0.24874523055407335\n",
            "Epoch 260  \tTraining Loss: 0.24371339523281757\tValidation Loss: 0.2485743420568226\n",
            "Epoch 261  \tTraining Loss: 0.24354225631243756\tValidation Loss: 0.24840495430379209\n",
            "Epoch 262  \tTraining Loss: 0.2433726788705292\tValidation Loss: 0.24823699032686422\n",
            "Epoch 263  \tTraining Loss: 0.24320460699878382\tValidation Loss: 0.24807048915265675\n",
            "Epoch 264  \tTraining Loss: 0.24303794122133568\tValidation Loss: 0.24790551202297526\n",
            "Epoch 265  \tTraining Loss: 0.242872615797319\tValidation Loss: 0.2477416022133903\n",
            "Epoch 266  \tTraining Loss: 0.24270854913709536\tValidation Loss: 0.2475791795602484\n",
            "Epoch 267  \tTraining Loss: 0.24254585232646195\tValidation Loss: 0.24741823334870353\n",
            "Epoch 268  \tTraining Loss: 0.2423844159060759\tValidation Loss: 0.24725860860150797\n",
            "Epoch 269  \tTraining Loss: 0.2422242713746643\tValidation Loss: 0.24710030580199627\n",
            "Epoch 270  \tTraining Loss: 0.24206542772452955\tValidation Loss: 0.24694303835339682\n",
            "Epoch 271  \tTraining Loss: 0.24190783262058257\tValidation Loss: 0.24678713601845995\n",
            "Epoch 272  \tTraining Loss: 0.24175150308485088\tValidation Loss: 0.24663239180166457\n",
            "Epoch 273  \tTraining Loss: 0.24159622098219538\tValidation Loss: 0.24647879428010896\n",
            "Epoch 274  \tTraining Loss: 0.2414420788202831\tValidation Loss: 0.2463263864142121\n",
            "Epoch 275  \tTraining Loss: 0.24128916776957676\tValidation Loss: 0.24617527040975923\n",
            "Epoch 276  \tTraining Loss: 0.24113747052225884\tValidation Loss: 0.24602524493132308\n",
            "Epoch 277  \tTraining Loss: 0.2409869329697373\tValidation Loss: 0.24587630350545342\n",
            "Epoch 278  \tTraining Loss: 0.24083760717861308\tValidation Loss: 0.24572844605901156\n",
            "Epoch 279  \tTraining Loss: 0.24068935292276392\tValidation Loss: 0.24558074193602977\n",
            "Epoch 280  \tTraining Loss: 0.24054190022581856\tValidation Loss: 0.24543324816969825\n",
            "Epoch 281  \tTraining Loss: 0.24039560972667856\tValidation Loss: 0.24528624209454936\n",
            "Epoch 282  \tTraining Loss: 0.2402504633096237\tValidation Loss: 0.24514020545829412\n",
            "Epoch 283  \tTraining Loss: 0.24010642984274008\tValidation Loss: 0.24499529033961176\n",
            "Epoch 284  \tTraining Loss: 0.23996339848682807\tValidation Loss: 0.2448507862973152\n",
            "Epoch 285  \tTraining Loss: 0.239821477630449\tValidation Loss: 0.24470742398884823\n",
            "Epoch 286  \tTraining Loss: 0.23968058208165507\tValidation Loss: 0.24456440985908512\n",
            "Epoch 287  \tTraining Loss: 0.239540284046024\tValidation Loss: 0.24442186149306613\n",
            "Epoch 288  \tTraining Loss: 0.23940079457013141\tValidation Loss: 0.2442801710951138\n",
            "Epoch 289  \tTraining Loss: 0.23926203769545779\tValidation Loss: 0.24413784213072023\n",
            "Epoch 290  \tTraining Loss: 0.23912323578080055\tValidation Loss: 0.2439954332963303\n",
            "Epoch 291  \tTraining Loss: 0.23898415414098453\tValidation Loss: 0.2438510329295777\n",
            "Epoch 292  \tTraining Loss: 0.2388435510810129\tValidation Loss: 0.24370154890191356\n",
            "Epoch 293  \tTraining Loss: 0.2387017320284666\tValidation Loss: 0.24354636810043767\n",
            "Epoch 294  \tTraining Loss: 0.23855588036980357\tValidation Loss: 0.24338331129288335\n",
            "Epoch 295  \tTraining Loss: 0.23840558702879777\tValidation Loss: 0.2432098149989956\n",
            "Epoch 296  \tTraining Loss: 0.23824738134798903\tValidation Loss: 0.24303150683763863\n",
            "Epoch 297  \tTraining Loss: 0.23807944800293082\tValidation Loss: 0.2428401665785304\n",
            "Epoch 298  \tTraining Loss: 0.23789480040450264\tValidation Loss: 0.24262279702839765\n",
            "Epoch 299  \tTraining Loss: 0.23768676557428456\tValidation Loss: 0.24236776955363837\n",
            "Epoch 300  \tTraining Loss: 0.2374441165977077\tValidation Loss: 0.24199911706769425\n",
            "Epoch 301  \tTraining Loss: 0.2371190360364851\tValidation Loss: 0.2414503203615154\n",
            "Epoch 302  \tTraining Loss: 0.23664635990797192\tValidation Loss: 0.240504087625262\n",
            "Epoch 303  \tTraining Loss: 0.23576932650994994\tValidation Loss: 0.23860790828836467\n",
            "Epoch 304  \tTraining Loss: 0.2339505231306391\tValidation Loss: 0.23562689985046156\n",
            "Epoch 305  \tTraining Loss: 0.23116349034681946\tValidation Loss: 0.23346759923443738\n",
            "Epoch 306  \tTraining Loss: 0.22918892519497983\tValidation Loss: 0.23202165285073756\n",
            "Epoch 307  \tTraining Loss: 0.22787025605719913\tValidation Loss: 0.22798800970002345\n",
            "Epoch 308  \tTraining Loss: 0.22406028835326122\tValidation Loss: 0.2195911544157873\n",
            "Epoch 309  \tTraining Loss: 0.21620636008387717\tValidation Loss: 0.21713669782950082\n",
            "Epoch 310  \tTraining Loss: 0.21397756602153664\tValidation Loss: 0.21634693061498922\n",
            "Epoch 311  \tTraining Loss: 0.2132478278776737\tValidation Loss: 0.2158441025143227\n",
            "Epoch 312  \tTraining Loss: 0.21274973883866777\tValidation Loss: 0.21540716880870467\n",
            "Epoch 313  \tTraining Loss: 0.21229978314888018\tValidation Loss: 0.21499604579210388\n",
            "Epoch 314  \tTraining Loss: 0.21186985340619077\tValidation Loss: 0.21460062100766555\n",
            "Epoch 315  \tTraining Loss: 0.2114548605089315\tValidation Loss: 0.2142184313974726\n",
            "Epoch 316  \tTraining Loss: 0.21105351571751635\tValidation Loss: 0.21384837432336584\n",
            "Epoch 317  \tTraining Loss: 0.2106649969213102\tValidation Loss: 0.21348956790000603\n",
            "Epoch 318  \tTraining Loss: 0.21028867687365096\tValidation Loss: 0.21314172695641756\n",
            "Epoch 319  \tTraining Loss: 0.20992395760168828\tValidation Loss: 0.21280376770224016\n",
            "Epoch 320  \tTraining Loss: 0.20957031887275399\tValidation Loss: 0.21247577921448285\n",
            "Epoch 321  \tTraining Loss: 0.20922728573673416\tValidation Loss: 0.21215707306720166\n",
            "Epoch 322  \tTraining Loss: 0.20889437270711247\tValidation Loss: 0.211847141706549\n",
            "Epoch 323  \tTraining Loss: 0.20857113514233377\tValidation Loss: 0.21154587205072456\n",
            "Epoch 324  \tTraining Loss: 0.2082572209603876\tValidation Loss: 0.21125267671709053\n",
            "Epoch 325  \tTraining Loss: 0.20795214628975675\tValidation Loss: 0.21096735315777373\n",
            "Epoch 326  \tTraining Loss: 0.20765558528940195\tValidation Loss: 0.2106895852741397\n",
            "Epoch 327  \tTraining Loss: 0.20736717396070006\tValidation Loss: 0.2104190807991803\n",
            "Epoch 328  \tTraining Loss: 0.2070865969005587\tValidation Loss: 0.21015555781570708\n",
            "Epoch 329  \tTraining Loss: 0.2068135360998984\tValidation Loss: 0.20989885186073526\n",
            "Epoch 330  \tTraining Loss: 0.20654766778897735\tValidation Loss: 0.20964867829599154\n",
            "Epoch 331  \tTraining Loss: 0.20628871772119928\tValidation Loss: 0.20940438974165226\n",
            "Epoch 332  \tTraining Loss: 0.20603635829412073\tValidation Loss: 0.20916627726920847\n",
            "Epoch 333  \tTraining Loss: 0.20579038680042874\tValidation Loss: 0.20893383376524655\n",
            "Epoch 334  \tTraining Loss: 0.20555053783617372\tValidation Loss: 0.2087070313701998\n",
            "Epoch 335  \tTraining Loss: 0.2053165203720626\tValidation Loss: 0.20848546413376287\n",
            "Epoch 336  \tTraining Loss: 0.20508813042358184\tValidation Loss: 0.2082689370852141\n",
            "Epoch 337  \tTraining Loss: 0.20486514468663272\tValidation Loss: 0.2080573012868868\n",
            "Epoch 338  \tTraining Loss: 0.20464738888930667\tValidation Loss: 0.20785031245499422\n",
            "Epoch 339  \tTraining Loss: 0.2044346599303532\tValidation Loss: 0.2076477720270024\n",
            "Epoch 340  \tTraining Loss: 0.2042267510949547\tValidation Loss: 0.20744967624539137\n",
            "Epoch 341  \tTraining Loss: 0.20402348388808322\tValidation Loss: 0.2072557638736306\n",
            "Epoch 342  \tTraining Loss: 0.20382466354558917\tValidation Loss: 0.20706570922945092\n",
            "Epoch 343  \tTraining Loss: 0.20363012687720086\tValidation Loss: 0.20687938865189678\n",
            "Epoch 344  \tTraining Loss: 0.2034397816180596\tValidation Loss: 0.20669696406379734\n",
            "Epoch 345  \tTraining Loss: 0.2032533591309524\tValidation Loss: 0.2065181988444592\n",
            "Epoch 346  \tTraining Loss: 0.2030707316186978\tValidation Loss: 0.20634257950336293\n",
            "Epoch 347  \tTraining Loss: 0.20289174844126948\tValidation Loss: 0.20617034994435185\n",
            "Epoch 348  \tTraining Loss: 0.20271622805164555\tValidation Loss: 0.20600144460504377\n",
            "Epoch 349  \tTraining Loss: 0.20254407883922032\tValidation Loss: 0.2058354274205832\n",
            "Epoch 350  \tTraining Loss: 0.20237508099259605\tValidation Loss: 0.20567218742548599\n",
            "Epoch 351  \tTraining Loss: 0.2022091795814626\tValidation Loss: 0.20551191668620356\n",
            "Epoch 352  \tTraining Loss: 0.2020462724801023\tValidation Loss: 0.2053545222797202\n",
            "Epoch 353  \tTraining Loss: 0.201886244858948\tValidation Loss: 0.20519975754314054\n",
            "Epoch 354  \tTraining Loss: 0.20172900861467843\tValidation Loss: 0.20504738180391038\n",
            "Epoch 355  \tTraining Loss: 0.20157442416578217\tValidation Loss: 0.2048975266158714\n",
            "Epoch 356  \tTraining Loss: 0.20142238691613054\tValidation Loss: 0.20475002146270083\n",
            "Epoch 357  \tTraining Loss: 0.20127283791004413\tValidation Loss: 0.204604772366269\n",
            "Epoch 358  \tTraining Loss: 0.20112566347637317\tValidation Loss: 0.20446172613537814\n",
            "Epoch 359  \tTraining Loss: 0.20098079532975197\tValidation Loss: 0.2043208491720846\n",
            "Epoch 360  \tTraining Loss: 0.20083813189002941\tValidation Loss: 0.20418206202291805\n",
            "Epoch 361  \tTraining Loss: 0.20069759728431652\tValidation Loss: 0.20404519705228705\n",
            "Epoch 362  \tTraining Loss: 0.200559116004986\tValidation Loss: 0.20391002863584956\n",
            "Epoch 363  \tTraining Loss: 0.200422619432688\tValidation Loss: 0.20377680757022085\n",
            "Epoch 364  \tTraining Loss: 0.2002880553938371\tValidation Loss: 0.20364532963344031\n",
            "Epoch 365  \tTraining Loss: 0.20015534756457531\tValidation Loss: 0.2035155673553647\n",
            "Epoch 366  \tTraining Loss: 0.2000244316043773\tValidation Loss: 0.2033874446417543\n",
            "Epoch 367  \tTraining Loss: 0.19989523875345022\tValidation Loss: 0.20326103992319805\n",
            "Epoch 368  \tTraining Loss: 0.19976770609728375\tValidation Loss: 0.20313611909957913\n",
            "Epoch 369  \tTraining Loss: 0.19964171479340123\tValidation Loss: 0.20301269013235593\n",
            "Epoch 370  \tTraining Loss: 0.19951725164512615\tValidation Loss: 0.20289072223320556\n",
            "Epoch 371  \tTraining Loss: 0.1993942912105269\tValidation Loss: 0.20277015016087666\n",
            "Epoch 372  \tTraining Loss: 0.19927277985340278\tValidation Loss: 0.20265098437495171\n",
            "Epoch 373  \tTraining Loss: 0.199152685500242\tValidation Loss: 0.2025331416200854\n",
            "Epoch 374  \tTraining Loss: 0.19903395694902992\tValidation Loss: 0.2024166387255176\n",
            "Epoch 375  \tTraining Loss: 0.19891650488585608\tValidation Loss: 0.2023013635138536\n",
            "Epoch 376  \tTraining Loss: 0.19880029812925312\tValidation Loss: 0.20218732502409817\n",
            "Epoch 377  \tTraining Loss: 0.19868530230603207\tValidation Loss: 0.20207436712364865\n",
            "Epoch 378  \tTraining Loss: 0.19857149512371708\tValidation Loss: 0.2019620350617955\n",
            "Epoch 379  \tTraining Loss: 0.19845880697611037\tValidation Loss: 0.2018510434837382\n",
            "Epoch 380  \tTraining Loss: 0.198347220659338\tValidation Loss: 0.20174114875980514\n",
            "Epoch 381  \tTraining Loss: 0.19823674459291962\tValidation Loss: 0.20163236876470722\n",
            "Epoch 382  \tTraining Loss: 0.19812735797951783\tValidation Loss: 0.2015244398727554\n",
            "Epoch 383  \tTraining Loss: 0.19801898732576606\tValidation Loss: 0.2014176276931776\n",
            "Epoch 384  \tTraining Loss: 0.19791161748355654\tValidation Loss: 0.201311941027459\n",
            "Epoch 385  \tTraining Loss: 0.19780519936559307\tValidation Loss: 0.20120717487137338\n",
            "Epoch 386  \tTraining Loss: 0.19769971949737275\tValidation Loss: 0.2011032577592311\n",
            "Epoch 387  \tTraining Loss: 0.1975951259942994\tValidation Loss: 0.2010004982254052\n",
            "Epoch 388  \tTraining Loss: 0.19749140968008716\tValidation Loss: 0.20089839241259497\n",
            "Epoch 389  \tTraining Loss: 0.19738856446295\tValidation Loss: 0.20079730523532544\n",
            "Epoch 390  \tTraining Loss: 0.19728652671329502\tValidation Loss: 0.20069704134010152\n",
            "Epoch 391  \tTraining Loss: 0.1971852902743452\tValidation Loss: 0.20059751849853003\n",
            "Epoch 392  \tTraining Loss: 0.19708486025636754\tValidation Loss: 0.2004986964664126\n",
            "Epoch 393  \tTraining Loss: 0.19698521072076774\tValidation Loss: 0.20040064952953768\n",
            "Epoch 394  \tTraining Loss: 0.1968863190152946\tValidation Loss: 0.2003033699514176\n",
            "Epoch 395  \tTraining Loss: 0.19678814179607673\tValidation Loss: 0.2002067991199133\n",
            "Epoch 396  \tTraining Loss: 0.1966907031614056\tValidation Loss: 0.20011087648354658\n",
            "Epoch 397  \tTraining Loss: 0.19659397258887643\tValidation Loss: 0.20001591732506116\n",
            "Epoch 398  \tTraining Loss: 0.19649792200102106\tValidation Loss: 0.19992154592302774\n",
            "Epoch 399  \tTraining Loss: 0.19640251327804076\tValidation Loss: 0.199827758237317\n",
            "Epoch 400  \tTraining Loss: 0.1963077304055683\tValidation Loss: 0.19973459556253859\n",
            "lr, batch_size: (0.0001, 400)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.7937891531497792\n",
            "Epoch 2  \tTraining Loss: 0.7903017837901265\tValidation Loss: 0.7396026472419468\n",
            "Epoch 3  \tTraining Loss: 0.7359201909958584\tValidation Loss: 0.675473217954124\n",
            "Epoch 4  \tTraining Loss: 0.6712661177190833\tValidation Loss: 0.6108661728883575\n",
            "Epoch 5  \tTraining Loss: 0.606227869917893\tValidation Loss: 0.5558117685758847\n",
            "Epoch 6  \tTraining Loss: 0.551030858571779\tValidation Loss: 0.5187382538776669\n",
            "Epoch 7  \tTraining Loss: 0.5139269378406597\tValidation Loss: 0.4972108725344907\n",
            "Epoch 8  \tTraining Loss: 0.49244450017491054\tValidation Loss: 0.48432523213891027\n",
            "Epoch 9  \tTraining Loss: 0.4797020446015512\tValidation Loss: 0.4773738634482261\n",
            "Epoch 10  \tTraining Loss: 0.47290961912875606\tValidation Loss: 0.4736185060113791\n",
            "Epoch 11  \tTraining Loss: 0.46926021319125616\tValidation Loss: 0.4713233593608902\n",
            "Epoch 12  \tTraining Loss: 0.4670303753036299\tValidation Loss: 0.46970762388687903\n",
            "Epoch 13  \tTraining Loss: 0.46544461208806204\tValidation Loss: 0.4684198117442504\n",
            "Epoch 14  \tTraining Loss: 0.4641654015678427\tValidation Loss: 0.4672966183346017\n",
            "Epoch 15  \tTraining Loss: 0.46303800712571347\tValidation Loss: 0.46626338537364975\n",
            "Epoch 16  \tTraining Loss: 0.4619919229554678\tValidation Loss: 0.4652792205581484\n",
            "Epoch 17  \tTraining Loss: 0.46099243968046144\tValidation Loss: 0.46432451352518544\n",
            "Epoch 18  \tTraining Loss: 0.46002063417970723\tValidation Loss: 0.4633874760150339\n",
            "Epoch 19  \tTraining Loss: 0.459067433278871\tValidation Loss: 0.46246451276390305\n",
            "Epoch 20  \tTraining Loss: 0.458127954812531\tValidation Loss: 0.4615526403955348\n",
            "Epoch 21  \tTraining Loss: 0.45719741263663033\tValidation Loss: 0.4606470049889237\n",
            "Epoch 22  \tTraining Loss: 0.4562721752260736\tValidation Loss: 0.45974690672242563\n",
            "Epoch 23  \tTraining Loss: 0.4553533368327994\tValidation Loss: 0.45885298383840567\n",
            "Epoch 24  \tTraining Loss: 0.45444194138137806\tValidation Loss: 0.4579642564831126\n",
            "Epoch 25  \tTraining Loss: 0.4535355234993162\tValidation Loss: 0.4570780289777831\n",
            "Epoch 26  \tTraining Loss: 0.4526326663956963\tValidation Loss: 0.45619294600287613\n",
            "Epoch 27  \tTraining Loss: 0.45173091406068294\tValidation Loss: 0.4553092818280986\n",
            "Epoch 28  \tTraining Loss: 0.4508295508974375\tValidation Loss: 0.4544281166304428\n",
            "Epoch 29  \tTraining Loss: 0.4499280641045611\tValidation Loss: 0.4535460810894125\n",
            "Epoch 30  \tTraining Loss: 0.4490266050027987\tValidation Loss: 0.4526627523871726\n",
            "Epoch 31  \tTraining Loss: 0.448125527230229\tValidation Loss: 0.45177785089323713\n",
            "Epoch 32  \tTraining Loss: 0.447223367716302\tValidation Loss: 0.4508931367513975\n",
            "Epoch 33  \tTraining Loss: 0.4463209541017042\tValidation Loss: 0.45000689094918694\n",
            "Epoch 34  \tTraining Loss: 0.44541803582112105\tValidation Loss: 0.4491199661315657\n",
            "Epoch 35  \tTraining Loss: 0.4445162676276918\tValidation Loss: 0.44823103305942563\n",
            "Epoch 36  \tTraining Loss: 0.4436145285252662\tValidation Loss: 0.4473405459754245\n",
            "Epoch 37  \tTraining Loss: 0.442712761510956\tValidation Loss: 0.44644912658120905\n",
            "Epoch 38  \tTraining Loss: 0.44181023781340545\tValidation Loss: 0.44555708468575717\n",
            "Epoch 39  \tTraining Loss: 0.4409063285477348\tValidation Loss: 0.4446622860276687\n",
            "Epoch 40  \tTraining Loss: 0.4400009766714702\tValidation Loss: 0.4437637195234951\n",
            "Epoch 41  \tTraining Loss: 0.4390935117968859\tValidation Loss: 0.4428622224727171\n",
            "Epoch 42  \tTraining Loss: 0.43818404115487314\tValidation Loss: 0.44196008741226256\n",
            "Epoch 43  \tTraining Loss: 0.43727251758489205\tValidation Loss: 0.4410549025980905\n",
            "Epoch 44  \tTraining Loss: 0.4363577298550211\tValidation Loss: 0.44014817672464374\n",
            "Epoch 45  \tTraining Loss: 0.4354408927441622\tValidation Loss: 0.4392394765795721\n",
            "Epoch 46  \tTraining Loss: 0.43452271408974724\tValidation Loss: 0.43832942279734805\n",
            "Epoch 47  \tTraining Loss: 0.4336034338246571\tValidation Loss: 0.43741781641838084\n",
            "Epoch 48  \tTraining Loss: 0.4326830516508903\tValidation Loss: 0.4365054167104606\n",
            "Epoch 49  \tTraining Loss: 0.43176187477250294\tValidation Loss: 0.43558941896999226\n",
            "Epoch 50  \tTraining Loss: 0.4308379506528068\tValidation Loss: 0.43467118108778274\n",
            "Epoch 51  \tTraining Loss: 0.42991166888939963\tValidation Loss: 0.4337509241495869\n",
            "Epoch 52  \tTraining Loss: 0.428983081615076\tValidation Loss: 0.432828364517572\n",
            "Epoch 53  \tTraining Loss: 0.4280524888701792\tValidation Loss: 0.4319035793437747\n",
            "Epoch 54  \tTraining Loss: 0.4271203463969871\tValidation Loss: 0.43097663765015837\n",
            "Epoch 55  \tTraining Loss: 0.42618638865120234\tValidation Loss: 0.4300480559391896\n",
            "Epoch 56  \tTraining Loss: 0.4252509474799466\tValidation Loss: 0.4291183386701177\n",
            "Epoch 57  \tTraining Loss: 0.424314598838022\tValidation Loss: 0.42818697978449116\n",
            "Epoch 58  \tTraining Loss: 0.42337771847203826\tValidation Loss: 0.4272541722273017\n",
            "Epoch 59  \tTraining Loss: 0.4224402268997953\tValidation Loss: 0.4263204541273726\n",
            "Epoch 60  \tTraining Loss: 0.4215014614127958\tValidation Loss: 0.4253850197519322\n",
            "Epoch 61  \tTraining Loss: 0.42056166830720654\tValidation Loss: 0.4244479583530851\n",
            "Epoch 62  \tTraining Loss: 0.419621525938314\tValidation Loss: 0.4235088817808493\n",
            "Epoch 63  \tTraining Loss: 0.41868041313554144\tValidation Loss: 0.42256926170653464\n",
            "Epoch 64  \tTraining Loss: 0.4177402197403607\tValidation Loss: 0.4216297366815058\n",
            "Epoch 65  \tTraining Loss: 0.4168000998141144\tValidation Loss: 0.4206892813926245\n",
            "Epoch 66  \tTraining Loss: 0.41585928458384297\tValidation Loss: 0.4197482707435665\n",
            "Epoch 67  \tTraining Loss: 0.41491855036513825\tValidation Loss: 0.41880596073056864\n",
            "Epoch 68  \tTraining Loss: 0.413978577361546\tValidation Loss: 0.41786206194199277\n",
            "Epoch 69  \tTraining Loss: 0.4130400450899515\tValidation Loss: 0.41691933687405114\n",
            "Epoch 70  \tTraining Loss: 0.41210227222878765\tValidation Loss: 0.41597396323368474\n",
            "Epoch 71  \tTraining Loss: 0.41116163754383495\tValidation Loss: 0.41502602601758426\n",
            "Epoch 72  \tTraining Loss: 0.41021941106156845\tValidation Loss: 0.4140780164116849\n",
            "Epoch 73  \tTraining Loss: 0.4092765633564972\tValidation Loss: 0.41312715347493956\n",
            "Epoch 74  \tTraining Loss: 0.4083268230266829\tValidation Loss: 0.4121638279758138\n",
            "Epoch 75  \tTraining Loss: 0.4073679729271208\tValidation Loss: 0.4111831821351704\n",
            "Epoch 76  \tTraining Loss: 0.4063939820249611\tValidation Loss: 0.410175280346891\n",
            "Epoch 77  \tTraining Loss: 0.4053928317175932\tValidation Loss: 0.40911316189709884\n",
            "Epoch 78  \tTraining Loss: 0.4043246329598917\tValidation Loss: 0.40795577453164183\n",
            "Epoch 79  \tTraining Loss: 0.40314475669915995\tValidation Loss: 0.4066988260329525\n",
            "Epoch 80  \tTraining Loss: 0.40183852644338475\tValidation Loss: 0.40528424180852307\n",
            "Epoch 81  \tTraining Loss: 0.40033127150317066\tValidation Loss: 0.4033127470782263\n",
            "Epoch 82  \tTraining Loss: 0.39831051316998156\tValidation Loss: 0.400257926193116\n",
            "Epoch 83  \tTraining Loss: 0.39511364824481876\tValidation Loss: 0.3957003898126781\n",
            "Epoch 84  \tTraining Loss: 0.3902521011241419\tValidation Loss: 0.391954731712515\n",
            "Epoch 85  \tTraining Loss: 0.3863132943699981\tValidation Loss: 0.3899705449331144\n",
            "Epoch 86  \tTraining Loss: 0.38421340357888406\tValidation Loss: 0.3886310300922752\n",
            "Epoch 87  \tTraining Loss: 0.38280890834540315\tValidation Loss: 0.3875141058473806\n",
            "Epoch 88  \tTraining Loss: 0.3816560226855519\tValidation Loss: 0.38647769186579767\n",
            "Epoch 89  \tTraining Loss: 0.3805986273853241\tValidation Loss: 0.38547265041057005\n",
            "Epoch 90  \tTraining Loss: 0.379581585861139\tValidation Loss: 0.3844815371383754\n",
            "Epoch 91  \tTraining Loss: 0.3785846789891427\tValidation Loss: 0.3835000776816758\n",
            "Epoch 92  \tTraining Loss: 0.37760138847171515\tValidation Loss: 0.38252661998758375\n",
            "Epoch 93  \tTraining Loss: 0.37662792022028835\tValidation Loss: 0.3815611341785969\n",
            "Epoch 94  \tTraining Loss: 0.3756629149509768\tValidation Loss: 0.3806037856234009\n",
            "Epoch 95  \tTraining Loss: 0.37470668189762063\tValidation Loss: 0.37965437927582263\n",
            "Epoch 96  \tTraining Loss: 0.37375969891191135\tValidation Loss: 0.37871238041477073\n",
            "Epoch 97  \tTraining Loss: 0.3728214167898011\tValidation Loss: 0.3777793962988748\n",
            "Epoch 98  \tTraining Loss: 0.37189256686400396\tValidation Loss: 0.3768553227785591\n",
            "Epoch 99  \tTraining Loss: 0.37097245620132974\tValidation Loss: 0.37593669274263697\n",
            "Epoch 100  \tTraining Loss: 0.37006037362677535\tValidation Loss: 0.37502619635204026\n",
            "Epoch 101  \tTraining Loss: 0.36915771393949237\tValidation Loss: 0.37412235158384555\n",
            "Epoch 102  \tTraining Loss: 0.36826183633322945\tValidation Loss: 0.37321941513811685\n",
            "Epoch 103  \tTraining Loss: 0.3673705113775454\tValidation Loss: 0.37232243423159084\n",
            "Epoch 104  \tTraining Loss: 0.3664864819199288\tValidation Loss: 0.37143004382535\n",
            "Epoch 105  \tTraining Loss: 0.3656073743154725\tValidation Loss: 0.37053228441930003\n",
            "Epoch 106  \tTraining Loss: 0.3647268672804672\tValidation Loss: 0.3696193174665022\n",
            "Epoch 107  \tTraining Loss: 0.36382633935339925\tValidation Loss: 0.36865453182117175\n",
            "Epoch 108  \tTraining Loss: 0.362861295500654\tValidation Loss: 0.3675364225314318\n",
            "Epoch 109  \tTraining Loss: 0.3617259905744314\tValidation Loss: 0.36598517033470696\n",
            "Epoch 110  \tTraining Loss: 0.3601286759791841\tValidation Loss: 0.3623225157604795\n",
            "Epoch 111  \tTraining Loss: 0.3563816414851839\tValidation Loss: 0.35400896276354543\n",
            "Epoch 112  \tTraining Loss: 0.34808402380264825\tValidation Loss: 0.3463954204948993\n",
            "Epoch 113  \tTraining Loss: 0.34043844735474493\tValidation Loss: 0.3431059831653356\n",
            "Epoch 114  \tTraining Loss: 0.33710340190212\tValidation Loss: 0.3414708530417665\n",
            "Epoch 115  \tTraining Loss: 0.33544001242899396\tValidation Loss: 0.340379028295548\n",
            "Epoch 116  \tTraining Loss: 0.3343320593182114\tValidation Loss: 0.3394673950873265\n",
            "Epoch 117  \tTraining Loss: 0.33341298084546617\tValidation Loss: 0.33862004123847683\n",
            "Epoch 118  \tTraining Loss: 0.33256359514592476\tValidation Loss: 0.33779998316860216\n",
            "Epoch 119  \tTraining Loss: 0.3317443711471905\tValidation Loss: 0.33699526951877706\n",
            "Epoch 120  \tTraining Loss: 0.33094274819252034\tValidation Loss: 0.33620224684133\n",
            "Epoch 121  \tTraining Loss: 0.33015418403850527\tValidation Loss: 0.33542050459792816\n",
            "Epoch 122  \tTraining Loss: 0.32937739430135404\tValidation Loss: 0.3346492088912953\n",
            "Epoch 123  \tTraining Loss: 0.32861181473718715\tValidation Loss: 0.33388837548704586\n",
            "Epoch 124  \tTraining Loss: 0.3278574258735916\tValidation Loss: 0.33313813163304345\n",
            "Epoch 125  \tTraining Loss: 0.3271139495743487\tValidation Loss: 0.33239908597681944\n",
            "Epoch 126  \tTraining Loss: 0.3263815752237541\tValidation Loss: 0.3316708038521047\n",
            "Epoch 127  \tTraining Loss: 0.3256601304957933\tValidation Loss: 0.3309531768453015\n",
            "Epoch 128  \tTraining Loss: 0.32494961273642664\tValidation Loss: 0.33024597792306065\n",
            "Epoch 129  \tTraining Loss: 0.32424995932294637\tValidation Loss: 0.3295485187354402\n",
            "Epoch 130  \tTraining Loss: 0.3235605244810727\tValidation Loss: 0.32886079677675617\n",
            "Epoch 131  \tTraining Loss: 0.3228812780143397\tValidation Loss: 0.3281829751392484\n",
            "Epoch 132  \tTraining Loss: 0.3222122356663404\tValidation Loss: 0.32751477524736167\n",
            "Epoch 133  \tTraining Loss: 0.3215530610881069\tValidation Loss: 0.3268560963983732\n",
            "Epoch 134  \tTraining Loss: 0.3209039021503009\tValidation Loss: 0.3262069361549692\n",
            "Epoch 135  \tTraining Loss: 0.3202642222331164\tValidation Loss: 0.3255674968587932\n",
            "Epoch 136  \tTraining Loss: 0.3196340453913359\tValidation Loss: 0.3249372978050929\n",
            "Epoch 137  \tTraining Loss: 0.31901324543471943\tValidation Loss: 0.32431586217390834\n",
            "Epoch 138  \tTraining Loss: 0.31840184695647233\tValidation Loss: 0.32370331061826363\n",
            "Epoch 139  \tTraining Loss: 0.3177995915334371\tValidation Loss: 0.32310027289091103\n",
            "Epoch 140  \tTraining Loss: 0.31720641342431233\tValidation Loss: 0.32250590628007875\n",
            "Epoch 141  \tTraining Loss: 0.3166219579076796\tValidation Loss: 0.32192012244198653\n",
            "Epoch 142  \tTraining Loss: 0.31604645171764384\tValidation Loss: 0.32134301869923565\n",
            "Epoch 143  \tTraining Loss: 0.3154796291990588\tValidation Loss: 0.3207744005954364\n",
            "Epoch 144  \tTraining Loss: 0.3149213442736238\tValidation Loss: 0.32021388493773756\n",
            "Epoch 145  \tTraining Loss: 0.31437141338284813\tValidation Loss: 0.3196617614483723\n",
            "Epoch 146  \tTraining Loss: 0.3138299827496432\tValidation Loss: 0.3191176703095907\n",
            "Epoch 147  \tTraining Loss: 0.31329645621691965\tValidation Loss: 0.31858133752763546\n",
            "Epoch 148  \tTraining Loss: 0.3127706292283547\tValidation Loss: 0.3180528176885997\n",
            "Epoch 149  \tTraining Loss: 0.3122526305009191\tValidation Loss: 0.3175318201326348\n",
            "Epoch 150  \tTraining Loss: 0.31174202744535734\tValidation Loss: 0.3170181424074684\n",
            "Epoch 151  \tTraining Loss: 0.31123885791867095\tValidation Loss: 0.31651177000220193\n",
            "Epoch 152  \tTraining Loss: 0.3107429688992671\tValidation Loss: 0.31601236148238426\n",
            "Epoch 153  \tTraining Loss: 0.31025438306970243\tValidation Loss: 0.3155196261628218\n",
            "Epoch 154  \tTraining Loss: 0.30977298544209964\tValidation Loss: 0.3150339730326214\n",
            "Epoch 155  \tTraining Loss: 0.3092986174664294\tValidation Loss: 0.31455521431246625\n",
            "Epoch 156  \tTraining Loss: 0.30883089934568775\tValidation Loss: 0.31408347475571813\n",
            "Epoch 157  \tTraining Loss: 0.30836993465785467\tValidation Loss: 0.3136182846987536\n",
            "Epoch 158  \tTraining Loss: 0.30791558127071333\tValidation Loss: 0.3131599836567367\n",
            "Epoch 159  \tTraining Loss: 0.3074677825867185\tValidation Loss: 0.3127082795439167\n",
            "Epoch 160  \tTraining Loss: 0.3070261044232248\tValidation Loss: 0.3122627418218909\n",
            "Epoch 161  \tTraining Loss: 0.3065905182845531\tValidation Loss: 0.31182313336908646\n",
            "Epoch 162  \tTraining Loss: 0.3061610470447732\tValidation Loss: 0.31138883825047237\n",
            "Epoch 163  \tTraining Loss: 0.3057369731480374\tValidation Loss: 0.31096036593733134\n",
            "Epoch 164  \tTraining Loss: 0.3053184531841\tValidation Loss: 0.31053683123016057\n",
            "Epoch 165  \tTraining Loss: 0.3049052128226577\tValidation Loss: 0.3101188330341531\n",
            "Epoch 166  \tTraining Loss: 0.3044973466744261\tValidation Loss: 0.30970624921007867\n",
            "Epoch 167  \tTraining Loss: 0.3040946847336912\tValidation Loss: 0.30929878153464185\n",
            "Epoch 168  \tTraining Loss: 0.30369663503731853\tValidation Loss: 0.3088957046102381\n",
            "Epoch 169  \tTraining Loss: 0.3033033526597804\tValidation Loss: 0.3084966605823968\n",
            "Epoch 170  \tTraining Loss: 0.30291483523060514\tValidation Loss: 0.30810225814732606\n",
            "Epoch 171  \tTraining Loss: 0.3025313901152937\tValidation Loss: 0.30771255084649085\n",
            "Epoch 172  \tTraining Loss: 0.3021524267735312\tValidation Loss: 0.30732771493884814\n",
            "Epoch 173  \tTraining Loss: 0.30177802131951537\tValidation Loss: 0.3069469827408685\n",
            "Epoch 174  \tTraining Loss: 0.30140753408986914\tValidation Loss: 0.3065707169511816\n",
            "Epoch 175  \tTraining Loss: 0.30104117147415377\tValidation Loss: 0.3061987895158401\n",
            "Epoch 176  \tTraining Loss: 0.3006785621997629\tValidation Loss: 0.30583114493903757\n",
            "Epoch 177  \tTraining Loss: 0.3003196629689193\tValidation Loss: 0.30546705930730295\n",
            "Epoch 178  \tTraining Loss: 0.2999635419936533\tValidation Loss: 0.3051046583306923\n",
            "Epoch 179  \tTraining Loss: 0.29961127322164727\tValidation Loss: 0.3047437464511171\n",
            "Epoch 180  \tTraining Loss: 0.29926137911989126\tValidation Loss: 0.304384713667403\n",
            "Epoch 181  \tTraining Loss: 0.2989128522975289\tValidation Loss: 0.3040253214282598\n",
            "Epoch 182  \tTraining Loss: 0.2985627816340139\tValidation Loss: 0.3036597170405859\n",
            "Epoch 183  \tTraining Loss: 0.29820389769326633\tValidation Loss: 0.3032892373471054\n",
            "Epoch 184  \tTraining Loss: 0.2978376578782676\tValidation Loss: 0.30290224312816816\n",
            "Epoch 185  \tTraining Loss: 0.29745247325957097\tValidation Loss: 0.30247418239203533\n",
            "Epoch 186  \tTraining Loss: 0.297026784567398\tValidation Loss: 0.30196910431825985\n",
            "Epoch 187  \tTraining Loss: 0.2965477723133139\tValidation Loss: 0.3013420497661846\n",
            "Epoch 188  \tTraining Loss: 0.2959794517872663\tValidation Loss: 0.30031168810243236\n",
            "Epoch 189  \tTraining Loss: 0.29512458892903914\tValidation Loss: 0.29794741252686535\n",
            "Epoch 190  \tTraining Loss: 0.2930947193476868\tValidation Loss: 0.2903655503375099\n",
            "Epoch 191  \tTraining Loss: 0.2857075063073514\tValidation Loss: 0.27499360013291374\n",
            "Epoch 192  \tTraining Loss: 0.2700004764750307\tValidation Loss: 0.2678783059340254\n",
            "Epoch 193  \tTraining Loss: 0.2627827819684379\tValidation Loss: 0.26594466387867055\n",
            "Epoch 194  \tTraining Loss: 0.2608043939273886\tValidation Loss: 0.26520792318403585\n",
            "Epoch 195  \tTraining Loss: 0.26005143821839993\tValidation Loss: 0.2647483536086536\n",
            "Epoch 196  \tTraining Loss: 0.259588624888154\tValidation Loss: 0.26435605396018974\n",
            "Epoch 197  \tTraining Loss: 0.2591988916911927\tValidation Loss: 0.26398322373623084\n",
            "Epoch 198  \tTraining Loss: 0.25883116965781516\tValidation Loss: 0.26361975104866586\n",
            "Epoch 199  \tTraining Loss: 0.2584735666258112\tValidation Loss: 0.263262880598646\n",
            "Epoch 200  \tTraining Loss: 0.25812287317101584\tValidation Loss: 0.262911966144481\n",
            "Epoch 201  \tTraining Loss: 0.25777819185068346\tValidation Loss: 0.2625670283165287\n",
            "Epoch 202  \tTraining Loss: 0.25743922241934813\tValidation Loss: 0.26222758032916504\n",
            "Epoch 203  \tTraining Loss: 0.25710572867557535\tValidation Loss: 0.2618938432603724\n",
            "Epoch 204  \tTraining Loss: 0.25677746466988954\tValidation Loss: 0.26156531050737974\n",
            "Epoch 205  \tTraining Loss: 0.25645422018305775\tValidation Loss: 0.2612419235256832\n",
            "Epoch 206  \tTraining Loss: 0.2561359027134254\tValidation Loss: 0.2609236061996937\n",
            "Epoch 207  \tTraining Loss: 0.2558224114898485\tValidation Loss: 0.2606102204444244\n",
            "Epoch 208  \tTraining Loss: 0.2555135552055597\tValidation Loss: 0.26030154772026987\n",
            "Epoch 209  \tTraining Loss: 0.2552090921919164\tValidation Loss: 0.2599971507334856\n",
            "Epoch 210  \tTraining Loss: 0.25490899039230336\tValidation Loss: 0.2596972989291461\n",
            "Epoch 211  \tTraining Loss: 0.25461323537867425\tValidation Loss: 0.2594020499199736\n",
            "Epoch 212  \tTraining Loss: 0.2543216207303062\tValidation Loss: 0.2591111106229058\n",
            "Epoch 213  \tTraining Loss: 0.2540341506392702\tValidation Loss: 0.25882439359413084\n",
            "Epoch 214  \tTraining Loss: 0.2537506605388366\tValidation Loss: 0.25854174264693214\n",
            "Epoch 215  \tTraining Loss: 0.25347100160131036\tValidation Loss: 0.2582631106284256\n",
            "Epoch 216  \tTraining Loss: 0.25319512753505063\tValidation Loss: 0.2579882318617196\n",
            "Epoch 217  \tTraining Loss: 0.25292292740621514\tValidation Loss: 0.2577170279508818\n",
            "Epoch 218  \tTraining Loss: 0.2526543181417047\tValidation Loss: 0.2574494160226696\n",
            "Epoch 219  \tTraining Loss: 0.252389177561973\tValidation Loss: 0.257185343059832\n",
            "Epoch 220  \tTraining Loss: 0.2521274161834666\tValidation Loss: 0.2569247315705698\n",
            "Epoch 221  \tTraining Loss: 0.25186897409037234\tValidation Loss: 0.2566674614626934\n",
            "Epoch 222  \tTraining Loss: 0.25161375961710675\tValidation Loss: 0.2564132345816694\n",
            "Epoch 223  \tTraining Loss: 0.2513616781119867\tValidation Loss: 0.2561620622421803\n",
            "Epoch 224  \tTraining Loss: 0.25111257827961314\tValidation Loss: 0.2559136683248965\n",
            "Epoch 225  \tTraining Loss: 0.25086635190708295\tValidation Loss: 0.2556682953301308\n",
            "Epoch 226  \tTraining Loss: 0.250622951022442\tValidation Loss: 0.25542609643681957\n",
            "Epoch 227  \tTraining Loss: 0.25038233352059663\tValidation Loss: 0.25518691764000334\n",
            "Epoch 228  \tTraining Loss: 0.2501445182205218\tValidation Loss: 0.25495069185954516\n",
            "Epoch 229  \tTraining Loss: 0.24990945344288684\tValidation Loss: 0.25471723838719645\n",
            "Epoch 230  \tTraining Loss: 0.24967707339342954\tValidation Loss: 0.254486439613132\n",
            "Epoch 231  \tTraining Loss: 0.24944734191645881\tValidation Loss: 0.2542584343325857\n",
            "Epoch 232  \tTraining Loss: 0.2492201835162948\tValidation Loss: 0.2540330844294091\n",
            "Epoch 233  \tTraining Loss: 0.24899555727521686\tValidation Loss: 0.25381027168818854\n",
            "Epoch 234  \tTraining Loss: 0.24877336807751516\tValidation Loss: 0.2535898703916114\n",
            "Epoch 235  \tTraining Loss: 0.2485535570598786\tValidation Loss: 0.25337202998409497\n",
            "Epoch 236  \tTraining Loss: 0.24833610836697553\tValidation Loss: 0.2531566216954869\n",
            "Epoch 237  \tTraining Loss: 0.24812099837684598\tValidation Loss: 0.2529435207393502\n",
            "Epoch 238  \tTraining Loss: 0.24790818792973934\tValidation Loss: 0.25273274805711543\n",
            "Epoch 239  \tTraining Loss: 0.24769756629475445\tValidation Loss: 0.2525242240920224\n",
            "Epoch 240  \tTraining Loss: 0.2474890540177743\tValidation Loss: 0.252317463814946\n",
            "Epoch 241  \tTraining Loss: 0.24728256080700609\tValidation Loss: 0.2521126251215407\n",
            "Epoch 242  \tTraining Loss: 0.247078093462745\tValidation Loss: 0.25190980895755455\n",
            "Epoch 243  \tTraining Loss: 0.24687560275384102\tValidation Loss: 0.2517088894240016\n",
            "Epoch 244  \tTraining Loss: 0.24667507902095925\tValidation Loss: 0.251510007700397\n",
            "Epoch 245  \tTraining Loss: 0.24647652225275662\tValidation Loss: 0.25131284475004056\n",
            "Epoch 246  \tTraining Loss: 0.24627991142028383\tValidation Loss: 0.25111769831348574\n",
            "Epoch 247  \tTraining Loss: 0.2460852595695485\tValidation Loss: 0.2509246985834862\n",
            "Epoch 248  \tTraining Loss: 0.24589246622668748\tValidation Loss: 0.25073361944076067\n",
            "Epoch 249  \tTraining Loss: 0.24570151478958496\tValidation Loss: 0.25054417906902265\n",
            "Epoch 250  \tTraining Loss: 0.24551234950175788\tValidation Loss: 0.25035671075792215\n",
            "Epoch 251  \tTraining Loss: 0.24532495470581606\tValidation Loss: 0.2501710016371513\n",
            "Epoch 252  \tTraining Loss: 0.24513927869417224\tValidation Loss: 0.24998690210579358\n",
            "Epoch 253  \tTraining Loss: 0.24495530401626914\tValidation Loss: 0.2498044652640512\n",
            "Epoch 254  \tTraining Loss: 0.2447730381608143\tValidation Loss: 0.24962381378385134\n",
            "Epoch 255  \tTraining Loss: 0.24459246165867918\tValidation Loss: 0.24944480703540833\n",
            "Epoch 256  \tTraining Loss: 0.2444134891000501\tValidation Loss: 0.24926746650105147\n",
            "Epoch 257  \tTraining Loss: 0.244236117078939\tValidation Loss: 0.24909176430771346\n",
            "Epoch 258  \tTraining Loss: 0.24406033279427053\tValidation Loss: 0.24891770593524992\n",
            "Epoch 259  \tTraining Loss: 0.2438860942888393\tValidation Loss: 0.24874523055407335\n",
            "Epoch 260  \tTraining Loss: 0.24371339523281757\tValidation Loss: 0.2485743420568226\n",
            "Epoch 261  \tTraining Loss: 0.24354225631243756\tValidation Loss: 0.24840495430379209\n",
            "Epoch 262  \tTraining Loss: 0.2433726788705292\tValidation Loss: 0.24823699032686422\n",
            "Epoch 263  \tTraining Loss: 0.24320460699878382\tValidation Loss: 0.24807048915265675\n",
            "Epoch 264  \tTraining Loss: 0.24303794122133568\tValidation Loss: 0.24790551202297526\n",
            "Epoch 265  \tTraining Loss: 0.242872615797319\tValidation Loss: 0.2477416022133903\n",
            "Epoch 266  \tTraining Loss: 0.24270854913709536\tValidation Loss: 0.2475791795602484\n",
            "Epoch 267  \tTraining Loss: 0.24254585232646195\tValidation Loss: 0.24741823334870353\n",
            "Epoch 268  \tTraining Loss: 0.2423844159060759\tValidation Loss: 0.24725860860150797\n",
            "Epoch 269  \tTraining Loss: 0.2422242713746643\tValidation Loss: 0.24710030580199627\n",
            "Epoch 270  \tTraining Loss: 0.24206542772452955\tValidation Loss: 0.24694303835339682\n",
            "Epoch 271  \tTraining Loss: 0.24190783262058257\tValidation Loss: 0.24678713601845995\n",
            "Epoch 272  \tTraining Loss: 0.24175150308485088\tValidation Loss: 0.24663239180166457\n",
            "Epoch 273  \tTraining Loss: 0.24159622098219538\tValidation Loss: 0.24647879428010896\n",
            "Epoch 274  \tTraining Loss: 0.2414420788202831\tValidation Loss: 0.2463263864142121\n",
            "Epoch 275  \tTraining Loss: 0.24128916776957676\tValidation Loss: 0.24617527040975923\n",
            "Epoch 276  \tTraining Loss: 0.24113747052225884\tValidation Loss: 0.24602524493132308\n",
            "Epoch 277  \tTraining Loss: 0.2409869329697373\tValidation Loss: 0.24587630350545342\n",
            "Epoch 278  \tTraining Loss: 0.24083760717861308\tValidation Loss: 0.24572844605901156\n",
            "Epoch 279  \tTraining Loss: 0.24068935292276392\tValidation Loss: 0.24558074193602977\n",
            "Epoch 280  \tTraining Loss: 0.24054190022581856\tValidation Loss: 0.24543324816969825\n",
            "Epoch 281  \tTraining Loss: 0.24039560972667856\tValidation Loss: 0.24528624209454936\n",
            "Epoch 282  \tTraining Loss: 0.2402504633096237\tValidation Loss: 0.24514020545829412\n",
            "Epoch 283  \tTraining Loss: 0.24010642984274008\tValidation Loss: 0.24499529033961176\n",
            "Epoch 284  \tTraining Loss: 0.23996339848682807\tValidation Loss: 0.2448507862973152\n",
            "Epoch 285  \tTraining Loss: 0.239821477630449\tValidation Loss: 0.24470742398884823\n",
            "Epoch 286  \tTraining Loss: 0.23968058208165507\tValidation Loss: 0.24456440985908512\n",
            "Epoch 287  \tTraining Loss: 0.239540284046024\tValidation Loss: 0.24442186149306613\n",
            "Epoch 288  \tTraining Loss: 0.23940079457013141\tValidation Loss: 0.2442801710951138\n",
            "Epoch 289  \tTraining Loss: 0.23926203769545779\tValidation Loss: 0.24413784213072023\n",
            "Epoch 290  \tTraining Loss: 0.23912323578080055\tValidation Loss: 0.2439954332963303\n",
            "Epoch 291  \tTraining Loss: 0.23898415414098453\tValidation Loss: 0.2438510329295777\n",
            "Epoch 292  \tTraining Loss: 0.2388435510810129\tValidation Loss: 0.24370154890191356\n",
            "Epoch 293  \tTraining Loss: 0.2387017320284666\tValidation Loss: 0.24354636810043767\n",
            "Epoch 294  \tTraining Loss: 0.23855588036980357\tValidation Loss: 0.24338331129288335\n",
            "Epoch 295  \tTraining Loss: 0.23840558702879777\tValidation Loss: 0.2432098149989956\n",
            "Epoch 296  \tTraining Loss: 0.23824738134798903\tValidation Loss: 0.24303150683763863\n",
            "Epoch 297  \tTraining Loss: 0.23807944800293082\tValidation Loss: 0.2428401665785304\n",
            "Epoch 298  \tTraining Loss: 0.23789480040450264\tValidation Loss: 0.24262279702839765\n",
            "Epoch 299  \tTraining Loss: 0.23768676557428456\tValidation Loss: 0.24236776955363837\n",
            "Epoch 300  \tTraining Loss: 0.2374441165977077\tValidation Loss: 0.24199911706769425\n",
            "Epoch 301  \tTraining Loss: 0.2371190360364851\tValidation Loss: 0.2414503203615154\n",
            "Epoch 302  \tTraining Loss: 0.23664635990797192\tValidation Loss: 0.240504087625262\n",
            "Epoch 303  \tTraining Loss: 0.23576932650994994\tValidation Loss: 0.23860790828836467\n",
            "Epoch 304  \tTraining Loss: 0.2339505231306391\tValidation Loss: 0.23562689985046156\n",
            "Epoch 305  \tTraining Loss: 0.23116349034681946\tValidation Loss: 0.23346759923443738\n",
            "Epoch 306  \tTraining Loss: 0.22918892519497983\tValidation Loss: 0.23202165285073756\n",
            "Epoch 307  \tTraining Loss: 0.22787025605719913\tValidation Loss: 0.22798800970002345\n",
            "Epoch 308  \tTraining Loss: 0.22406028835326122\tValidation Loss: 0.2195911544157873\n",
            "Epoch 309  \tTraining Loss: 0.21620636008387717\tValidation Loss: 0.21713669782950082\n",
            "Epoch 310  \tTraining Loss: 0.21397756602153664\tValidation Loss: 0.21634693061498922\n",
            "Epoch 311  \tTraining Loss: 0.2132478278776737\tValidation Loss: 0.2158441025143227\n",
            "Epoch 312  \tTraining Loss: 0.21274973883866777\tValidation Loss: 0.21540716880870467\n",
            "Epoch 313  \tTraining Loss: 0.21229978314888018\tValidation Loss: 0.21499604579210388\n",
            "Epoch 314  \tTraining Loss: 0.21186985340619077\tValidation Loss: 0.21460062100766555\n",
            "Epoch 315  \tTraining Loss: 0.2114548605089315\tValidation Loss: 0.2142184313974726\n",
            "Epoch 316  \tTraining Loss: 0.21105351571751635\tValidation Loss: 0.21384837432336584\n",
            "Epoch 317  \tTraining Loss: 0.2106649969213102\tValidation Loss: 0.21348956790000603\n",
            "Epoch 318  \tTraining Loss: 0.21028867687365096\tValidation Loss: 0.21314172695641756\n",
            "Epoch 319  \tTraining Loss: 0.20992395760168828\tValidation Loss: 0.21280376770224016\n",
            "Epoch 320  \tTraining Loss: 0.20957031887275399\tValidation Loss: 0.21247577921448285\n",
            "Epoch 321  \tTraining Loss: 0.20922728573673416\tValidation Loss: 0.21215707306720166\n",
            "Epoch 322  \tTraining Loss: 0.20889437270711247\tValidation Loss: 0.211847141706549\n",
            "Epoch 323  \tTraining Loss: 0.20857113514233377\tValidation Loss: 0.21154587205072456\n",
            "Epoch 324  \tTraining Loss: 0.2082572209603876\tValidation Loss: 0.21125267671709053\n",
            "Epoch 325  \tTraining Loss: 0.20795214628975675\tValidation Loss: 0.21096735315777373\n",
            "Epoch 326  \tTraining Loss: 0.20765558528940195\tValidation Loss: 0.2106895852741397\n",
            "Epoch 327  \tTraining Loss: 0.20736717396070006\tValidation Loss: 0.2104190807991803\n",
            "Epoch 328  \tTraining Loss: 0.2070865969005587\tValidation Loss: 0.21015555781570708\n",
            "Epoch 329  \tTraining Loss: 0.2068135360998984\tValidation Loss: 0.20989885186073526\n",
            "Epoch 330  \tTraining Loss: 0.20654766778897735\tValidation Loss: 0.20964867829599154\n",
            "Epoch 331  \tTraining Loss: 0.20628871772119928\tValidation Loss: 0.20940438974165226\n",
            "Epoch 332  \tTraining Loss: 0.20603635829412073\tValidation Loss: 0.20916627726920847\n",
            "Epoch 333  \tTraining Loss: 0.20579038680042874\tValidation Loss: 0.20893383376524655\n",
            "Epoch 334  \tTraining Loss: 0.20555053783617372\tValidation Loss: 0.2087070313701998\n",
            "Epoch 335  \tTraining Loss: 0.2053165203720626\tValidation Loss: 0.20848546413376287\n",
            "Epoch 336  \tTraining Loss: 0.20508813042358184\tValidation Loss: 0.2082689370852141\n",
            "Epoch 337  \tTraining Loss: 0.20486514468663272\tValidation Loss: 0.2080573012868868\n",
            "Epoch 338  \tTraining Loss: 0.20464738888930667\tValidation Loss: 0.20785031245499422\n",
            "Epoch 339  \tTraining Loss: 0.2044346599303532\tValidation Loss: 0.2076477720270024\n",
            "Epoch 340  \tTraining Loss: 0.2042267510949547\tValidation Loss: 0.20744967624539137\n",
            "Epoch 341  \tTraining Loss: 0.20402348388808322\tValidation Loss: 0.2072557638736306\n",
            "Epoch 342  \tTraining Loss: 0.20382466354558917\tValidation Loss: 0.20706570922945092\n",
            "Epoch 343  \tTraining Loss: 0.20363012687720086\tValidation Loss: 0.20687938865189678\n",
            "Epoch 344  \tTraining Loss: 0.2034397816180596\tValidation Loss: 0.20669696406379734\n",
            "Epoch 345  \tTraining Loss: 0.2032533591309524\tValidation Loss: 0.2065181988444592\n",
            "Epoch 346  \tTraining Loss: 0.2030707316186978\tValidation Loss: 0.20634257950336293\n",
            "Epoch 347  \tTraining Loss: 0.20289174844126948\tValidation Loss: 0.20617034994435185\n",
            "Epoch 348  \tTraining Loss: 0.20271622805164555\tValidation Loss: 0.20600144460504377\n",
            "Epoch 349  \tTraining Loss: 0.20254407883922032\tValidation Loss: 0.2058354274205832\n",
            "Epoch 350  \tTraining Loss: 0.20237508099259605\tValidation Loss: 0.20567218742548599\n",
            "Epoch 351  \tTraining Loss: 0.2022091795814626\tValidation Loss: 0.20551191668620356\n",
            "Epoch 352  \tTraining Loss: 0.2020462724801023\tValidation Loss: 0.2053545222797202\n",
            "Epoch 353  \tTraining Loss: 0.201886244858948\tValidation Loss: 0.20519975754314054\n",
            "Epoch 354  \tTraining Loss: 0.20172900861467843\tValidation Loss: 0.20504738180391038\n",
            "Epoch 355  \tTraining Loss: 0.20157442416578217\tValidation Loss: 0.2048975266158714\n",
            "Epoch 356  \tTraining Loss: 0.20142238691613054\tValidation Loss: 0.20475002146270083\n",
            "Epoch 357  \tTraining Loss: 0.20127283791004413\tValidation Loss: 0.204604772366269\n",
            "Epoch 358  \tTraining Loss: 0.20112566347637317\tValidation Loss: 0.20446172613537814\n",
            "Epoch 359  \tTraining Loss: 0.20098079532975197\tValidation Loss: 0.2043208491720846\n",
            "Epoch 360  \tTraining Loss: 0.20083813189002941\tValidation Loss: 0.20418206202291805\n",
            "Epoch 361  \tTraining Loss: 0.20069759728431652\tValidation Loss: 0.20404519705228705\n",
            "Epoch 362  \tTraining Loss: 0.200559116004986\tValidation Loss: 0.20391002863584956\n",
            "Epoch 363  \tTraining Loss: 0.200422619432688\tValidation Loss: 0.20377680757022085\n",
            "Epoch 364  \tTraining Loss: 0.2002880553938371\tValidation Loss: 0.20364532963344031\n",
            "Epoch 365  \tTraining Loss: 0.20015534756457531\tValidation Loss: 0.2035155673553647\n",
            "Epoch 366  \tTraining Loss: 0.2000244316043773\tValidation Loss: 0.2033874446417543\n",
            "Epoch 367  \tTraining Loss: 0.19989523875345022\tValidation Loss: 0.20326103992319805\n",
            "Epoch 368  \tTraining Loss: 0.19976770609728375\tValidation Loss: 0.20313611909957913\n",
            "Epoch 369  \tTraining Loss: 0.19964171479340123\tValidation Loss: 0.20301269013235593\n",
            "Epoch 370  \tTraining Loss: 0.19951725164512615\tValidation Loss: 0.20289072223320556\n",
            "Epoch 371  \tTraining Loss: 0.1993942912105269\tValidation Loss: 0.20277015016087666\n",
            "Epoch 372  \tTraining Loss: 0.19927277985340278\tValidation Loss: 0.20265098437495171\n",
            "Epoch 373  \tTraining Loss: 0.199152685500242\tValidation Loss: 0.2025331416200854\n",
            "Epoch 374  \tTraining Loss: 0.19903395694902992\tValidation Loss: 0.2024166387255176\n",
            "Epoch 375  \tTraining Loss: 0.19891650488585608\tValidation Loss: 0.2023013635138536\n",
            "Epoch 376  \tTraining Loss: 0.19880029812925312\tValidation Loss: 0.20218732502409817\n",
            "Epoch 377  \tTraining Loss: 0.19868530230603207\tValidation Loss: 0.20207436712364865\n",
            "Epoch 378  \tTraining Loss: 0.19857149512371708\tValidation Loss: 0.2019620350617955\n",
            "Epoch 379  \tTraining Loss: 0.19845880697611037\tValidation Loss: 0.2018510434837382\n",
            "Epoch 380  \tTraining Loss: 0.198347220659338\tValidation Loss: 0.20174114875980514\n",
            "Epoch 381  \tTraining Loss: 0.19823674459291962\tValidation Loss: 0.20163236876470722\n",
            "Epoch 382  \tTraining Loss: 0.19812735797951783\tValidation Loss: 0.2015244398727554\n",
            "Epoch 383  \tTraining Loss: 0.19801898732576606\tValidation Loss: 0.2014176276931776\n",
            "Epoch 384  \tTraining Loss: 0.19791161748355654\tValidation Loss: 0.201311941027459\n",
            "Epoch 385  \tTraining Loss: 0.19780519936559307\tValidation Loss: 0.20120717487137338\n",
            "Epoch 386  \tTraining Loss: 0.19769971949737275\tValidation Loss: 0.2011032577592311\n",
            "Epoch 387  \tTraining Loss: 0.1975951259942994\tValidation Loss: 0.2010004982254052\n",
            "Epoch 388  \tTraining Loss: 0.19749140968008716\tValidation Loss: 0.20089839241259497\n",
            "Epoch 389  \tTraining Loss: 0.19738856446295\tValidation Loss: 0.20079730523532544\n",
            "Epoch 390  \tTraining Loss: 0.19728652671329502\tValidation Loss: 0.20069704134010152\n",
            "Epoch 391  \tTraining Loss: 0.1971852902743452\tValidation Loss: 0.20059751849853003\n",
            "Epoch 392  \tTraining Loss: 0.19708486025636754\tValidation Loss: 0.2004986964664126\n",
            "Epoch 393  \tTraining Loss: 0.19698521072076774\tValidation Loss: 0.20040064952953768\n",
            "Epoch 394  \tTraining Loss: 0.1968863190152946\tValidation Loss: 0.2003033699514176\n",
            "Epoch 395  \tTraining Loss: 0.19678814179607673\tValidation Loss: 0.2002067991199133\n",
            "Epoch 396  \tTraining Loss: 0.1966907031614056\tValidation Loss: 0.20011087648354658\n",
            "Epoch 397  \tTraining Loss: 0.19659397258887643\tValidation Loss: 0.20001591732506116\n",
            "Epoch 398  \tTraining Loss: 0.19649792200102106\tValidation Loss: 0.19992154592302774\n",
            "Epoch 399  \tTraining Loss: 0.19640251327804076\tValidation Loss: 0.199827758237317\n",
            "Epoch 400  \tTraining Loss: 0.1963077304055683\tValidation Loss: 0.19973459556253859\n",
            "lr, batch_size: (0.0001, 500)\n",
            "Epoch 1  \tTraining Loss: 0.8343354864252978\tValidation Loss: 0.7937891531497792\n",
            "Epoch 2  \tTraining Loss: 0.7903017837901265\tValidation Loss: 0.7396026472419468\n",
            "Epoch 3  \tTraining Loss: 0.7359201909958584\tValidation Loss: 0.675473217954124\n",
            "Epoch 4  \tTraining Loss: 0.6712661177190833\tValidation Loss: 0.6108661728883575\n",
            "Epoch 5  \tTraining Loss: 0.606227869917893\tValidation Loss: 0.5558117685758847\n",
            "Epoch 6  \tTraining Loss: 0.551030858571779\tValidation Loss: 0.5187382538776669\n",
            "Epoch 7  \tTraining Loss: 0.5139269378406597\tValidation Loss: 0.4972108725344907\n",
            "Epoch 8  \tTraining Loss: 0.49244450017491054\tValidation Loss: 0.48432523213891027\n",
            "Epoch 9  \tTraining Loss: 0.4797020446015512\tValidation Loss: 0.4773738634482261\n",
            "Epoch 10  \tTraining Loss: 0.47290961912875606\tValidation Loss: 0.4736185060113791\n",
            "Epoch 11  \tTraining Loss: 0.46926021319125616\tValidation Loss: 0.4713233593608902\n",
            "Epoch 12  \tTraining Loss: 0.4670303753036299\tValidation Loss: 0.46970762388687903\n",
            "Epoch 13  \tTraining Loss: 0.46544461208806204\tValidation Loss: 0.4684198117442504\n",
            "Epoch 14  \tTraining Loss: 0.4641654015678427\tValidation Loss: 0.4672966183346017\n",
            "Epoch 15  \tTraining Loss: 0.46303800712571347\tValidation Loss: 0.46626338537364975\n",
            "Epoch 16  \tTraining Loss: 0.4619919229554678\tValidation Loss: 0.4652792205581484\n",
            "Epoch 17  \tTraining Loss: 0.46099243968046144\tValidation Loss: 0.46432451352518544\n",
            "Epoch 18  \tTraining Loss: 0.46002063417970723\tValidation Loss: 0.4633874760150339\n",
            "Epoch 19  \tTraining Loss: 0.459067433278871\tValidation Loss: 0.46246451276390305\n",
            "Epoch 20  \tTraining Loss: 0.458127954812531\tValidation Loss: 0.4615526403955348\n",
            "Epoch 21  \tTraining Loss: 0.45719741263663033\tValidation Loss: 0.4606470049889237\n",
            "Epoch 22  \tTraining Loss: 0.4562721752260736\tValidation Loss: 0.45974690672242563\n",
            "Epoch 23  \tTraining Loss: 0.4553533368327994\tValidation Loss: 0.45885298383840567\n",
            "Epoch 24  \tTraining Loss: 0.45444194138137806\tValidation Loss: 0.4579642564831126\n",
            "Epoch 25  \tTraining Loss: 0.4535355234993162\tValidation Loss: 0.4570780289777831\n",
            "Epoch 26  \tTraining Loss: 0.4526326663956963\tValidation Loss: 0.45619294600287613\n",
            "Epoch 27  \tTraining Loss: 0.45173091406068294\tValidation Loss: 0.4553092818280986\n",
            "Epoch 28  \tTraining Loss: 0.4508295508974375\tValidation Loss: 0.4544281166304428\n",
            "Epoch 29  \tTraining Loss: 0.4499280641045611\tValidation Loss: 0.4535460810894125\n",
            "Epoch 30  \tTraining Loss: 0.4490266050027987\tValidation Loss: 0.4526627523871726\n",
            "Epoch 31  \tTraining Loss: 0.448125527230229\tValidation Loss: 0.45177785089323713\n",
            "Epoch 32  \tTraining Loss: 0.447223367716302\tValidation Loss: 0.4508931367513975\n",
            "Epoch 33  \tTraining Loss: 0.4463209541017042\tValidation Loss: 0.45000689094918694\n",
            "Epoch 34  \tTraining Loss: 0.44541803582112105\tValidation Loss: 0.4491199661315657\n",
            "Epoch 35  \tTraining Loss: 0.4445162676276918\tValidation Loss: 0.44823103305942563\n",
            "Epoch 36  \tTraining Loss: 0.4436145285252662\tValidation Loss: 0.4473405459754245\n",
            "Epoch 37  \tTraining Loss: 0.442712761510956\tValidation Loss: 0.44644912658120905\n",
            "Epoch 38  \tTraining Loss: 0.44181023781340545\tValidation Loss: 0.44555708468575717\n",
            "Epoch 39  \tTraining Loss: 0.4409063285477348\tValidation Loss: 0.4446622860276687\n",
            "Epoch 40  \tTraining Loss: 0.4400009766714702\tValidation Loss: 0.4437637195234951\n",
            "Epoch 41  \tTraining Loss: 0.4390935117968859\tValidation Loss: 0.4428622224727171\n",
            "Epoch 42  \tTraining Loss: 0.43818404115487314\tValidation Loss: 0.44196008741226256\n",
            "Epoch 43  \tTraining Loss: 0.43727251758489205\tValidation Loss: 0.4410549025980905\n",
            "Epoch 44  \tTraining Loss: 0.4363577298550211\tValidation Loss: 0.44014817672464374\n",
            "Epoch 45  \tTraining Loss: 0.4354408927441622\tValidation Loss: 0.4392394765795721\n",
            "Epoch 46  \tTraining Loss: 0.43452271408974724\tValidation Loss: 0.43832942279734805\n",
            "Epoch 47  \tTraining Loss: 0.4336034338246571\tValidation Loss: 0.43741781641838084\n",
            "Epoch 48  \tTraining Loss: 0.4326830516508903\tValidation Loss: 0.4365054167104606\n",
            "Epoch 49  \tTraining Loss: 0.43176187477250294\tValidation Loss: 0.43558941896999226\n",
            "Epoch 50  \tTraining Loss: 0.4308379506528068\tValidation Loss: 0.43467118108778274\n",
            "Epoch 51  \tTraining Loss: 0.42991166888939963\tValidation Loss: 0.4337509241495869\n",
            "Epoch 52  \tTraining Loss: 0.428983081615076\tValidation Loss: 0.432828364517572\n",
            "Epoch 53  \tTraining Loss: 0.4280524888701792\tValidation Loss: 0.4319035793437747\n",
            "Epoch 54  \tTraining Loss: 0.4271203463969871\tValidation Loss: 0.43097663765015837\n",
            "Epoch 55  \tTraining Loss: 0.42618638865120234\tValidation Loss: 0.4300480559391896\n",
            "Epoch 56  \tTraining Loss: 0.4252509474799466\tValidation Loss: 0.4291183386701177\n",
            "Epoch 57  \tTraining Loss: 0.424314598838022\tValidation Loss: 0.42818697978449116\n",
            "Epoch 58  \tTraining Loss: 0.42337771847203826\tValidation Loss: 0.4272541722273017\n",
            "Epoch 59  \tTraining Loss: 0.4224402268997953\tValidation Loss: 0.4263204541273726\n",
            "Epoch 60  \tTraining Loss: 0.4215014614127958\tValidation Loss: 0.4253850197519322\n",
            "Epoch 61  \tTraining Loss: 0.42056166830720654\tValidation Loss: 0.4244479583530851\n",
            "Epoch 62  \tTraining Loss: 0.419621525938314\tValidation Loss: 0.4235088817808493\n",
            "Epoch 63  \tTraining Loss: 0.41868041313554144\tValidation Loss: 0.42256926170653464\n",
            "Epoch 64  \tTraining Loss: 0.4177402197403607\tValidation Loss: 0.4216297366815058\n",
            "Epoch 65  \tTraining Loss: 0.4168000998141144\tValidation Loss: 0.4206892813926245\n",
            "Epoch 66  \tTraining Loss: 0.41585928458384297\tValidation Loss: 0.4197482707435665\n",
            "Epoch 67  \tTraining Loss: 0.41491855036513825\tValidation Loss: 0.41880596073056864\n",
            "Epoch 68  \tTraining Loss: 0.413978577361546\tValidation Loss: 0.41786206194199277\n",
            "Epoch 69  \tTraining Loss: 0.4130400450899515\tValidation Loss: 0.41691933687405114\n",
            "Epoch 70  \tTraining Loss: 0.41210227222878765\tValidation Loss: 0.41597396323368474\n",
            "Epoch 71  \tTraining Loss: 0.41116163754383495\tValidation Loss: 0.41502602601758426\n",
            "Epoch 72  \tTraining Loss: 0.41021941106156845\tValidation Loss: 0.4140780164116849\n",
            "Epoch 73  \tTraining Loss: 0.4092765633564972\tValidation Loss: 0.41312715347493956\n",
            "Epoch 74  \tTraining Loss: 0.4083268230266829\tValidation Loss: 0.4121638279758138\n",
            "Epoch 75  \tTraining Loss: 0.4073679729271208\tValidation Loss: 0.4111831821351704\n",
            "Epoch 76  \tTraining Loss: 0.4063939820249611\tValidation Loss: 0.410175280346891\n",
            "Epoch 77  \tTraining Loss: 0.4053928317175932\tValidation Loss: 0.40911316189709884\n",
            "Epoch 78  \tTraining Loss: 0.4043246329598917\tValidation Loss: 0.40795577453164183\n",
            "Epoch 79  \tTraining Loss: 0.40314475669915995\tValidation Loss: 0.4066988260329525\n",
            "Epoch 80  \tTraining Loss: 0.40183852644338475\tValidation Loss: 0.40528424180852307\n",
            "Epoch 81  \tTraining Loss: 0.40033127150317066\tValidation Loss: 0.4033127470782263\n",
            "Epoch 82  \tTraining Loss: 0.39831051316998156\tValidation Loss: 0.400257926193116\n",
            "Epoch 83  \tTraining Loss: 0.39511364824481876\tValidation Loss: 0.3957003898126781\n",
            "Epoch 84  \tTraining Loss: 0.3902521011241419\tValidation Loss: 0.391954731712515\n",
            "Epoch 85  \tTraining Loss: 0.3863132943699981\tValidation Loss: 0.3899705449331144\n",
            "Epoch 86  \tTraining Loss: 0.38421340357888406\tValidation Loss: 0.3886310300922752\n",
            "Epoch 87  \tTraining Loss: 0.38280890834540315\tValidation Loss: 0.3875141058473806\n",
            "Epoch 88  \tTraining Loss: 0.3816560226855519\tValidation Loss: 0.38647769186579767\n",
            "Epoch 89  \tTraining Loss: 0.3805986273853241\tValidation Loss: 0.38547265041057005\n",
            "Epoch 90  \tTraining Loss: 0.379581585861139\tValidation Loss: 0.3844815371383754\n",
            "Epoch 91  \tTraining Loss: 0.3785846789891427\tValidation Loss: 0.3835000776816758\n",
            "Epoch 92  \tTraining Loss: 0.37760138847171515\tValidation Loss: 0.38252661998758375\n",
            "Epoch 93  \tTraining Loss: 0.37662792022028835\tValidation Loss: 0.3815611341785969\n",
            "Epoch 94  \tTraining Loss: 0.3756629149509768\tValidation Loss: 0.3806037856234009\n",
            "Epoch 95  \tTraining Loss: 0.37470668189762063\tValidation Loss: 0.37965437927582263\n",
            "Epoch 96  \tTraining Loss: 0.37375969891191135\tValidation Loss: 0.37871238041477073\n",
            "Epoch 97  \tTraining Loss: 0.3728214167898011\tValidation Loss: 0.3777793962988748\n",
            "Epoch 98  \tTraining Loss: 0.37189256686400396\tValidation Loss: 0.3768553227785591\n",
            "Epoch 99  \tTraining Loss: 0.37097245620132974\tValidation Loss: 0.37593669274263697\n",
            "Epoch 100  \tTraining Loss: 0.37006037362677535\tValidation Loss: 0.37502619635204026\n",
            "Epoch 101  \tTraining Loss: 0.36915771393949237\tValidation Loss: 0.37412235158384555\n",
            "Epoch 102  \tTraining Loss: 0.36826183633322945\tValidation Loss: 0.37321941513811685\n",
            "Epoch 103  \tTraining Loss: 0.3673705113775454\tValidation Loss: 0.37232243423159084\n",
            "Epoch 104  \tTraining Loss: 0.3664864819199288\tValidation Loss: 0.37143004382535\n",
            "Epoch 105  \tTraining Loss: 0.3656073743154725\tValidation Loss: 0.37053228441930003\n",
            "Epoch 106  \tTraining Loss: 0.3647268672804672\tValidation Loss: 0.3696193174665022\n",
            "Epoch 107  \tTraining Loss: 0.36382633935339925\tValidation Loss: 0.36865453182117175\n",
            "Epoch 108  \tTraining Loss: 0.362861295500654\tValidation Loss: 0.3675364225314318\n",
            "Epoch 109  \tTraining Loss: 0.3617259905744314\tValidation Loss: 0.36598517033470696\n",
            "Epoch 110  \tTraining Loss: 0.3601286759791841\tValidation Loss: 0.3623225157604795\n",
            "Epoch 111  \tTraining Loss: 0.3563816414851839\tValidation Loss: 0.35400896276354543\n",
            "Epoch 112  \tTraining Loss: 0.34808402380264825\tValidation Loss: 0.3463954204948993\n",
            "Epoch 113  \tTraining Loss: 0.34043844735474493\tValidation Loss: 0.3431059831653356\n",
            "Epoch 114  \tTraining Loss: 0.33710340190212\tValidation Loss: 0.3414708530417665\n",
            "Epoch 115  \tTraining Loss: 0.33544001242899396\tValidation Loss: 0.340379028295548\n",
            "Epoch 116  \tTraining Loss: 0.3343320593182114\tValidation Loss: 0.3394673950873265\n",
            "Epoch 117  \tTraining Loss: 0.33341298084546617\tValidation Loss: 0.33862004123847683\n",
            "Epoch 118  \tTraining Loss: 0.33256359514592476\tValidation Loss: 0.33779998316860216\n",
            "Epoch 119  \tTraining Loss: 0.3317443711471905\tValidation Loss: 0.33699526951877706\n",
            "Epoch 120  \tTraining Loss: 0.33094274819252034\tValidation Loss: 0.33620224684133\n",
            "Epoch 121  \tTraining Loss: 0.33015418403850527\tValidation Loss: 0.33542050459792816\n",
            "Epoch 122  \tTraining Loss: 0.32937739430135404\tValidation Loss: 0.3346492088912953\n",
            "Epoch 123  \tTraining Loss: 0.32861181473718715\tValidation Loss: 0.33388837548704586\n",
            "Epoch 124  \tTraining Loss: 0.3278574258735916\tValidation Loss: 0.33313813163304345\n",
            "Epoch 125  \tTraining Loss: 0.3271139495743487\tValidation Loss: 0.33239908597681944\n",
            "Epoch 126  \tTraining Loss: 0.3263815752237541\tValidation Loss: 0.3316708038521047\n",
            "Epoch 127  \tTraining Loss: 0.3256601304957933\tValidation Loss: 0.3309531768453015\n",
            "Epoch 128  \tTraining Loss: 0.32494961273642664\tValidation Loss: 0.33024597792306065\n",
            "Epoch 129  \tTraining Loss: 0.32424995932294637\tValidation Loss: 0.3295485187354402\n",
            "Epoch 130  \tTraining Loss: 0.3235605244810727\tValidation Loss: 0.32886079677675617\n",
            "Epoch 131  \tTraining Loss: 0.3228812780143397\tValidation Loss: 0.3281829751392484\n",
            "Epoch 132  \tTraining Loss: 0.3222122356663404\tValidation Loss: 0.32751477524736167\n",
            "Epoch 133  \tTraining Loss: 0.3215530610881069\tValidation Loss: 0.3268560963983732\n",
            "Epoch 134  \tTraining Loss: 0.3209039021503009\tValidation Loss: 0.3262069361549692\n",
            "Epoch 135  \tTraining Loss: 0.3202642222331164\tValidation Loss: 0.3255674968587932\n",
            "Epoch 136  \tTraining Loss: 0.3196340453913359\tValidation Loss: 0.3249372978050929\n",
            "Epoch 137  \tTraining Loss: 0.31901324543471943\tValidation Loss: 0.32431586217390834\n",
            "Epoch 138  \tTraining Loss: 0.31840184695647233\tValidation Loss: 0.32370331061826363\n",
            "Epoch 139  \tTraining Loss: 0.3177995915334371\tValidation Loss: 0.32310027289091103\n",
            "Epoch 140  \tTraining Loss: 0.31720641342431233\tValidation Loss: 0.32250590628007875\n",
            "Epoch 141  \tTraining Loss: 0.3166219579076796\tValidation Loss: 0.32192012244198653\n",
            "Epoch 142  \tTraining Loss: 0.31604645171764384\tValidation Loss: 0.32134301869923565\n",
            "Epoch 143  \tTraining Loss: 0.3154796291990588\tValidation Loss: 0.3207744005954364\n",
            "Epoch 144  \tTraining Loss: 0.3149213442736238\tValidation Loss: 0.32021388493773756\n",
            "Epoch 145  \tTraining Loss: 0.31437141338284813\tValidation Loss: 0.3196617614483723\n",
            "Epoch 146  \tTraining Loss: 0.3138299827496432\tValidation Loss: 0.3191176703095907\n",
            "Epoch 147  \tTraining Loss: 0.31329645621691965\tValidation Loss: 0.31858133752763546\n",
            "Epoch 148  \tTraining Loss: 0.3127706292283547\tValidation Loss: 0.3180528176885997\n",
            "Epoch 149  \tTraining Loss: 0.3122526305009191\tValidation Loss: 0.3175318201326348\n",
            "Epoch 150  \tTraining Loss: 0.31174202744535734\tValidation Loss: 0.3170181424074684\n",
            "Epoch 151  \tTraining Loss: 0.31123885791867095\tValidation Loss: 0.31651177000220193\n",
            "Epoch 152  \tTraining Loss: 0.3107429688992671\tValidation Loss: 0.31601236148238426\n",
            "Epoch 153  \tTraining Loss: 0.31025438306970243\tValidation Loss: 0.3155196261628218\n",
            "Epoch 154  \tTraining Loss: 0.30977298544209964\tValidation Loss: 0.3150339730326214\n",
            "Epoch 155  \tTraining Loss: 0.3092986174664294\tValidation Loss: 0.31455521431246625\n",
            "Epoch 156  \tTraining Loss: 0.30883089934568775\tValidation Loss: 0.31408347475571813\n",
            "Epoch 157  \tTraining Loss: 0.30836993465785467\tValidation Loss: 0.3136182846987536\n",
            "Epoch 158  \tTraining Loss: 0.30791558127071333\tValidation Loss: 0.3131599836567367\n",
            "Epoch 159  \tTraining Loss: 0.3074677825867185\tValidation Loss: 0.3127082795439167\n",
            "Epoch 160  \tTraining Loss: 0.3070261044232248\tValidation Loss: 0.3122627418218909\n",
            "Epoch 161  \tTraining Loss: 0.3065905182845531\tValidation Loss: 0.31182313336908646\n",
            "Epoch 162  \tTraining Loss: 0.3061610470447732\tValidation Loss: 0.31138883825047237\n",
            "Epoch 163  \tTraining Loss: 0.3057369731480374\tValidation Loss: 0.31096036593733134\n",
            "Epoch 164  \tTraining Loss: 0.3053184531841\tValidation Loss: 0.31053683123016057\n",
            "Epoch 165  \tTraining Loss: 0.3049052128226577\tValidation Loss: 0.3101188330341531\n",
            "Epoch 166  \tTraining Loss: 0.3044973466744261\tValidation Loss: 0.30970624921007867\n",
            "Epoch 167  \tTraining Loss: 0.3040946847336912\tValidation Loss: 0.30929878153464185\n",
            "Epoch 168  \tTraining Loss: 0.30369663503731853\tValidation Loss: 0.3088957046102381\n",
            "Epoch 169  \tTraining Loss: 0.3033033526597804\tValidation Loss: 0.3084966605823968\n",
            "Epoch 170  \tTraining Loss: 0.30291483523060514\tValidation Loss: 0.30810225814732606\n",
            "Epoch 171  \tTraining Loss: 0.3025313901152937\tValidation Loss: 0.30771255084649085\n",
            "Epoch 172  \tTraining Loss: 0.3021524267735312\tValidation Loss: 0.30732771493884814\n",
            "Epoch 173  \tTraining Loss: 0.30177802131951537\tValidation Loss: 0.3069469827408685\n",
            "Epoch 174  \tTraining Loss: 0.30140753408986914\tValidation Loss: 0.3065707169511816\n",
            "Epoch 175  \tTraining Loss: 0.30104117147415377\tValidation Loss: 0.3061987895158401\n",
            "Epoch 176  \tTraining Loss: 0.3006785621997629\tValidation Loss: 0.30583114493903757\n",
            "Epoch 177  \tTraining Loss: 0.3003196629689193\tValidation Loss: 0.30546705930730295\n",
            "Epoch 178  \tTraining Loss: 0.2999635419936533\tValidation Loss: 0.3051046583306923\n",
            "Epoch 179  \tTraining Loss: 0.29961127322164727\tValidation Loss: 0.3047437464511171\n",
            "Epoch 180  \tTraining Loss: 0.29926137911989126\tValidation Loss: 0.304384713667403\n",
            "Epoch 181  \tTraining Loss: 0.2989128522975289\tValidation Loss: 0.3040253214282598\n",
            "Epoch 182  \tTraining Loss: 0.2985627816340139\tValidation Loss: 0.3036597170405859\n",
            "Epoch 183  \tTraining Loss: 0.29820389769326633\tValidation Loss: 0.3032892373471054\n",
            "Epoch 184  \tTraining Loss: 0.2978376578782676\tValidation Loss: 0.30290224312816816\n",
            "Epoch 185  \tTraining Loss: 0.29745247325957097\tValidation Loss: 0.30247418239203533\n",
            "Epoch 186  \tTraining Loss: 0.297026784567398\tValidation Loss: 0.30196910431825985\n",
            "Epoch 187  \tTraining Loss: 0.2965477723133139\tValidation Loss: 0.3013420497661846\n",
            "Epoch 188  \tTraining Loss: 0.2959794517872663\tValidation Loss: 0.30031168810243236\n",
            "Epoch 189  \tTraining Loss: 0.29512458892903914\tValidation Loss: 0.29794741252686535\n",
            "Epoch 190  \tTraining Loss: 0.2930947193476868\tValidation Loss: 0.2903655503375099\n",
            "Epoch 191  \tTraining Loss: 0.2857075063073514\tValidation Loss: 0.27499360013291374\n",
            "Epoch 192  \tTraining Loss: 0.2700004764750307\tValidation Loss: 0.2678783059340254\n",
            "Epoch 193  \tTraining Loss: 0.2627827819684379\tValidation Loss: 0.26594466387867055\n",
            "Epoch 194  \tTraining Loss: 0.2608043939273886\tValidation Loss: 0.26520792318403585\n",
            "Epoch 195  \tTraining Loss: 0.26005143821839993\tValidation Loss: 0.2647483536086536\n",
            "Epoch 196  \tTraining Loss: 0.259588624888154\tValidation Loss: 0.26435605396018974\n",
            "Epoch 197  \tTraining Loss: 0.2591988916911927\tValidation Loss: 0.26398322373623084\n",
            "Epoch 198  \tTraining Loss: 0.25883116965781516\tValidation Loss: 0.26361975104866586\n",
            "Epoch 199  \tTraining Loss: 0.2584735666258112\tValidation Loss: 0.263262880598646\n",
            "Epoch 200  \tTraining Loss: 0.25812287317101584\tValidation Loss: 0.262911966144481\n",
            "Epoch 201  \tTraining Loss: 0.25777819185068346\tValidation Loss: 0.2625670283165287\n",
            "Epoch 202  \tTraining Loss: 0.25743922241934813\tValidation Loss: 0.26222758032916504\n",
            "Epoch 203  \tTraining Loss: 0.25710572867557535\tValidation Loss: 0.2618938432603724\n",
            "Epoch 204  \tTraining Loss: 0.25677746466988954\tValidation Loss: 0.26156531050737974\n",
            "Epoch 205  \tTraining Loss: 0.25645422018305775\tValidation Loss: 0.2612419235256832\n",
            "Epoch 206  \tTraining Loss: 0.2561359027134254\tValidation Loss: 0.2609236061996937\n",
            "Epoch 207  \tTraining Loss: 0.2558224114898485\tValidation Loss: 0.2606102204444244\n",
            "Epoch 208  \tTraining Loss: 0.2555135552055597\tValidation Loss: 0.26030154772026987\n",
            "Epoch 209  \tTraining Loss: 0.2552090921919164\tValidation Loss: 0.2599971507334856\n",
            "Epoch 210  \tTraining Loss: 0.25490899039230336\tValidation Loss: 0.2596972989291461\n",
            "Epoch 211  \tTraining Loss: 0.25461323537867425\tValidation Loss: 0.2594020499199736\n",
            "Epoch 212  \tTraining Loss: 0.2543216207303062\tValidation Loss: 0.2591111106229058\n",
            "Epoch 213  \tTraining Loss: 0.2540341506392702\tValidation Loss: 0.25882439359413084\n",
            "Epoch 214  \tTraining Loss: 0.2537506605388366\tValidation Loss: 0.25854174264693214\n",
            "Epoch 215  \tTraining Loss: 0.25347100160131036\tValidation Loss: 0.2582631106284256\n",
            "Epoch 216  \tTraining Loss: 0.25319512753505063\tValidation Loss: 0.2579882318617196\n",
            "Epoch 217  \tTraining Loss: 0.25292292740621514\tValidation Loss: 0.2577170279508818\n",
            "Epoch 218  \tTraining Loss: 0.2526543181417047\tValidation Loss: 0.2574494160226696\n",
            "Epoch 219  \tTraining Loss: 0.252389177561973\tValidation Loss: 0.257185343059832\n",
            "Epoch 220  \tTraining Loss: 0.2521274161834666\tValidation Loss: 0.2569247315705698\n",
            "Epoch 221  \tTraining Loss: 0.25186897409037234\tValidation Loss: 0.2566674614626934\n",
            "Epoch 222  \tTraining Loss: 0.25161375961710675\tValidation Loss: 0.2564132345816694\n",
            "Epoch 223  \tTraining Loss: 0.2513616781119867\tValidation Loss: 0.2561620622421803\n",
            "Epoch 224  \tTraining Loss: 0.25111257827961314\tValidation Loss: 0.2559136683248965\n",
            "Epoch 225  \tTraining Loss: 0.25086635190708295\tValidation Loss: 0.2556682953301308\n",
            "Epoch 226  \tTraining Loss: 0.250622951022442\tValidation Loss: 0.25542609643681957\n",
            "Epoch 227  \tTraining Loss: 0.25038233352059663\tValidation Loss: 0.25518691764000334\n",
            "Epoch 228  \tTraining Loss: 0.2501445182205218\tValidation Loss: 0.25495069185954516\n",
            "Epoch 229  \tTraining Loss: 0.24990945344288684\tValidation Loss: 0.25471723838719645\n",
            "Epoch 230  \tTraining Loss: 0.24967707339342954\tValidation Loss: 0.254486439613132\n",
            "Epoch 231  \tTraining Loss: 0.24944734191645881\tValidation Loss: 0.2542584343325857\n",
            "Epoch 232  \tTraining Loss: 0.2492201835162948\tValidation Loss: 0.2540330844294091\n",
            "Epoch 233  \tTraining Loss: 0.24899555727521686\tValidation Loss: 0.25381027168818854\n",
            "Epoch 234  \tTraining Loss: 0.24877336807751516\tValidation Loss: 0.2535898703916114\n",
            "Epoch 235  \tTraining Loss: 0.2485535570598786\tValidation Loss: 0.25337202998409497\n",
            "Epoch 236  \tTraining Loss: 0.24833610836697553\tValidation Loss: 0.2531566216954869\n",
            "Epoch 237  \tTraining Loss: 0.24812099837684598\tValidation Loss: 0.2529435207393502\n",
            "Epoch 238  \tTraining Loss: 0.24790818792973934\tValidation Loss: 0.25273274805711543\n",
            "Epoch 239  \tTraining Loss: 0.24769756629475445\tValidation Loss: 0.2525242240920224\n",
            "Epoch 240  \tTraining Loss: 0.2474890540177743\tValidation Loss: 0.252317463814946\n",
            "Epoch 241  \tTraining Loss: 0.24728256080700609\tValidation Loss: 0.2521126251215407\n",
            "Epoch 242  \tTraining Loss: 0.247078093462745\tValidation Loss: 0.25190980895755455\n",
            "Epoch 243  \tTraining Loss: 0.24687560275384102\tValidation Loss: 0.2517088894240016\n",
            "Epoch 244  \tTraining Loss: 0.24667507902095925\tValidation Loss: 0.251510007700397\n",
            "Epoch 245  \tTraining Loss: 0.24647652225275662\tValidation Loss: 0.25131284475004056\n",
            "Epoch 246  \tTraining Loss: 0.24627991142028383\tValidation Loss: 0.25111769831348574\n",
            "Epoch 247  \tTraining Loss: 0.2460852595695485\tValidation Loss: 0.2509246985834862\n",
            "Epoch 248  \tTraining Loss: 0.24589246622668748\tValidation Loss: 0.25073361944076067\n",
            "Epoch 249  \tTraining Loss: 0.24570151478958496\tValidation Loss: 0.25054417906902265\n",
            "Epoch 250  \tTraining Loss: 0.24551234950175788\tValidation Loss: 0.25035671075792215\n",
            "Epoch 251  \tTraining Loss: 0.24532495470581606\tValidation Loss: 0.2501710016371513\n",
            "Epoch 252  \tTraining Loss: 0.24513927869417224\tValidation Loss: 0.24998690210579358\n",
            "Epoch 253  \tTraining Loss: 0.24495530401626914\tValidation Loss: 0.2498044652640512\n",
            "Epoch 254  \tTraining Loss: 0.2447730381608143\tValidation Loss: 0.24962381378385134\n",
            "Epoch 255  \tTraining Loss: 0.24459246165867918\tValidation Loss: 0.24944480703540833\n",
            "Epoch 256  \tTraining Loss: 0.2444134891000501\tValidation Loss: 0.24926746650105147\n",
            "Epoch 257  \tTraining Loss: 0.244236117078939\tValidation Loss: 0.24909176430771346\n",
            "Epoch 258  \tTraining Loss: 0.24406033279427053\tValidation Loss: 0.24891770593524992\n",
            "Epoch 259  \tTraining Loss: 0.2438860942888393\tValidation Loss: 0.24874523055407335\n",
            "Epoch 260  \tTraining Loss: 0.24371339523281757\tValidation Loss: 0.2485743420568226\n",
            "Epoch 261  \tTraining Loss: 0.24354225631243756\tValidation Loss: 0.24840495430379209\n",
            "Epoch 262  \tTraining Loss: 0.2433726788705292\tValidation Loss: 0.24823699032686422\n",
            "Epoch 263  \tTraining Loss: 0.24320460699878382\tValidation Loss: 0.24807048915265675\n",
            "Epoch 264  \tTraining Loss: 0.24303794122133568\tValidation Loss: 0.24790551202297526\n",
            "Epoch 265  \tTraining Loss: 0.242872615797319\tValidation Loss: 0.2477416022133903\n",
            "Epoch 266  \tTraining Loss: 0.24270854913709536\tValidation Loss: 0.2475791795602484\n",
            "Epoch 267  \tTraining Loss: 0.24254585232646195\tValidation Loss: 0.24741823334870353\n",
            "Epoch 268  \tTraining Loss: 0.2423844159060759\tValidation Loss: 0.24725860860150797\n",
            "Epoch 269  \tTraining Loss: 0.2422242713746643\tValidation Loss: 0.24710030580199627\n",
            "Epoch 270  \tTraining Loss: 0.24206542772452955\tValidation Loss: 0.24694303835339682\n",
            "Epoch 271  \tTraining Loss: 0.24190783262058257\tValidation Loss: 0.24678713601845995\n",
            "Epoch 272  \tTraining Loss: 0.24175150308485088\tValidation Loss: 0.24663239180166457\n",
            "Epoch 273  \tTraining Loss: 0.24159622098219538\tValidation Loss: 0.24647879428010896\n",
            "Epoch 274  \tTraining Loss: 0.2414420788202831\tValidation Loss: 0.2463263864142121\n",
            "Epoch 275  \tTraining Loss: 0.24128916776957676\tValidation Loss: 0.24617527040975923\n",
            "Epoch 276  \tTraining Loss: 0.24113747052225884\tValidation Loss: 0.24602524493132308\n",
            "Epoch 277  \tTraining Loss: 0.2409869329697373\tValidation Loss: 0.24587630350545342\n",
            "Epoch 278  \tTraining Loss: 0.24083760717861308\tValidation Loss: 0.24572844605901156\n",
            "Epoch 279  \tTraining Loss: 0.24068935292276392\tValidation Loss: 0.24558074193602977\n",
            "Epoch 280  \tTraining Loss: 0.24054190022581856\tValidation Loss: 0.24543324816969825\n",
            "Epoch 281  \tTraining Loss: 0.24039560972667856\tValidation Loss: 0.24528624209454936\n",
            "Epoch 282  \tTraining Loss: 0.2402504633096237\tValidation Loss: 0.24514020545829412\n",
            "Epoch 283  \tTraining Loss: 0.24010642984274008\tValidation Loss: 0.24499529033961176\n",
            "Epoch 284  \tTraining Loss: 0.23996339848682807\tValidation Loss: 0.2448507862973152\n",
            "Epoch 285  \tTraining Loss: 0.239821477630449\tValidation Loss: 0.24470742398884823\n",
            "Epoch 286  \tTraining Loss: 0.23968058208165507\tValidation Loss: 0.24456440985908512\n",
            "Epoch 287  \tTraining Loss: 0.239540284046024\tValidation Loss: 0.24442186149306613\n",
            "Epoch 288  \tTraining Loss: 0.23940079457013141\tValidation Loss: 0.2442801710951138\n",
            "Epoch 289  \tTraining Loss: 0.23926203769545779\tValidation Loss: 0.24413784213072023\n",
            "Epoch 290  \tTraining Loss: 0.23912323578080055\tValidation Loss: 0.2439954332963303\n",
            "Epoch 291  \tTraining Loss: 0.23898415414098453\tValidation Loss: 0.2438510329295777\n",
            "Epoch 292  \tTraining Loss: 0.2388435510810129\tValidation Loss: 0.24370154890191356\n",
            "Epoch 293  \tTraining Loss: 0.2387017320284666\tValidation Loss: 0.24354636810043767\n",
            "Epoch 294  \tTraining Loss: 0.23855588036980357\tValidation Loss: 0.24338331129288335\n",
            "Epoch 295  \tTraining Loss: 0.23840558702879777\tValidation Loss: 0.2432098149989956\n",
            "Epoch 296  \tTraining Loss: 0.23824738134798903\tValidation Loss: 0.24303150683763863\n",
            "Epoch 297  \tTraining Loss: 0.23807944800293082\tValidation Loss: 0.2428401665785304\n",
            "Epoch 298  \tTraining Loss: 0.23789480040450264\tValidation Loss: 0.24262279702839765\n",
            "Epoch 299  \tTraining Loss: 0.23768676557428456\tValidation Loss: 0.24236776955363837\n",
            "Epoch 300  \tTraining Loss: 0.2374441165977077\tValidation Loss: 0.24199911706769425\n",
            "Epoch 301  \tTraining Loss: 0.2371190360364851\tValidation Loss: 0.2414503203615154\n",
            "Epoch 302  \tTraining Loss: 0.23664635990797192\tValidation Loss: 0.240504087625262\n",
            "Epoch 303  \tTraining Loss: 0.23576932650994994\tValidation Loss: 0.23860790828836467\n",
            "Epoch 304  \tTraining Loss: 0.2339505231306391\tValidation Loss: 0.23562689985046156\n",
            "Epoch 305  \tTraining Loss: 0.23116349034681946\tValidation Loss: 0.23346759923443738\n",
            "Epoch 306  \tTraining Loss: 0.22918892519497983\tValidation Loss: 0.23202165285073756\n",
            "Epoch 307  \tTraining Loss: 0.22787025605719913\tValidation Loss: 0.22798800970002345\n",
            "Epoch 308  \tTraining Loss: 0.22406028835326122\tValidation Loss: 0.2195911544157873\n",
            "Epoch 309  \tTraining Loss: 0.21620636008387717\tValidation Loss: 0.21713669782950082\n",
            "Epoch 310  \tTraining Loss: 0.21397756602153664\tValidation Loss: 0.21634693061498922\n",
            "Epoch 311  \tTraining Loss: 0.2132478278776737\tValidation Loss: 0.2158441025143227\n",
            "Epoch 312  \tTraining Loss: 0.21274973883866777\tValidation Loss: 0.21540716880870467\n",
            "Epoch 313  \tTraining Loss: 0.21229978314888018\tValidation Loss: 0.21499604579210388\n",
            "Epoch 314  \tTraining Loss: 0.21186985340619077\tValidation Loss: 0.21460062100766555\n",
            "Epoch 315  \tTraining Loss: 0.2114548605089315\tValidation Loss: 0.2142184313974726\n",
            "Epoch 316  \tTraining Loss: 0.21105351571751635\tValidation Loss: 0.21384837432336584\n",
            "Epoch 317  \tTraining Loss: 0.2106649969213102\tValidation Loss: 0.21348956790000603\n",
            "Epoch 318  \tTraining Loss: 0.21028867687365096\tValidation Loss: 0.21314172695641756\n",
            "Epoch 319  \tTraining Loss: 0.20992395760168828\tValidation Loss: 0.21280376770224016\n",
            "Epoch 320  \tTraining Loss: 0.20957031887275399\tValidation Loss: 0.21247577921448285\n",
            "Epoch 321  \tTraining Loss: 0.20922728573673416\tValidation Loss: 0.21215707306720166\n",
            "Epoch 322  \tTraining Loss: 0.20889437270711247\tValidation Loss: 0.211847141706549\n",
            "Epoch 323  \tTraining Loss: 0.20857113514233377\tValidation Loss: 0.21154587205072456\n",
            "Epoch 324  \tTraining Loss: 0.2082572209603876\tValidation Loss: 0.21125267671709053\n",
            "Epoch 325  \tTraining Loss: 0.20795214628975675\tValidation Loss: 0.21096735315777373\n",
            "Epoch 326  \tTraining Loss: 0.20765558528940195\tValidation Loss: 0.2106895852741397\n",
            "Epoch 327  \tTraining Loss: 0.20736717396070006\tValidation Loss: 0.2104190807991803\n",
            "Epoch 328  \tTraining Loss: 0.2070865969005587\tValidation Loss: 0.21015555781570708\n",
            "Epoch 329  \tTraining Loss: 0.2068135360998984\tValidation Loss: 0.20989885186073526\n",
            "Epoch 330  \tTraining Loss: 0.20654766778897735\tValidation Loss: 0.20964867829599154\n",
            "Epoch 331  \tTraining Loss: 0.20628871772119928\tValidation Loss: 0.20940438974165226\n",
            "Epoch 332  \tTraining Loss: 0.20603635829412073\tValidation Loss: 0.20916627726920847\n",
            "Epoch 333  \tTraining Loss: 0.20579038680042874\tValidation Loss: 0.20893383376524655\n",
            "Epoch 334  \tTraining Loss: 0.20555053783617372\tValidation Loss: 0.2087070313701998\n",
            "Epoch 335  \tTraining Loss: 0.2053165203720626\tValidation Loss: 0.20848546413376287\n",
            "Epoch 336  \tTraining Loss: 0.20508813042358184\tValidation Loss: 0.2082689370852141\n",
            "Epoch 337  \tTraining Loss: 0.20486514468663272\tValidation Loss: 0.2080573012868868\n",
            "Epoch 338  \tTraining Loss: 0.20464738888930667\tValidation Loss: 0.20785031245499422\n",
            "Epoch 339  \tTraining Loss: 0.2044346599303532\tValidation Loss: 0.2076477720270024\n",
            "Epoch 340  \tTraining Loss: 0.2042267510949547\tValidation Loss: 0.20744967624539137\n",
            "Epoch 341  \tTraining Loss: 0.20402348388808322\tValidation Loss: 0.2072557638736306\n",
            "Epoch 342  \tTraining Loss: 0.20382466354558917\tValidation Loss: 0.20706570922945092\n",
            "Epoch 343  \tTraining Loss: 0.20363012687720086\tValidation Loss: 0.20687938865189678\n",
            "Epoch 344  \tTraining Loss: 0.2034397816180596\tValidation Loss: 0.20669696406379734\n",
            "Epoch 345  \tTraining Loss: 0.2032533591309524\tValidation Loss: 0.2065181988444592\n",
            "Epoch 346  \tTraining Loss: 0.2030707316186978\tValidation Loss: 0.20634257950336293\n",
            "Epoch 347  \tTraining Loss: 0.20289174844126948\tValidation Loss: 0.20617034994435185\n",
            "Epoch 348  \tTraining Loss: 0.20271622805164555\tValidation Loss: 0.20600144460504377\n",
            "Epoch 349  \tTraining Loss: 0.20254407883922032\tValidation Loss: 0.2058354274205832\n",
            "Epoch 350  \tTraining Loss: 0.20237508099259605\tValidation Loss: 0.20567218742548599\n",
            "Epoch 351  \tTraining Loss: 0.2022091795814626\tValidation Loss: 0.20551191668620356\n",
            "Epoch 352  \tTraining Loss: 0.2020462724801023\tValidation Loss: 0.2053545222797202\n",
            "Epoch 353  \tTraining Loss: 0.201886244858948\tValidation Loss: 0.20519975754314054\n",
            "Epoch 354  \tTraining Loss: 0.20172900861467843\tValidation Loss: 0.20504738180391038\n",
            "Epoch 355  \tTraining Loss: 0.20157442416578217\tValidation Loss: 0.2048975266158714\n",
            "Epoch 356  \tTraining Loss: 0.20142238691613054\tValidation Loss: 0.20475002146270083\n",
            "Epoch 357  \tTraining Loss: 0.20127283791004413\tValidation Loss: 0.204604772366269\n",
            "Epoch 358  \tTraining Loss: 0.20112566347637317\tValidation Loss: 0.20446172613537814\n",
            "Epoch 359  \tTraining Loss: 0.20098079532975197\tValidation Loss: 0.2043208491720846\n",
            "Epoch 360  \tTraining Loss: 0.20083813189002941\tValidation Loss: 0.20418206202291805\n",
            "Epoch 361  \tTraining Loss: 0.20069759728431652\tValidation Loss: 0.20404519705228705\n",
            "Epoch 362  \tTraining Loss: 0.200559116004986\tValidation Loss: 0.20391002863584956\n",
            "Epoch 363  \tTraining Loss: 0.200422619432688\tValidation Loss: 0.20377680757022085\n",
            "Epoch 364  \tTraining Loss: 0.2002880553938371\tValidation Loss: 0.20364532963344031\n",
            "Epoch 365  \tTraining Loss: 0.20015534756457531\tValidation Loss: 0.2035155673553647\n",
            "Epoch 366  \tTraining Loss: 0.2000244316043773\tValidation Loss: 0.2033874446417543\n",
            "Epoch 367  \tTraining Loss: 0.19989523875345022\tValidation Loss: 0.20326103992319805\n",
            "Epoch 368  \tTraining Loss: 0.19976770609728375\tValidation Loss: 0.20313611909957913\n",
            "Epoch 369  \tTraining Loss: 0.19964171479340123\tValidation Loss: 0.20301269013235593\n",
            "Epoch 370  \tTraining Loss: 0.19951725164512615\tValidation Loss: 0.20289072223320556\n",
            "Epoch 371  \tTraining Loss: 0.1993942912105269\tValidation Loss: 0.20277015016087666\n",
            "Epoch 372  \tTraining Loss: 0.19927277985340278\tValidation Loss: 0.20265098437495171\n",
            "Epoch 373  \tTraining Loss: 0.199152685500242\tValidation Loss: 0.2025331416200854\n",
            "Epoch 374  \tTraining Loss: 0.19903395694902992\tValidation Loss: 0.2024166387255176\n",
            "Epoch 375  \tTraining Loss: 0.19891650488585608\tValidation Loss: 0.2023013635138536\n",
            "Epoch 376  \tTraining Loss: 0.19880029812925312\tValidation Loss: 0.20218732502409817\n",
            "Epoch 377  \tTraining Loss: 0.19868530230603207\tValidation Loss: 0.20207436712364865\n",
            "Epoch 378  \tTraining Loss: 0.19857149512371708\tValidation Loss: 0.2019620350617955\n",
            "Epoch 379  \tTraining Loss: 0.19845880697611037\tValidation Loss: 0.2018510434837382\n",
            "Epoch 380  \tTraining Loss: 0.198347220659338\tValidation Loss: 0.20174114875980514\n",
            "Epoch 381  \tTraining Loss: 0.19823674459291962\tValidation Loss: 0.20163236876470722\n",
            "Epoch 382  \tTraining Loss: 0.19812735797951783\tValidation Loss: 0.2015244398727554\n",
            "Epoch 383  \tTraining Loss: 0.19801898732576606\tValidation Loss: 0.2014176276931776\n",
            "Epoch 384  \tTraining Loss: 0.19791161748355654\tValidation Loss: 0.201311941027459\n",
            "Epoch 385  \tTraining Loss: 0.19780519936559307\tValidation Loss: 0.20120717487137338\n",
            "Epoch 386  \tTraining Loss: 0.19769971949737275\tValidation Loss: 0.2011032577592311\n",
            "Epoch 387  \tTraining Loss: 0.1975951259942994\tValidation Loss: 0.2010004982254052\n",
            "Epoch 388  \tTraining Loss: 0.19749140968008716\tValidation Loss: 0.20089839241259497\n",
            "Epoch 389  \tTraining Loss: 0.19738856446295\tValidation Loss: 0.20079730523532544\n",
            "Epoch 390  \tTraining Loss: 0.19728652671329502\tValidation Loss: 0.20069704134010152\n",
            "Epoch 391  \tTraining Loss: 0.1971852902743452\tValidation Loss: 0.20059751849853003\n",
            "Epoch 392  \tTraining Loss: 0.19708486025636754\tValidation Loss: 0.2004986964664126\n",
            "Epoch 393  \tTraining Loss: 0.19698521072076774\tValidation Loss: 0.20040064952953768\n",
            "Epoch 394  \tTraining Loss: 0.1968863190152946\tValidation Loss: 0.2003033699514176\n",
            "Epoch 395  \tTraining Loss: 0.19678814179607673\tValidation Loss: 0.2002067991199133\n",
            "Epoch 396  \tTraining Loss: 0.1966907031614056\tValidation Loss: 0.20011087648354658\n",
            "Epoch 397  \tTraining Loss: 0.19659397258887643\tValidation Loss: 0.20001591732506116\n",
            "Epoch 398  \tTraining Loss: 0.19649792200102106\tValidation Loss: 0.19992154592302774\n",
            "Epoch 399  \tTraining Loss: 0.19640251327804076\tValidation Loss: 0.199827758237317\n",
            "Epoch 400  \tTraining Loss: 0.1963077304055683\tValidation Loss: 0.19973459556253859\n",
            "Validation loss list: [0.8731200521874258, 0.8731200521874258, 0.8731200521874258, 0.8731200521874258, 0.6526048988345521, 0.6526048988345521, 0.6526048988345521, 0.6526048988345521, 0.20016177270973023, 0.20016177270973023, 0.20016177270973023, 0.20016177270973023]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Autoencoder"
      ],
      "metadata": {
        "id": "hZucsOLkLBcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_autoencoder = NeuralNetwork(nn_architecture,\n",
        "                                lr = 1e-3,\n",
        "                                seed = 8,\n",
        "                                batch_size = 400,\n",
        "                                epochs = 500,\n",
        "                                loss_function='mean_squared_error')\n",
        "\n",
        "# Run final model\n",
        "per_epoch_loss_train, per_epoch_loss_val=final_autoencoder.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B97BfppLANo",
        "outputId": "c8c55d67-f51e-440b-beee-8310bcfab48b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1  \tTraining Loss: 0.01599011130627455\tValidation Loss: 0.06089715252004367\n",
            "Epoch 2  \tTraining Loss: 0.015748153697381844\tValidation Loss: 0.05991095129431239\n",
            "Epoch 3  \tTraining Loss: 0.01549736658186733\tValidation Loss: 0.058887419921102316\n",
            "Epoch 4  \tTraining Loss: 0.015237221392125378\tValidation Loss: 0.05783082292895255\n",
            "Epoch 5  \tTraining Loss: 0.014968807596973352\tValidation Loss: 0.056750314397978625\n",
            "Epoch 6  \tTraining Loss: 0.014692404554937128\tValidation Loss: 0.05563590219602768\n",
            "Epoch 7  \tTraining Loss: 0.014406874952601368\tValidation Loss: 0.05450065526724745\n",
            "Epoch 8  \tTraining Loss: 0.0141151496433577\tValidation Loss: 0.053353580514096846\n",
            "Epoch 9  \tTraining Loss: 0.01382017447897612\tValidation Loss: 0.052207918783569375\n",
            "Epoch 10  \tTraining Loss: 0.013525393641657831\tValidation Loss: 0.05108503219732496\n",
            "Epoch 11  \tTraining Loss: 0.013236259997758338\tValidation Loss: 0.04999712327668758\n",
            "Epoch 12  \tTraining Loss: 0.012956155798196594\tValidation Loss: 0.04896717487524577\n",
            "Epoch 13  \tTraining Loss: 0.012689513666140675\tValidation Loss: 0.048002066170044655\n",
            "Epoch 14  \tTraining Loss: 0.012439457931958511\tValidation Loss: 0.047101397883227404\n",
            "Epoch 15  \tTraining Loss: 0.012207788421486157\tValidation Loss: 0.04626927859864001\n",
            "Epoch 16  \tTraining Loss: 0.011994262803924608\tValidation Loss: 0.04550422710446339\n",
            "Epoch 17  \tTraining Loss: 0.011797539312582369\tValidation Loss: 0.04480202324818164\n",
            "Epoch 18  \tTraining Loss: 0.0116169952717486\tValidation Loss: 0.044155832256245094\n",
            "Epoch 19  \tTraining Loss: 0.011451293898467515\tValidation Loss: 0.04356195803622998\n",
            "Epoch 20  \tTraining Loss: 0.011299041849657748\tValidation Loss: 0.043022407196829295\n",
            "Epoch 21  \tTraining Loss: 0.01116088186554012\tValidation Loss: 0.04253099712292381\n",
            "Epoch 22  \tTraining Loss: 0.011034954460252691\tValidation Loss: 0.04207689384162612\n",
            "Epoch 23  \tTraining Loss: 0.010918353428533305\tValidation Loss: 0.04165152211911397\n",
            "Epoch 24  \tTraining Loss: 0.010808214931958446\tValidation Loss: 0.041243584347957475\n",
            "Epoch 25  \tTraining Loss: 0.010701535292298286\tValidation Loss: 0.04084878931967831\n",
            "Epoch 26  \tTraining Loss: 0.010597956239330355\tValidation Loss: 0.04047171091558632\n",
            "Epoch 27  \tTraining Loss: 0.010498644789346671\tValidation Loss: 0.04011479104785955\n",
            "Epoch 28  \tTraining Loss: 0.01040487998671688\tValidation Loss: 0.03978580837375681\n",
            "Epoch 29  \tTraining Loss: 0.010318365383078605\tValidation Loss: 0.03948913269829292\n",
            "Epoch 30  \tTraining Loss: 0.01024042812760318\tValidation Loss: 0.03922522095799216\n",
            "Epoch 31  \tTraining Loss: 0.010171092594773022\tValidation Loss: 0.038988166846858414\n",
            "Epoch 32  \tTraining Loss: 0.010108813117134356\tValidation Loss: 0.03876890336144587\n",
            "Epoch 33  \tTraining Loss: 0.010051170617559323\tValidation Loss: 0.03855673793027102\n",
            "Epoch 34  \tTraining Loss: 0.009995695337767159\tValidation Loss: 0.03834537939489865\n",
            "Epoch 35  \tTraining Loss: 0.00994015999872163\tValidation Loss: 0.03813039868082079\n",
            "Epoch 36  \tTraining Loss: 0.009883862474781069\tValidation Loss: 0.03791189098191165\n",
            "Epoch 37  \tTraining Loss: 0.009827540937894319\tValidation Loss: 0.037700963390448504\n",
            "Epoch 38  \tTraining Loss: 0.009772227301619895\tValidation Loss: 0.0374995836670037\n",
            "Epoch 39  \tTraining Loss: 0.009719223892837537\tValidation Loss: 0.037308685303807945\n",
            "Epoch 40  \tTraining Loss: 0.009669276142829081\tValidation Loss: 0.03713326135482981\n",
            "Epoch 41  \tTraining Loss: 0.00962310840791489\tValidation Loss: 0.03697069140418991\n",
            "Epoch 42  \tTraining Loss: 0.009580136878512158\tValidation Loss: 0.03681852226686371\n",
            "Epoch 43  \tTraining Loss: 0.009539842057031896\tValidation Loss: 0.03666736214820239\n",
            "Epoch 44  \tTraining Loss: 0.009500111663363143\tValidation Loss: 0.03650567542805109\n",
            "Epoch 45  \tTraining Loss: 0.009457692388325629\tValidation Loss: 0.03632517261625625\n",
            "Epoch 46  \tTraining Loss: 0.009410968624473463\tValidation Loss: 0.0361375697774538\n",
            "Epoch 47  \tTraining Loss: 0.009361977216816125\tValidation Loss: 0.03595453285785064\n",
            "Epoch 48  \tTraining Loss: 0.00931338445850286\tValidation Loss: 0.0357877901403312\n",
            "Epoch 49  \tTraining Loss: 0.009268775505805836\tValidation Loss: 0.035638092075995546\n",
            "Epoch 50  \tTraining Loss: 0.009228654907237293\tValidation Loss: 0.03550160285557343\n",
            "Epoch 51  \tTraining Loss: 0.009192083453517328\tValidation Loss: 0.03537548639585463\n",
            "Epoch 52  \tTraining Loss: 0.00915820433465003\tValidation Loss: 0.035255730891971856\n",
            "Epoch 53  \tTraining Loss: 0.009126209098017757\tValidation Loss: 0.0351396600120269\n",
            "Epoch 54  \tTraining Loss: 0.009095240787226613\tValidation Loss: 0.03502828309431287\n",
            "Epoch 55  \tTraining Loss: 0.009065087416297276\tValidation Loss: 0.03491907480007982\n",
            "Epoch 56  \tTraining Loss: 0.009035304616759202\tValidation Loss: 0.03481087295175264\n",
            "Epoch 57  \tTraining Loss: 0.009006098336117044\tValidation Loss: 0.03470528547490093\n",
            "Epoch 58  \tTraining Loss: 0.008977204967583257\tValidation Loss: 0.034603612115240844\n",
            "Epoch 59  \tTraining Loss: 0.0089492259707322\tValidation Loss: 0.03450531397507773\n",
            "Epoch 60  \tTraining Loss: 0.008922544010258474\tValidation Loss: 0.03441200213555056\n",
            "Epoch 61  \tTraining Loss: 0.00889749556459869\tValidation Loss: 0.034324212834914804\n",
            "Epoch 62  \tTraining Loss: 0.00887401282640334\tValidation Loss: 0.03424280472542962\n",
            "Epoch 63  \tTraining Loss: 0.008852158140407611\tValidation Loss: 0.03416637965797673\n",
            "Epoch 64  \tTraining Loss: 0.00883165831662999\tValidation Loss: 0.03409293728558604\n",
            "Epoch 65  \tTraining Loss: 0.00881194946925745\tValidation Loss: 0.03402117178451492\n",
            "Epoch 66  \tTraining Loss: 0.008792752176432871\tValidation Loss: 0.033949103348562985\n",
            "Epoch 67  \tTraining Loss: 0.008773432643628676\tValidation Loss: 0.033874046765465904\n",
            "Epoch 68  \tTraining Loss: 0.008753749002811145\tValidation Loss: 0.03379388024951489\n",
            "Epoch 69  \tTraining Loss: 0.008732708091752079\tValidation Loss: 0.033703289957385085\n",
            "Epoch 70  \tTraining Loss: 0.008709230124760263\tValidation Loss: 0.033607259617432515\n",
            "Epoch 71  \tTraining Loss: 0.008684125513594041\tValidation Loss: 0.033507001314382584\n",
            "Epoch 72  \tTraining Loss: 0.008657701462775056\tValidation Loss: 0.03340492251529776\n",
            "Epoch 73  \tTraining Loss: 0.008630277586763916\tValidation Loss: 0.033305449098749655\n",
            "Epoch 74  \tTraining Loss: 0.008603455655157159\tValidation Loss: 0.03321158942139592\n",
            "Epoch 75  \tTraining Loss: 0.008577809465374004\tValidation Loss: 0.0331230567075225\n",
            "Epoch 76  \tTraining Loss: 0.008553172241670178\tValidation Loss: 0.033041260553245314\n",
            "Epoch 77  \tTraining Loss: 0.008530093278627604\tValidation Loss: 0.03296471166379532\n",
            "Epoch 78  \tTraining Loss: 0.008508356134880657\tValidation Loss: 0.032893458782337716\n",
            "Epoch 79  \tTraining Loss: 0.00848801083753958\tValidation Loss: 0.03282786396512037\n",
            "Epoch 80  \tTraining Loss: 0.008469104457187474\tValidation Loss: 0.03276702443214576\n",
            "Epoch 81  \tTraining Loss: 0.008451455313145813\tValidation Loss: 0.03271012340029407\n",
            "Epoch 82  \tTraining Loss: 0.008434864715919834\tValidation Loss: 0.03265652167679917\n",
            "Epoch 83  \tTraining Loss: 0.008419133379331234\tValidation Loss: 0.03260561734349983\n",
            "Epoch 84  \tTraining Loss: 0.00840411571428109\tValidation Loss: 0.03255695524243511\n",
            "Epoch 85  \tTraining Loss: 0.008389711401883503\tValidation Loss: 0.032510071895962604\n",
            "Epoch 86  \tTraining Loss: 0.008375694254933832\tValidation Loss: 0.03246376811212299\n",
            "Epoch 87  \tTraining Loss: 0.008362041352788334\tValidation Loss: 0.03241799805129199\n",
            "Epoch 88  \tTraining Loss: 0.008348457733299884\tValidation Loss: 0.03237223634820042\n",
            "Epoch 89  \tTraining Loss: 0.008334760706752896\tValidation Loss: 0.03232543205994985\n",
            "Epoch 90  \tTraining Loss: 0.008320856473161472\tValidation Loss: 0.03227567818657088\n",
            "Epoch 91  \tTraining Loss: 0.008306611586920915\tValidation Loss: 0.03222299824022452\n",
            "Epoch 92  \tTraining Loss: 0.008291644304041465\tValidation Loss: 0.03216731457605886\n",
            "Epoch 93  \tTraining Loss: 0.008275500939487984\tValidation Loss: 0.03210514827670286\n",
            "Epoch 94  \tTraining Loss: 0.008257916465808599\tValidation Loss: 0.03203657321643227\n",
            "Epoch 95  \tTraining Loss: 0.0082386224963686\tValidation Loss: 0.03195994251733436\n",
            "Epoch 96  \tTraining Loss: 0.008216927291093214\tValidation Loss: 0.03187342386399167\n",
            "Epoch 97  \tTraining Loss: 0.008193298526768035\tValidation Loss: 0.03177938793079567\n",
            "Epoch 98  \tTraining Loss: 0.008167772722479384\tValidation Loss: 0.031682122693315586\n",
            "Epoch 99  \tTraining Loss: 0.008141529591415305\tValidation Loss: 0.03158814105587425\n",
            "Epoch 100  \tTraining Loss: 0.008115895164080296\tValidation Loss: 0.031499141524449964\n",
            "Epoch 101  \tTraining Loss: 0.008091362581590288\tValidation Loss: 0.03141545748310645\n",
            "Epoch 102  \tTraining Loss: 0.008068202423426074\tValidation Loss: 0.031336757434764544\n",
            "Epoch 103  \tTraining Loss: 0.00804611182373373\tValidation Loss: 0.031262921589793304\n",
            "Epoch 104  \tTraining Loss: 0.008025277126652882\tValidation Loss: 0.031194018562174124\n",
            "Epoch 105  \tTraining Loss: 0.00800589646168252\tValidation Loss: 0.031131288988870277\n",
            "Epoch 106  \tTraining Loss: 0.007988158803755547\tValidation Loss: 0.031073828010347193\n",
            "Epoch 107  \tTraining Loss: 0.007971816307101217\tValidation Loss: 0.03102104808879041\n",
            "Epoch 108  \tTraining Loss: 0.007956636507554574\tValidation Loss: 0.030972092468753973\n",
            "Epoch 109  \tTraining Loss: 0.007942418766175916\tValidation Loss: 0.030926247663710202\n",
            "Epoch 110  \tTraining Loss: 0.00792898686794993\tValidation Loss: 0.03088295109319922\n",
            "Epoch 111  \tTraining Loss: 0.00791620385565339\tValidation Loss: 0.03084171091971716\n",
            "Epoch 112  \tTraining Loss: 0.00790394733472855\tValidation Loss: 0.03080217300849046\n",
            "Epoch 113  \tTraining Loss: 0.007892132231085622\tValidation Loss: 0.03076400261805895\n",
            "Epoch 114  \tTraining Loss: 0.007880662919407167\tValidation Loss: 0.030726908664687264\n",
            "Epoch 115  \tTraining Loss: 0.007869470096206366\tValidation Loss: 0.030690742911103616\n",
            "Epoch 116  \tTraining Loss: 0.007858515618354484\tValidation Loss: 0.030655321363083295\n",
            "Epoch 117  \tTraining Loss: 0.007847754349706734\tValidation Loss: 0.030620508291875213\n",
            "Epoch 118  \tTraining Loss: 0.007837151221609166\tValidation Loss: 0.030586176180792752\n",
            "Epoch 119  \tTraining Loss: 0.007826674862189084\tValidation Loss: 0.030552289116247692\n",
            "Epoch 120  \tTraining Loss: 0.007816312835221193\tValidation Loss: 0.03051877902704364\n",
            "Epoch 121  \tTraining Loss: 0.007806045495839582\tValidation Loss: 0.030485565959535894\n",
            "Epoch 122  \tTraining Loss: 0.007795847308153504\tValidation Loss: 0.03045262133136802\n",
            "Epoch 123  \tTraining Loss: 0.007785721846692998\tValidation Loss: 0.030419937811345603\n",
            "Epoch 124  \tTraining Loss: 0.0077756581284199955\tValidation Loss: 0.030387421606019944\n",
            "Epoch 125  \tTraining Loss: 0.007765640932386583\tValidation Loss: 0.030355080396027957\n",
            "Epoch 126  \tTraining Loss: 0.007755663604577213\tValidation Loss: 0.030322880207841043\n",
            "Epoch 127  \tTraining Loss: 0.0077457210226174345\tValidation Loss: 0.030290820232910666\n",
            "Epoch 128  \tTraining Loss: 0.007735815114027554\tValidation Loss: 0.03025886339150708\n",
            "Epoch 129  \tTraining Loss: 0.007725947988943368\tValidation Loss: 0.030227014287349224\n",
            "Epoch 130  \tTraining Loss: 0.007716104915504432\tValidation Loss: 0.03019527530966354\n",
            "Epoch 131  \tTraining Loss: 0.0077062703557091074\tValidation Loss: 0.030163587380999897\n",
            "Epoch 132  \tTraining Loss: 0.007696430946473062\tValidation Loss: 0.030131909887730794\n",
            "Epoch 133  \tTraining Loss: 0.0076865825251698034\tValidation Loss: 0.030100259019382226\n",
            "Epoch 134  \tTraining Loss: 0.0076767385575940055\tValidation Loss: 0.030068653476586743\n",
            "Epoch 135  \tTraining Loss: 0.007666878872303639\tValidation Loss: 0.030037078955515748\n",
            "Epoch 136  \tTraining Loss: 0.007657029738880523\tValidation Loss: 0.030005411776712395\n",
            "Epoch 137  \tTraining Loss: 0.007647172804448987\tValidation Loss: 0.029973688492679652\n",
            "Epoch 138  \tTraining Loss: 0.007637283341149757\tValidation Loss: 0.029941595058604376\n",
            "Epoch 139  \tTraining Loss: 0.0076273259225070516\tValidation Loss: 0.029909291181361226\n",
            "Epoch 140  \tTraining Loss: 0.007617249565226991\tValidation Loss: 0.029876778767776245\n",
            "Epoch 141  \tTraining Loss: 0.007607062691630733\tValidation Loss: 0.029843414534536983\n",
            "Epoch 142  \tTraining Loss: 0.00759667320499413\tValidation Loss: 0.02980930486596802\n",
            "Epoch 143  \tTraining Loss: 0.007585988839589278\tValidation Loss: 0.029774058845731387\n",
            "Epoch 144  \tTraining Loss: 0.007574798284414473\tValidation Loss: 0.029736804980789178\n",
            "Epoch 145  \tTraining Loss: 0.0075630353984602805\tValidation Loss: 0.029696765220211772\n",
            "Epoch 146  \tTraining Loss: 0.007550548678288859\tValidation Loss: 0.02965007109362175\n",
            "Epoch 147  \tTraining Loss: 0.007535961787544533\tValidation Loss: 0.02959407266096468\n",
            "Epoch 148  \tTraining Loss: 0.007518434109478687\tValidation Loss: 0.029522186274240438\n",
            "Epoch 149  \tTraining Loss: 0.0074965563196942656\tValidation Loss: 0.029429401630623804\n",
            "Epoch 150  \tTraining Loss: 0.00746904500508638\tValidation Loss: 0.029312364222121298\n",
            "Epoch 151  \tTraining Loss: 0.007436459465053557\tValidation Loss: 0.029185369764092774\n",
            "Epoch 152  \tTraining Loss: 0.007402503092686759\tValidation Loss: 0.02906851801699319\n",
            "Epoch 153  \tTraining Loss: 0.007371174533872092\tValidation Loss: 0.028964790308454565\n",
            "Epoch 154  \tTraining Loss: 0.0073431304702904375\tValidation Loss: 0.028873140596162007\n",
            "Epoch 155  \tTraining Loss: 0.007318106517971925\tValidation Loss: 0.028792175051697132\n",
            "Epoch 156  \tTraining Loss: 0.007295749952411396\tValidation Loss: 0.028720358468795178\n",
            "Epoch 157  \tTraining Loss: 0.0072757163944511425\tValidation Loss: 0.02865617106509637\n",
            "Epoch 158  \tTraining Loss: 0.007257596619348866\tValidation Loss: 0.028598243237182092\n",
            "Epoch 159  \tTraining Loss: 0.007241062013501761\tValidation Loss: 0.02854546419595971\n",
            "Epoch 160  \tTraining Loss: 0.0072258261826759025\tValidation Loss: 0.02849697159560479\n",
            "Epoch 161  \tTraining Loss: 0.007211652515316525\tValidation Loss: 0.028451990720638573\n",
            "Epoch 162  \tTraining Loss: 0.007198371163051463\tValidation Loss: 0.02840992456744907\n",
            "Epoch 163  \tTraining Loss: 0.007185823673048999\tValidation Loss: 0.02837026279989773\n",
            "Epoch 164  \tTraining Loss: 0.007173889086022958\tValidation Loss: 0.028332606442746988\n",
            "Epoch 165  \tTraining Loss: 0.007162461797075898\tValidation Loss: 0.02829649177510982\n",
            "Epoch 166  \tTraining Loss: 0.007151456909948682\tValidation Loss: 0.028261769727656046\n",
            "Epoch 167  \tTraining Loss: 0.007140816205296209\tValidation Loss: 0.02822822034790126\n",
            "Epoch 168  \tTraining Loss: 0.007130487302112396\tValidation Loss: 0.028195638840660793\n",
            "Epoch 169  \tTraining Loss: 0.007120412932070206\tValidation Loss: 0.02816390455560539\n",
            "Epoch 170  \tTraining Loss: 0.00711056343672579\tValidation Loss: 0.028132875358154178\n",
            "Epoch 171  \tTraining Loss: 0.007100900162154379\tValidation Loss: 0.02810246428863226\n",
            "Epoch 172  \tTraining Loss: 0.007091400265894446\tValidation Loss: 0.028072582934400563\n",
            "Epoch 173  \tTraining Loss: 0.007082036120561907\tValidation Loss: 0.028043117891110195\n",
            "Epoch 174  \tTraining Loss: 0.007072799488181911\tValidation Loss: 0.028014063406919407\n",
            "Epoch 175  \tTraining Loss: 0.007063688199962418\tValidation Loss: 0.027985297775486247\n",
            "Epoch 176  \tTraining Loss: 0.007054680121601145\tValidation Loss: 0.02795683677355444\n",
            "Epoch 177  \tTraining Loss: 0.007045757497873289\tValidation Loss: 0.02792863354745029\n",
            "Epoch 178  \tTraining Loss: 0.0070369093384506495\tValidation Loss: 0.027900615321604583\n",
            "Epoch 179  \tTraining Loss: 0.007028120447827593\tValidation Loss: 0.027872822430362525\n",
            "Epoch 180  \tTraining Loss: 0.007019401522230419\tValidation Loss: 0.027845268246424287\n",
            "Epoch 181  \tTraining Loss: 0.007010743176407804\tValidation Loss: 0.027817903213960826\n",
            "Epoch 182  \tTraining Loss: 0.007002134095721886\tValidation Loss: 0.027790702033495335\n",
            "Epoch 183  \tTraining Loss: 0.0069935628528736945\tValidation Loss: 0.02776364721243137\n",
            "Epoch 184  \tTraining Loss: 0.006985041166391071\tValidation Loss: 0.02773679891143072\n",
            "Epoch 185  \tTraining Loss: 0.006976564825269468\tValidation Loss: 0.02770998999062542\n",
            "Epoch 186  \tTraining Loss: 0.00696811824844286\tValidation Loss: 0.027683264125044016\n",
            "Epoch 187  \tTraining Loss: 0.0069597294737487195\tValidation Loss: 0.027656661223425385\n",
            "Epoch 188  \tTraining Loss: 0.006951388859526381\tValidation Loss: 0.02763024050092117\n",
            "Epoch 189  \tTraining Loss: 0.006943096601977793\tValidation Loss: 0.027603962503267902\n",
            "Epoch 190  \tTraining Loss: 0.006934848556575081\tValidation Loss: 0.027577838496567954\n",
            "Epoch 191  \tTraining Loss: 0.006926634504299563\tValidation Loss: 0.027551836096993704\n",
            "Epoch 192  \tTraining Loss: 0.0069184526673536475\tValidation Loss: 0.027525981351841946\n",
            "Epoch 193  \tTraining Loss: 0.006910315124078065\tValidation Loss: 0.027500242345092143\n",
            "Epoch 194  \tTraining Loss: 0.0069022127708629494\tValidation Loss: 0.027474592361716052\n",
            "Epoch 195  \tTraining Loss: 0.0068941409863883035\tValidation Loss: 0.027449033240300288\n",
            "Epoch 196  \tTraining Loss: 0.006886085324794952\tValidation Loss: 0.02742344233175334\n",
            "Epoch 197  \tTraining Loss: 0.006878048627585975\tValidation Loss: 0.027397916094211036\n",
            "Epoch 198  \tTraining Loss: 0.006870045345752764\tValidation Loss: 0.027372441794304477\n",
            "Epoch 199  \tTraining Loss: 0.00686208570598767\tValidation Loss: 0.027347106585572458\n",
            "Epoch 200  \tTraining Loss: 0.006854161905620824\tValidation Loss: 0.02732183211751095\n",
            "Epoch 201  \tTraining Loss: 0.006846254055109791\tValidation Loss: 0.027296734573237343\n",
            "Epoch 202  \tTraining Loss: 0.006838392017414068\tValidation Loss: 0.027271772719651054\n",
            "Epoch 203  \tTraining Loss: 0.00683056358250364\tValidation Loss: 0.02724699441275958\n",
            "Epoch 204  \tTraining Loss: 0.0068227741217572946\tValidation Loss: 0.027222311793704164\n",
            "Epoch 205  \tTraining Loss: 0.0068150172408661\tValidation Loss: 0.027197704113593003\n",
            "Epoch 206  \tTraining Loss: 0.006807276844212769\tValidation Loss: 0.02717321676514031\n",
            "Epoch 207  \tTraining Loss: 0.006799575972663074\tValidation Loss: 0.027148905296683398\n",
            "Epoch 208  \tTraining Loss: 0.006791917776976597\tValidation Loss: 0.027124684923620846\n",
            "Epoch 209  \tTraining Loss: 0.006784292534327453\tValidation Loss: 0.027100601764135823\n",
            "Epoch 210  \tTraining Loss: 0.006776708035306424\tValidation Loss: 0.027076668368559317\n",
            "Epoch 211  \tTraining Loss: 0.0067691687811147405\tValidation Loss: 0.027052895728692215\n",
            "Epoch 212  \tTraining Loss: 0.006761669528236631\tValidation Loss: 0.027029283301206353\n",
            "Epoch 213  \tTraining Loss: 0.006754205931153602\tValidation Loss: 0.02700577483320021\n",
            "Epoch 214  \tTraining Loss: 0.006746777925498086\tValidation Loss: 0.026982414829944675\n",
            "Epoch 215  \tTraining Loss: 0.006739397073101463\tValidation Loss: 0.02695912447232966\n",
            "Epoch 216  \tTraining Loss: 0.006732044751210454\tValidation Loss: 0.026936013496405113\n",
            "Epoch 217  \tTraining Loss: 0.006724730477438836\tValidation Loss: 0.02691306261599834\n",
            "Epoch 218  \tTraining Loss: 0.006717440581568132\tValidation Loss: 0.026890275003219173\n",
            "Epoch 219  \tTraining Loss: 0.006710187605075273\tValidation Loss: 0.026867588359244864\n",
            "Epoch 220  \tTraining Loss: 0.006702963086560582\tValidation Loss: 0.026845013873866775\n",
            "Epoch 221  \tTraining Loss: 0.006695768571665414\tValidation Loss: 0.026822597206567118\n",
            "Epoch 222  \tTraining Loss: 0.0066886194369132545\tValidation Loss: 0.026800284113379362\n",
            "Epoch 223  \tTraining Loss: 0.006681493320481978\tValidation Loss: 0.026778017342324503\n",
            "Epoch 224  \tTraining Loss: 0.006674392052179106\tValidation Loss: 0.026755932638407042\n",
            "Epoch 225  \tTraining Loss: 0.006667343462871126\tValidation Loss: 0.02673399090247168\n",
            "Epoch 226  \tTraining Loss: 0.00666033839101657\tValidation Loss: 0.02671215908516264\n",
            "Epoch 227  \tTraining Loss: 0.006653368383231605\tValidation Loss: 0.02669047857490535\n",
            "Epoch 228  \tTraining Loss: 0.006646428368339022\tValidation Loss: 0.026668894963494193\n",
            "Epoch 229  \tTraining Loss: 0.006639517997141005\tValidation Loss: 0.02664741871877465\n",
            "Epoch 230  \tTraining Loss: 0.006632642538258066\tValidation Loss: 0.02662592203158555\n",
            "Epoch 231  \tTraining Loss: 0.00662579718646166\tValidation Loss: 0.026604567995532534\n",
            "Epoch 232  \tTraining Loss: 0.006618988004577928\tValidation Loss: 0.02658333436369578\n",
            "Epoch 233  \tTraining Loss: 0.006612218822717136\tValidation Loss: 0.026562251094406148\n",
            "Epoch 234  \tTraining Loss: 0.0066055037719202694\tValidation Loss: 0.026541282547780837\n",
            "Epoch 235  \tTraining Loss: 0.006598829208603584\tValidation Loss: 0.02652054533421891\n",
            "Epoch 236  \tTraining Loss: 0.006592205325712725\tValidation Loss: 0.02649995286098583\n",
            "Epoch 237  \tTraining Loss: 0.006585606231243634\tValidation Loss: 0.026479464453723338\n",
            "Epoch 238  \tTraining Loss: 0.00657904487394173\tValidation Loss: 0.026459065383430888\n",
            "Epoch 239  \tTraining Loss: 0.006572524381666635\tValidation Loss: 0.026438784647344882\n",
            "Epoch 240  \tTraining Loss: 0.006566040796665965\tValidation Loss: 0.02641860873595449\n",
            "Epoch 241  \tTraining Loss: 0.006559593928223852\tValidation Loss: 0.026398556987875705\n",
            "Epoch 242  \tTraining Loss: 0.006553181639437733\tValidation Loss: 0.026378575153111962\n",
            "Epoch 243  \tTraining Loss: 0.006546790100191942\tValidation Loss: 0.026358714771987524\n",
            "Epoch 244  \tTraining Loss: 0.006540434312711857\tValidation Loss: 0.026338891872721003\n",
            "Epoch 245  \tTraining Loss: 0.0065341028485353374\tValidation Loss: 0.026319163127401016\n",
            "Epoch 246  \tTraining Loss: 0.006527802883432292\tValidation Loss: 0.026299539339164322\n",
            "Epoch 247  \tTraining Loss: 0.006521534819888677\tValidation Loss: 0.026279946660840197\n",
            "Epoch 248  \tTraining Loss: 0.006515290885488749\tValidation Loss: 0.026260542567252233\n",
            "Epoch 249  \tTraining Loss: 0.0065090858668327605\tValidation Loss: 0.026241251231338192\n",
            "Epoch 250  \tTraining Loss: 0.0065029104572452475\tValidation Loss: 0.026221990648472363\n",
            "Epoch 251  \tTraining Loss: 0.0064967555658168846\tValidation Loss: 0.02620278429846587\n",
            "Epoch 252  \tTraining Loss: 0.006490628783704978\tValidation Loss: 0.02618364642388162\n",
            "Epoch 253  \tTraining Loss: 0.006484521123047134\tValidation Loss: 0.02616449112341359\n",
            "Epoch 254  \tTraining Loss: 0.006478440751551016\tValidation Loss: 0.0261455084978818\n",
            "Epoch 255  \tTraining Loss: 0.0064724014556964025\tValidation Loss: 0.026126695166197998\n",
            "Epoch 256  \tTraining Loss: 0.006466404589511662\tValidation Loss: 0.026108101284888795\n",
            "Epoch 257  \tTraining Loss: 0.006460454707246532\tValidation Loss: 0.026089599534211944\n",
            "Epoch 258  \tTraining Loss: 0.006454526327548913\tValidation Loss: 0.02607122101897645\n",
            "Epoch 259  \tTraining Loss: 0.006448628339630123\tValidation Loss: 0.02605291685917546\n",
            "Epoch 260  \tTraining Loss: 0.006442755129010623\tValidation Loss: 0.02603472670573822\n",
            "Epoch 261  \tTraining Loss: 0.006436919062430716\tValidation Loss: 0.026016666406217158\n",
            "Epoch 262  \tTraining Loss: 0.006431124332024285\tValidation Loss: 0.025998721134133845\n",
            "Epoch 263  \tTraining Loss: 0.0064253642266911565\tValidation Loss: 0.025980925828996387\n",
            "Epoch 264  \tTraining Loss: 0.006419649154638959\tValidation Loss: 0.025963235937517277\n",
            "Epoch 265  \tTraining Loss: 0.006413967449801444\tValidation Loss: 0.025945623863163843\n",
            "Epoch 266  \tTraining Loss: 0.00640831100788889\tValidation Loss: 0.0259280775622596\n",
            "Epoch 267  \tTraining Loss: 0.006402682093573448\tValidation Loss: 0.025910645856319345\n",
            "Epoch 268  \tTraining Loss: 0.0063970823434239465\tValidation Loss: 0.02589330189083419\n",
            "Epoch 269  \tTraining Loss: 0.006391513429085221\tValidation Loss: 0.02587605504352307\n",
            "Epoch 270  \tTraining Loss: 0.006385971497280796\tValidation Loss: 0.025858892546668098\n",
            "Epoch 271  \tTraining Loss: 0.006380452823930827\tValidation Loss: 0.025841800958598733\n",
            "Epoch 272  \tTraining Loss: 0.00637496214508292\tValidation Loss: 0.02582480869156392\n",
            "Epoch 273  \tTraining Loss: 0.006369508421360541\tValidation Loss: 0.025807915659406645\n",
            "Epoch 274  \tTraining Loss: 0.006364085545087329\tValidation Loss: 0.025791118809896058\n",
            "Epoch 275  \tTraining Loss: 0.00635869080665454\tValidation Loss: 0.02577444188553837\n",
            "Epoch 276  \tTraining Loss: 0.0063533248253748\tValidation Loss: 0.02575791146064802\n",
            "Epoch 277  \tTraining Loss: 0.006347991167451107\tValidation Loss: 0.025741465646996496\n",
            "Epoch 278  \tTraining Loss: 0.006342688408510413\tValidation Loss: 0.025725153668281136\n",
            "Epoch 279  \tTraining Loss: 0.006337420429875235\tValidation Loss: 0.025708961700816855\n",
            "Epoch 280  \tTraining Loss: 0.006332180156055122\tValidation Loss: 0.025692869917366995\n",
            "Epoch 281  \tTraining Loss: 0.006326971289185813\tValidation Loss: 0.025676862143899002\n",
            "Epoch 282  \tTraining Loss: 0.006321794342805275\tValidation Loss: 0.02566100823890514\n",
            "Epoch 283  \tTraining Loss: 0.006316650625183303\tValidation Loss: 0.02564523669766378\n",
            "Epoch 284  \tTraining Loss: 0.006311536880093933\tValidation Loss: 0.02562956897873309\n",
            "Epoch 285  \tTraining Loss: 0.006306455263711578\tValidation Loss: 0.02561403808794551\n",
            "Epoch 286  \tTraining Loss: 0.006301409017305908\tValidation Loss: 0.0255985875168995\n",
            "Epoch 287  \tTraining Loss: 0.006296391660861986\tValidation Loss: 0.02558322028356519\n",
            "Epoch 288  \tTraining Loss: 0.0062914022331457715\tValidation Loss: 0.025567933792466066\n",
            "Epoch 289  \tTraining Loss: 0.0062864293495038534\tValidation Loss: 0.025552749552443837\n",
            "Epoch 290  \tTraining Loss: 0.006281485007283286\tValidation Loss: 0.025537658652356833\n",
            "Epoch 291  \tTraining Loss: 0.006276568613358135\tValidation Loss: 0.025522686670618827\n",
            "Epoch 292  \tTraining Loss: 0.006271678069300664\tValidation Loss: 0.025507786565570168\n",
            "Epoch 293  \tTraining Loss: 0.006266813545427346\tValidation Loss: 0.025492946485200063\n",
            "Epoch 294  \tTraining Loss: 0.006261974706529369\tValidation Loss: 0.02547817259929121\n",
            "Epoch 295  \tTraining Loss: 0.006257157188406668\tValidation Loss: 0.025463521426592778\n",
            "Epoch 296  \tTraining Loss: 0.006252366405312487\tValidation Loss: 0.02544893927058901\n",
            "Epoch 297  \tTraining Loss: 0.006247601900989307\tValidation Loss: 0.025434412792753103\n",
            "Epoch 298  \tTraining Loss: 0.006242863332779724\tValidation Loss: 0.02541987188457922\n",
            "Epoch 299  \tTraining Loss: 0.006238145040147147\tValidation Loss: 0.02540539965111883\n",
            "Epoch 300  \tTraining Loss: 0.006233452964288954\tValidation Loss: 0.025391104901121578\n",
            "Epoch 301  \tTraining Loss: 0.006228797188322342\tValidation Loss: 0.025376921261893042\n",
            "Epoch 302  \tTraining Loss: 0.006224169527703329\tValidation Loss: 0.025362843311254546\n",
            "Epoch 303  \tTraining Loss: 0.006219571905722848\tValidation Loss: 0.025348833509065888\n",
            "Epoch 304  \tTraining Loss: 0.006215002408937234\tValidation Loss: 0.02533489136867375\n",
            "Epoch 305  \tTraining Loss: 0.006210457917618981\tValidation Loss: 0.02532101504361032\n",
            "Epoch 306  \tTraining Loss: 0.006205940959526725\tValidation Loss: 0.0253072095294057\n",
            "Epoch 307  \tTraining Loss: 0.006201455136532791\tValidation Loss: 0.025293483382116635\n",
            "Epoch 308  \tTraining Loss: 0.006196993832102121\tValidation Loss: 0.02527982161695974\n",
            "Epoch 309  \tTraining Loss: 0.006192558247292224\tValidation Loss: 0.025266222659784673\n",
            "Epoch 310  \tTraining Loss: 0.0061881506376590305\tValidation Loss: 0.025252651520962736\n",
            "Epoch 311  \tTraining Loss: 0.006183769323347049\tValidation Loss: 0.02523916562179195\n",
            "Epoch 312  \tTraining Loss: 0.006179414533114811\tValidation Loss: 0.0252257524703807\n",
            "Epoch 313  \tTraining Loss: 0.0061750828941677105\tValidation Loss: 0.02521242122820261\n",
            "Epoch 314  \tTraining Loss: 0.006170774978912473\tValidation Loss: 0.025199153286047167\n",
            "Epoch 315  \tTraining Loss: 0.006166489772364963\tValidation Loss: 0.025185950940179298\n",
            "Epoch 316  \tTraining Loss: 0.006162219075679068\tValidation Loss: 0.02517282594019085\n",
            "Epoch 317  \tTraining Loss: 0.006157962677533544\tValidation Loss: 0.025159741624143653\n",
            "Epoch 318  \tTraining Loss: 0.006153733365014095\tValidation Loss: 0.02514667666390965\n",
            "Epoch 319  \tTraining Loss: 0.006149513787719518\tValidation Loss: 0.02513368267514921\n",
            "Epoch 320  \tTraining Loss: 0.006145303084744939\tValidation Loss: 0.025120788505914694\n",
            "Epoch 321  \tTraining Loss: 0.006141118230174872\tValidation Loss: 0.02510796850298339\n",
            "Epoch 322  \tTraining Loss: 0.006136961076910121\tValidation Loss: 0.025095254035010506\n",
            "Epoch 323  \tTraining Loss: 0.006132837971046288\tValidation Loss: 0.025082632970352167\n",
            "Epoch 324  \tTraining Loss: 0.0061287391956638855\tValidation Loss: 0.025070064923974608\n",
            "Epoch 325  \tTraining Loss: 0.006124663222088348\tValidation Loss: 0.02505756192217693\n",
            "Epoch 326  \tTraining Loss: 0.0061206110284681686\tValidation Loss: 0.02504512958601065\n",
            "Epoch 327  \tTraining Loss: 0.006116579925504812\tValidation Loss: 0.025032800322559626\n",
            "Epoch 328  \tTraining Loss: 0.00611257257230705\tValidation Loss: 0.025020523604499228\n",
            "Epoch 329  \tTraining Loss: 0.006108587634881633\tValidation Loss: 0.02500830562941747\n",
            "Epoch 330  \tTraining Loss: 0.0061046227912236566\tValidation Loss: 0.02499615083590381\n",
            "Epoch 331  \tTraining Loss: 0.006100677063021779\tValidation Loss: 0.024984045442677412\n",
            "Epoch 332  \tTraining Loss: 0.006096752373211996\tValidation Loss: 0.02497198035468053\n",
            "Epoch 333  \tTraining Loss: 0.0060928520335199335\tValidation Loss: 0.024959975757604752\n",
            "Epoch 334  \tTraining Loss: 0.006088973623069008\tValidation Loss: 0.02494802643734658\n",
            "Epoch 335  \tTraining Loss: 0.006085116081539122\tValidation Loss: 0.02493610915888324\n",
            "Epoch 336  \tTraining Loss: 0.006081279927018387\tValidation Loss: 0.024924227326751914\n",
            "Epoch 337  \tTraining Loss: 0.006077468551391712\tValidation Loss: 0.024912408207839142\n",
            "Epoch 338  \tTraining Loss: 0.006073681912855458\tValidation Loss: 0.024900657583559176\n",
            "Epoch 339  \tTraining Loss: 0.006069923287413627\tValidation Loss: 0.024888964799464702\n",
            "Epoch 340  \tTraining Loss: 0.006066185531978873\tValidation Loss: 0.02487732613820413\n",
            "Epoch 341  \tTraining Loss: 0.0060624675762309165\tValidation Loss: 0.02486577852727973\n",
            "Epoch 342  \tTraining Loss: 0.006058767263975141\tValidation Loss: 0.024854295240014965\n",
            "Epoch 343  \tTraining Loss: 0.006055088686443548\tValidation Loss: 0.02484286088797013\n",
            "Epoch 344  \tTraining Loss: 0.006051425437972922\tValidation Loss: 0.02483146821620398\n",
            "Epoch 345  \tTraining Loss: 0.006047775894972265\tValidation Loss: 0.02482015856498149\n",
            "Epoch 346  \tTraining Loss: 0.006044139965964359\tValidation Loss: 0.02480884919747158\n",
            "Epoch 347  \tTraining Loss: 0.006040514177029029\tValidation Loss: 0.02479759855212006\n",
            "Epoch 348  \tTraining Loss: 0.006036904882502216\tValidation Loss: 0.024786390956627664\n",
            "Epoch 349  \tTraining Loss: 0.006033311798411562\tValidation Loss: 0.02477527508143065\n",
            "Epoch 350  \tTraining Loss: 0.00602973579154105\tValidation Loss: 0.02476420174406114\n",
            "Epoch 351  \tTraining Loss: 0.006026181055569789\tValidation Loss: 0.024753183267305634\n",
            "Epoch 352  \tTraining Loss: 0.006022640442661387\tValidation Loss: 0.0247422008508211\n",
            "Epoch 353  \tTraining Loss: 0.0060191099485247795\tValidation Loss: 0.0247312818603758\n",
            "Epoch 354  \tTraining Loss: 0.006015596977320467\tValidation Loss: 0.02472041119565034\n",
            "Epoch 355  \tTraining Loss: 0.006012102915027658\tValidation Loss: 0.024709590027352575\n",
            "Epoch 356  \tTraining Loss: 0.0060086265491844275\tValidation Loss: 0.02469876440827605\n",
            "Epoch 357  \tTraining Loss: 0.006005179570553633\tValidation Loss: 0.024687963800295473\n",
            "Epoch 358  \tTraining Loss: 0.0060017508708216484\tValidation Loss: 0.024677217920034928\n",
            "Epoch 359  \tTraining Loss: 0.005998344707489967\tValidation Loss: 0.024666508964407217\n",
            "Epoch 360  \tTraining Loss: 0.00599496670136872\tValidation Loss: 0.024655872155912566\n",
            "Epoch 361  \tTraining Loss: 0.005991613875476972\tValidation Loss: 0.02464528481427316\n",
            "Epoch 362  \tTraining Loss: 0.0059882779245242195\tValidation Loss: 0.024634750756991935\n",
            "Epoch 363  \tTraining Loss: 0.005984957373859214\tValidation Loss: 0.024624237701034356\n",
            "Epoch 364  \tTraining Loss: 0.00598165161455534\tValidation Loss: 0.024613768002242006\n",
            "Epoch 365  \tTraining Loss: 0.005978359168914991\tValidation Loss: 0.0246033867502414\n",
            "Epoch 366  \tTraining Loss: 0.005975089272836013\tValidation Loss: 0.024593044973024208\n",
            "Epoch 367  \tTraining Loss: 0.005971836017505228\tValidation Loss: 0.02458273619583983\n",
            "Epoch 368  \tTraining Loss: 0.005968598616174104\tValidation Loss: 0.024572523245327427\n",
            "Epoch 369  \tTraining Loss: 0.005965377763979198\tValidation Loss: 0.024562338331712372\n",
            "Epoch 370  \tTraining Loss: 0.005962172636388458\tValidation Loss: 0.024552199084367916\n",
            "Epoch 371  \tTraining Loss: 0.005958977861394876\tValidation Loss: 0.024542087500622436\n",
            "Epoch 372  \tTraining Loss: 0.005955791800836383\tValidation Loss: 0.024532010686753317\n",
            "Epoch 373  \tTraining Loss: 0.005952616612105868\tValidation Loss: 0.024521985253394697\n",
            "Epoch 374  \tTraining Loss: 0.005949458510364312\tValidation Loss: 0.024511983612755403\n",
            "Epoch 375  \tTraining Loss: 0.005946318764994233\tValidation Loss: 0.02450201202186725\n",
            "Epoch 376  \tTraining Loss: 0.005943194317467602\tValidation Loss: 0.024492080536859633\n",
            "Epoch 377  \tTraining Loss: 0.005940086842315963\tValidation Loss: 0.02448220304084032\n",
            "Epoch 378  \tTraining Loss: 0.0059369921978746935\tValidation Loss: 0.024472345501000824\n",
            "Epoch 379  \tTraining Loss: 0.005933902303196457\tValidation Loss: 0.0244625564507686\n",
            "Epoch 380  \tTraining Loss: 0.005930813451788752\tValidation Loss: 0.024452810313460048\n",
            "Epoch 381  \tTraining Loss: 0.005927740650972654\tValidation Loss: 0.024443140851907984\n",
            "Epoch 382  \tTraining Loss: 0.0059246754537937395\tValidation Loss: 0.02443342536946566\n",
            "Epoch 383  \tTraining Loss: 0.005921632443179107\tValidation Loss: 0.024423743835006523\n",
            "Epoch 384  \tTraining Loss: 0.005918599381311699\tValidation Loss: 0.024414105747899834\n",
            "Epoch 385  \tTraining Loss: 0.0059155715308031945\tValidation Loss: 0.02440451489483918\n",
            "Epoch 386  \tTraining Loss: 0.005912556363838289\tValidation Loss: 0.024394985861921507\n",
            "Epoch 387  \tTraining Loss: 0.005909560520124632\tValidation Loss: 0.024385496459485165\n",
            "Epoch 388  \tTraining Loss: 0.005906584067104617\tValidation Loss: 0.024376039032274616\n",
            "Epoch 389  \tTraining Loss: 0.005903624588738941\tValidation Loss: 0.02436666051753521\n",
            "Epoch 390  \tTraining Loss: 0.005900675996308177\tValidation Loss: 0.024357276169043263\n",
            "Epoch 391  \tTraining Loss: 0.0058977435113512205\tValidation Loss: 0.02434800499941245\n",
            "Epoch 392  \tTraining Loss: 0.005894820535743524\tValidation Loss: 0.02433881900546679\n",
            "Epoch 393  \tTraining Loss: 0.005891912266248286\tValidation Loss: 0.02432970055409091\n",
            "Epoch 394  \tTraining Loss: 0.005889017850178476\tValidation Loss: 0.024320622019551902\n",
            "Epoch 395  \tTraining Loss: 0.005886131996207189\tValidation Loss: 0.02431156413137527\n",
            "Epoch 396  \tTraining Loss: 0.00588326426386797\tValidation Loss: 0.024302520109501086\n",
            "Epoch 397  \tTraining Loss: 0.005880414127670158\tValidation Loss: 0.024293526112822712\n",
            "Epoch 398  \tTraining Loss: 0.005877580033333596\tValidation Loss: 0.024284575429145406\n",
            "Epoch 399  \tTraining Loss: 0.005874759406744365\tValidation Loss: 0.0242756488968767\n",
            "Epoch 400  \tTraining Loss: 0.005871949827928915\tValidation Loss: 0.02426671966685692\n",
            "Epoch 401  \tTraining Loss: 0.0058691538351817885\tValidation Loss: 0.024257806070194614\n",
            "Epoch 402  \tTraining Loss: 0.005866370687523138\tValidation Loss: 0.02424895751155671\n",
            "Epoch 403  \tTraining Loss: 0.005863586920787413\tValidation Loss: 0.024240144418750886\n",
            "Epoch 404  \tTraining Loss: 0.005860804376829901\tValidation Loss: 0.02423142262190537\n",
            "Epoch 405  \tTraining Loss: 0.005858025959402686\tValidation Loss: 0.02422266360610839\n",
            "Epoch 406  \tTraining Loss: 0.0058552556167398755\tValidation Loss: 0.024213955688313984\n",
            "Epoch 407  \tTraining Loss: 0.005852487772269871\tValidation Loss: 0.02420526358257537\n",
            "Epoch 408  \tTraining Loss: 0.0058497366358423435\tValidation Loss: 0.024196593682697946\n",
            "Epoch 409  \tTraining Loss: 0.005846998224219795\tValidation Loss: 0.0241879577618548\n",
            "Epoch 410  \tTraining Loss: 0.005844267561183133\tValidation Loss: 0.02417938401005802\n",
            "Epoch 411  \tTraining Loss: 0.005841546502440121\tValidation Loss: 0.024170829499947113\n",
            "Epoch 412  \tTraining Loss: 0.005838835224665966\tValidation Loss: 0.024162306225114425\n",
            "Epoch 413  \tTraining Loss: 0.005836133544536065\tValidation Loss: 0.02415380477094903\n",
            "Epoch 414  \tTraining Loss: 0.0058334422353289\tValidation Loss: 0.024145326415115395\n",
            "Epoch 415  \tTraining Loss: 0.005830762720594098\tValidation Loss: 0.024136840203847952\n",
            "Epoch 416  \tTraining Loss: 0.005828097863336435\tValidation Loss: 0.02412834693419025\n",
            "Epoch 417  \tTraining Loss: 0.005825446383948628\tValidation Loss: 0.024119904865117853\n",
            "Epoch 418  \tTraining Loss: 0.005822801522728229\tValidation Loss: 0.024111512240575256\n",
            "Epoch 419  \tTraining Loss: 0.005820168555894729\tValidation Loss: 0.024103135342256627\n",
            "Epoch 420  \tTraining Loss: 0.005817547068111239\tValidation Loss: 0.024094714670398663\n",
            "Epoch 421  \tTraining Loss: 0.005814932494952511\tValidation Loss: 0.024086329495848782\n",
            "Epoch 422  \tTraining Loss: 0.005812325510848505\tValidation Loss: 0.024077940876865128\n",
            "Epoch 423  \tTraining Loss: 0.00580973323222652\tValidation Loss: 0.02406955810985377\n",
            "Epoch 424  \tTraining Loss: 0.005807152855063608\tValidation Loss: 0.02406123515060278\n",
            "Epoch 425  \tTraining Loss: 0.0058045879442513515\tValidation Loss: 0.024052872294927568\n",
            "Epoch 426  \tTraining Loss: 0.00580203270402154\tValidation Loss: 0.02404447913877349\n",
            "Epoch 427  \tTraining Loss: 0.005799497669222312\tValidation Loss: 0.024036038771243905\n",
            "Epoch 428  \tTraining Loss: 0.0057969772351630375\tValidation Loss: 0.024027615381880123\n",
            "Epoch 429  \tTraining Loss: 0.00579446918593026\tValidation Loss: 0.02401922911130167\n",
            "Epoch 430  \tTraining Loss: 0.005791969295359116\tValidation Loss: 0.02401079076494602\n",
            "Epoch 431  \tTraining Loss: 0.005789472864988753\tValidation Loss: 0.02400236866981541\n",
            "Epoch 432  \tTraining Loss: 0.005786986669173714\tValidation Loss: 0.023994038287054206\n",
            "Epoch 433  \tTraining Loss: 0.005784499296858358\tValidation Loss: 0.023985793260368905\n",
            "Epoch 434  \tTraining Loss: 0.005782015596836749\tValidation Loss: 0.02397756444389925\n",
            "Epoch 435  \tTraining Loss: 0.0057795437678861096\tValidation Loss: 0.023969357188340555\n",
            "Epoch 436  \tTraining Loss: 0.005777078423082752\tValidation Loss: 0.02396116570154891\n",
            "Epoch 437  \tTraining Loss: 0.005774624132603379\tValidation Loss: 0.023952942012738978\n",
            "Epoch 438  \tTraining Loss: 0.005772182298324571\tValidation Loss: 0.023944760418919325\n",
            "Epoch 439  \tTraining Loss: 0.005769750743940489\tValidation Loss: 0.023936550099140523\n",
            "Epoch 440  \tTraining Loss: 0.005767325836654109\tValidation Loss: 0.023928350983951168\n",
            "Epoch 441  \tTraining Loss: 0.005764912996006401\tValidation Loss: 0.02392016250007607\n",
            "Epoch 442  \tTraining Loss: 0.005762511536607375\tValidation Loss: 0.023912011901500087\n",
            "Epoch 443  \tTraining Loss: 0.005760124432078018\tValidation Loss: 0.023903887507676785\n",
            "Epoch 444  \tTraining Loss: 0.005757753710973016\tValidation Loss: 0.02389579879013231\n",
            "Epoch 445  \tTraining Loss: 0.005755390556161221\tValidation Loss: 0.02388767708725064\n",
            "Epoch 446  \tTraining Loss: 0.00575302756482359\tValidation Loss: 0.02387956699201888\n",
            "Epoch 447  \tTraining Loss: 0.005750671225647\tValidation Loss: 0.023871481374269997\n",
            "Epoch 448  \tTraining Loss: 0.0057483241948842745\tValidation Loss: 0.023863445055202426\n",
            "Epoch 449  \tTraining Loss: 0.005745987407044732\tValidation Loss: 0.02385548095105634\n",
            "Epoch 450  \tTraining Loss: 0.005743663535715496\tValidation Loss: 0.02384756971842509\n",
            "Epoch 451  \tTraining Loss: 0.005741346930296899\tValidation Loss: 0.023839705728282028\n",
            "Epoch 452  \tTraining Loss: 0.0057390389114175535\tValidation Loss: 0.023831890306593828\n",
            "Epoch 453  \tTraining Loss: 0.00573673866227381\tValidation Loss: 0.023824054083497196\n",
            "Epoch 454  \tTraining Loss: 0.0057344413911827\tValidation Loss: 0.02381626676848849\n",
            "Epoch 455  \tTraining Loss: 0.005732153433039789\tValidation Loss: 0.0238085009616017\n",
            "Epoch 456  \tTraining Loss: 0.0057298711382639395\tValidation Loss: 0.023800762036810703\n",
            "Epoch 457  \tTraining Loss: 0.005727592095459722\tValidation Loss: 0.023793036964462673\n",
            "Epoch 458  \tTraining Loss: 0.005725320215442672\tValidation Loss: 0.023785243257570227\n",
            "Epoch 459  \tTraining Loss: 0.005723053158020496\tValidation Loss: 0.023777415025872446\n",
            "Epoch 460  \tTraining Loss: 0.005720794679973818\tValidation Loss: 0.023769646963115173\n",
            "Epoch 461  \tTraining Loss: 0.005718546847642319\tValidation Loss: 0.023761898945965563\n",
            "Epoch 462  \tTraining Loss: 0.005716308840866115\tValidation Loss: 0.023754140620177635\n",
            "Epoch 463  \tTraining Loss: 0.005714076687838009\tValidation Loss: 0.023746340982514304\n",
            "Epoch 464  \tTraining Loss: 0.005711842012283602\tValidation Loss: 0.023738570807101577\n",
            "Epoch 465  \tTraining Loss: 0.005709615964352984\tValidation Loss: 0.023730815853693844\n",
            "Epoch 466  \tTraining Loss: 0.005707398556841289\tValidation Loss: 0.02372304243051942\n",
            "Epoch 467  \tTraining Loss: 0.005705191762281052\tValidation Loss: 0.023715268286392303\n",
            "Epoch 468  \tTraining Loss: 0.005702993005678498\tValidation Loss: 0.02370744203689141\n",
            "Epoch 469  \tTraining Loss: 0.005700793343988152\tValidation Loss: 0.023699633906385142\n",
            "Epoch 470  \tTraining Loss: 0.0056986011635385\tValidation Loss: 0.02369183106696202\n",
            "Epoch 471  \tTraining Loss: 0.005696410709253893\tValidation Loss: 0.02368403250683896\n",
            "Epoch 472  \tTraining Loss: 0.005694223950968417\tValidation Loss: 0.02367625737221816\n",
            "Epoch 473  \tTraining Loss: 0.005692037756825151\tValidation Loss: 0.023668464349451825\n",
            "Epoch 474  \tTraining Loss: 0.005689851485010376\tValidation Loss: 0.023660690361743827\n",
            "Epoch 475  \tTraining Loss: 0.005687673507566199\tValidation Loss: 0.023652931463297267\n",
            "Epoch 476  \tTraining Loss: 0.005685505455826143\tValidation Loss: 0.023645130497473056\n",
            "Epoch 477  \tTraining Loss: 0.005683340179723348\tValidation Loss: 0.023637346549597687\n",
            "Epoch 478  \tTraining Loss: 0.005681182807828976\tValidation Loss: 0.023629555808025643\n",
            "Epoch 479  \tTraining Loss: 0.005679017221768274\tValidation Loss: 0.023621777869986746\n",
            "Epoch 480  \tTraining Loss: 0.005676850397883568\tValidation Loss: 0.02361401039216142\n",
            "Epoch 481  \tTraining Loss: 0.005674684811494056\tValidation Loss: 0.023606270402411502\n",
            "Epoch 482  \tTraining Loss: 0.005672523648952259\tValidation Loss: 0.023598552304826787\n",
            "Epoch 483  \tTraining Loss: 0.0056703661168511586\tValidation Loss: 0.02359079651372738\n",
            "Epoch 484  \tTraining Loss: 0.005668209106246023\tValidation Loss: 0.023583028251802654\n",
            "Epoch 485  \tTraining Loss: 0.0056660548405269\tValidation Loss: 0.02357532980764822\n",
            "Epoch 486  \tTraining Loss: 0.005663895507916825\tValidation Loss: 0.023567635448510354\n",
            "Epoch 487  \tTraining Loss: 0.005661740697705759\tValidation Loss: 0.023559943104493913\n",
            "Epoch 488  \tTraining Loss: 0.005659588216445859\tValidation Loss: 0.023552254903448245\n",
            "Epoch 489  \tTraining Loss: 0.005657440786652459\tValidation Loss: 0.02354459101640477\n",
            "Epoch 490  \tTraining Loss: 0.005655294468820186\tValidation Loss: 0.023536995919810417\n",
            "Epoch 491  \tTraining Loss: 0.005653151740800929\tValidation Loss: 0.023529406936008777\n",
            "Epoch 492  \tTraining Loss: 0.0056510065421986385\tValidation Loss: 0.0235217946905726\n",
            "Epoch 493  \tTraining Loss: 0.005648863558411932\tValidation Loss: 0.023514157586593858\n",
            "Epoch 494  \tTraining Loss: 0.005646720765445694\tValidation Loss: 0.023506531901251574\n",
            "Epoch 495  \tTraining Loss: 0.0056445754127555006\tValidation Loss: 0.023498887980760608\n",
            "Epoch 496  \tTraining Loss: 0.00564242288972748\tValidation Loss: 0.023491198139201627\n",
            "Epoch 497  \tTraining Loss: 0.005640264401375142\tValidation Loss: 0.02348345420302146\n",
            "Epoch 498  \tTraining Loss: 0.005638107841559422\tValidation Loss: 0.023475702867234022\n",
            "Epoch 499  \tTraining Loss: 0.00563594984432167\tValidation Loss: 0.023467913983206835\n",
            "Epoch 500  \tTraining Loss: 0.00563378361452961\tValidation Loss: 0.023460120872414822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot your training and validation loss by epoch\n"
      ],
      "metadata": {
        "id": "PyOLmrJ5lEob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.plot(per_epoch_loss_train, label = 'Training')\n",
        "plt.plot(per_epoch_loss_val, label = 'Validation')\n",
        "\n",
        "\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dB14Zd7imO1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b6b9449a-a5d0-44b2-ea02-1383eebb3ad7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantify average reconstruction error & explain chosen hyperparameter values"
      ],
      "metadata": {
        "id": "8xVYT1TZlGBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_error = np.mean(per_epoch_loss_val)\n",
        "reconstruction_error"
      ],
      "metadata": {
        "id": "PN9r01bkmPQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0de44ed1-4232-4da4-e4a3-3cbc7a15c934"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02848846774497668"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining chosen hyperparameter values:\n",
        "\n",
        "I tried several combinations of hyperparameters, adjusting the learning rate, batch size, and epochs (as seen above). Overall, the final ones chosen were learning rate = 1e-3, batch size = 400, and epochs = 500.\n"
      ],
      "metadata": {
        "id": "KOx1FUX5G4qZ"
      }
    }
  ]
}