{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Background**\n",
        "Transcription factors are proteins that bind DNA at promoters to drive gene expression. Most preferentially bind to specific sequences while ignoring others. Traditional methods to determine these sequences (called motifs) have assumed that binding sites in the genome are all independent. However, in some cases people have identified motifs where positional interdependencies exist.\n",
        "\n",
        "# **Task:**\n",
        "Implement a multi-layer fully connected neural network using your NeuralNetwork class to predict whether a short DNA sequence is a binding site for the yeast transcription factor Rap1. The training data is incredibly imbalanced, with way fewer positive sequences than negative sequences, so you will implement a sampling scheme to ensure that class imbalance does not affect training."
      ],
      "metadata": {
        "id": "mgBM2zwP6sqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading nn.py\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ZlChGuAhJJpV",
        "outputId": "ed586718-ae94-4c7b-fd0a-9a9a21847743"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-25a2bd5f-6c85-4254-8289-21e469647491\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-25a2bd5f-6c85-4254-8289-21e469647491\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nn.py to nn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mrgA_hX_e-5v"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from nn import NeuralNetwork\n",
        "from io import read_text_file, read_fasta_file\n",
        "from preprocess import one_hot_encode_seqs, sample_seqs\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the read_text_file function from io.py to read in the 137 positive Rap1 motif examples"
      ],
      "metadata": {
        "id": "vWO7AsUs7PO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_seqs = read_text_file('./data/rap1-lieb-positives.txt')"
      ],
      "metadata": {
        "id": "JE4yBS-N6sao"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the read_fasta_file function from io.py to read in all the negative examples (Note that these sequences are much longer than the positive sequences, so you will need to process them to the same length)\n"
      ],
      "metadata": {
        "id": "WtZ_5T377nDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_seqs = read_fasta_file('./data/yeast-upstream-1k-negative.fa')"
      ],
      "metadata": {
        "id": "yfWmImiU7_g5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling Scheme (see comments within code as well):\n",
        "\n",
        "1. Balance your classes using sample_seq function\n",
        "\n",
        "2. One-hot encode the data using your one_hot_encode_seqs function\n",
        "\n",
        "3. Split the data into training and validation sets\n"
      ],
      "metadata": {
        "id": "DZwVHMHN7oE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Ensure both positive and negative sequences are of the same length\n",
        "min_seq_length = min(len(seq) for seq in positive_seqs)  # Get the shortest sequence length\n",
        "negative_seqs_trunc = [seq[:min_seq_length] for seq in negative_seqs]  # Truncate negative sequences\n",
        "\n",
        "# Step 2: Combine positive and negative sequences\n",
        "combined_seqs = positive_seqs + negative_seqs_trunc\n",
        "\n",
        "# Step 3: Create labels for sequences (1 for positive, 0 for negative)\n",
        "combined_labels = [1] * len(positive_seqs) + [0] * len(negative_seqs_trunc)\n",
        "\n",
        "# Step 4: Shuffle the dataset to randomize the order\n",
        "shuffled_indices = list(range(len(combined_seqs)))\n",
        "random.shuffle(shuffled_indices)\n",
        "\n",
        "# Shuffle both sequences and labels\n",
        "shuffled_seqs = [combined_seqs[i] for i in shuffled_indices]\n",
        "shuffled_labels = [combined_labels[i] for i in shuffled_indices]\n",
        "\n",
        "# Step 5: Balance the dataset by sampling (oversampling the minority class)\n",
        "sampled_seqs, sampled_labels = sample_seqs(shuffled_seqs, shuffled_labels, seed=8)\n",
        "\n",
        "# Step 6: One-hot encode the sequences\n",
        "encoded_seqs = one_hot_encode_seqs(sampled_seqs)\n",
        "\n",
        "#  Reshape encoded_seqs to match the number of samples in sampled_labels\n",
        "encoded_seqs = encoded_seqs.reshape(len(sampled_labels), -1) # Reshape encoded_seqs to have the correct number of samples (rows)\n",
        "\n",
        "# Step 7: Split the dataset into training and validation sets (80/20 split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(encoded_seqs, sampled_labels, test_size=0.2, random_state=88)\n",
        "\n",
        "# Step 8: Convert to numpy arrays for use in the model\n",
        "X_train = np.array(X_train)\n",
        "X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)"
      ],
      "metadata": {
        "id": "hK6MuJnPhWjW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W3wSG4e8BG0",
        "outputId": "b2e4c93c-908c-409e-ae88-c820bba7a26d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5060, 68)\n",
            "(5060,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate an instance of your NeuralNetwork class with an appropriate architecture"
      ],
      "metadata": {
        "id": "e2nT6Fan7yJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_architecture = [\n",
        "    {'input_dim': 68, 'output_dim': 34, 'activation': 'sigmoid'},  # Input -> Hidden layer (68 -> 34)\n",
        "    {'input_dim': 34, 'output_dim': 17, 'activation': 'sigmoid'}, # Hidden -> Hidden layer (34 -> 17)\n",
        "    {'input_dim': 17, 'output_dim': 1, 'activation': 'sigmoid'}   # Hidden -> Output layer (17 -> 1)\n",
        "]\n",
        "\n",
        "learning_rate = 1e-2\n",
        "random_seed = 8\n",
        "batch_size = 100\n",
        "epochs = 10000\n",
        "loss_function = 'binary_cross_entropy'\n",
        "\n",
        "network = NeuralNetwork(nn_architecture, learning_rate, random_seed, batch_size, epochs, loss_function)"
      ],
      "metadata": {
        "id": "_6D_yDiQ8EUi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your neural network on the training data"
      ],
      "metadata": {
        "id": "KLqqGmGK70Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "nn_architecture = [\n",
        "    {'input_dim': 68, 'output_dim': 34, 'activation': 'sigmoid'},  # Input -> Hidden layer (68 -> 34)\n",
        "    {'input_dim': 34, 'output_dim': 17, 'activation': 'sigmoid'},  # Hidden -> Hidden layer (34 -> 17)\n",
        "    {'input_dim': 17, 'output_dim': 1, 'activation': 'sigmoid'}    # Hidden -> Output layer (17 -> 1)\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store the validation losses\n",
        "validation_loss = []\n",
        "\n",
        "# List of hyperparameters to test\n",
        "lr_test = [1e-2, 1e-3, 1e-4]\n",
        "batch_size_test = [200, 300, 400, 500]\n",
        "hyperparameters = [(x, y) for x in lr_test for y in batch_size_test]\n",
        "\n",
        "# Loss function and other settings\n",
        "loss_function = 'binary_cross_entropy'\n",
        "epochs = 500\n",
        "random_seed = 8\n",
        "\n",
        "# Loop through all hyperparameter combinations\n",
        "for var in hyperparameters:\n",
        "    print('lr, batch_size:', var)  # Print the current hyperparameters\n",
        "\n",
        "    # Create the neural network with the current hyperparameters\n",
        "    network = NeuralNetwork(\n",
        "        nn_architecture,\n",
        "        lr=var[0],\n",
        "        seed=random_seed,\n",
        "        batch_size=var[1],\n",
        "        epochs=epochs,\n",
        "        loss_function=loss_function\n",
        "    )\n",
        "\n",
        "    # Train the network\n",
        "    per_epoch_loss_train, per_epoch_loss_val = network.fit(X_train.T, X_train.T, X_val.T, X_val.T)\n",
        "\n",
        "    # Store the average of the last 10 epochs of validation loss\n",
        "    validation_loss.append(np.mean(per_epoch_loss_val[-10:]))\n",
        "\n",
        "# Print the results of validation losses for each combination of hyperparameters\n",
        "print(\"Validation loss list:\", validation_loss)"
      ],
      "metadata": {
        "id": "V0pUhdsWSm8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Classifier"
      ],
      "metadata": {
        "id": "ctrNo69nShYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_classifier = NeuralNetwork(\n",
        "    nn_architecture,\n",
        "    lr=1e-2,\n",
        "    seed=random_seed,\n",
        "    batch_size=10,\n",
        "    epochs=20000,\n",
        "    loss_function=loss_function\n",
        ")\n",
        "\n",
        "per_epoch_loss_train, per_epoch_loss_val = final_classifier.fit(np.array(X_train.T), np.array(y_train.T), np.array(X_val.T), np.array(y_val.T))"
      ],
      "metadata": {
        "id": "uGEVaYbd8FVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab2c197-c3ee-4d0d-ef04-9f027d3c002e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 15001  \tTraining Loss: 0.6917467252995307\tValidation Loss: 0.6917414142863415\n",
            "Epoch 15002  \tTraining Loss: 0.6917466659843842\tValidation Loss: 0.6917413544600116\n",
            "Epoch 15003  \tTraining Loss: 0.6917466066685422\tValidation Loss: 0.691741294632985\n",
            "Epoch 15004  \tTraining Loss: 0.6917465473520044\tValidation Loss: 0.6917412348052613\n",
            "Epoch 15005  \tTraining Loss: 0.6917464880347709\tValidation Loss: 0.6917411749768407\n",
            "Epoch 15006  \tTraining Loss: 0.6917464287168417\tValidation Loss: 0.6917411151477233\n",
            "Epoch 15007  \tTraining Loss: 0.6917463693982167\tValidation Loss: 0.691741055317909\n",
            "Epoch 15008  \tTraining Loss: 0.6917463100788958\tValidation Loss: 0.6917409954873975\n",
            "Epoch 15009  \tTraining Loss: 0.691746250758879\tValidation Loss: 0.6917409356561891\n",
            "Epoch 15010  \tTraining Loss: 0.6917461914381663\tValidation Loss: 0.6917408758242836\n",
            "Epoch 15011  \tTraining Loss: 0.6917461321167578\tValidation Loss: 0.6917408159916811\n",
            "Epoch 15012  \tTraining Loss: 0.6917460727946532\tValidation Loss: 0.6917407561583814\n",
            "Epoch 15013  \tTraining Loss: 0.6917460134718528\tValidation Loss: 0.6917406963243847\n",
            "Epoch 15014  \tTraining Loss: 0.6917459541483564\tValidation Loss: 0.6917406364896906\n",
            "Epoch 15015  \tTraining Loss: 0.6917458948241638\tValidation Loss: 0.6917405766542996\n",
            "Epoch 15016  \tTraining Loss: 0.6917458354992752\tValidation Loss: 0.6917405168182111\n",
            "Epoch 15017  \tTraining Loss: 0.6917457761736905\tValidation Loss: 0.6917404569814254\n",
            "Epoch 15018  \tTraining Loss: 0.6917457168474096\tValidation Loss: 0.6917403971439424\n",
            "Epoch 15019  \tTraining Loss: 0.6917456575204326\tValidation Loss: 0.6917403373057621\n",
            "Epoch 15020  \tTraining Loss: 0.6917455981927594\tValidation Loss: 0.6917402774668844\n",
            "Epoch 15021  \tTraining Loss: 0.6917455388643898\tValidation Loss: 0.6917402176273094\n",
            "Epoch 15022  \tTraining Loss: 0.6917454795353241\tValidation Loss: 0.691740157787037\n",
            "Epoch 15023  \tTraining Loss: 0.6917454202055621\tValidation Loss: 0.691740097946067\n",
            "Epoch 15024  \tTraining Loss: 0.6917453608751037\tValidation Loss: 0.6917400381043997\n",
            "Epoch 15025  \tTraining Loss: 0.691745301543949\tValidation Loss: 0.6917399782620347\n",
            "Epoch 15026  \tTraining Loss: 0.6917452422120979\tValidation Loss: 0.6917399184189722\n",
            "Epoch 15027  \tTraining Loss: 0.6917451828795503\tValidation Loss: 0.6917398585752123\n",
            "Epoch 15028  \tTraining Loss: 0.6917451235463064\tValidation Loss: 0.6917397987307545\n",
            "Epoch 15029  \tTraining Loss: 0.6917450642123656\tValidation Loss: 0.6917397388855993\n",
            "Epoch 15030  \tTraining Loss: 0.6917450048777286\tValidation Loss: 0.6917396790397463\n",
            "Epoch 15031  \tTraining Loss: 0.691744945542395\tValidation Loss: 0.6917396191931957\n",
            "Epoch 15032  \tTraining Loss: 0.6917448862063649\tValidation Loss: 0.6917395593459473\n",
            "Epoch 15033  \tTraining Loss: 0.691744826869638\tValidation Loss: 0.6917394994980012\n",
            "Epoch 15034  \tTraining Loss: 0.6917447675322147\tValidation Loss: 0.6917394396493571\n",
            "Epoch 15035  \tTraining Loss: 0.6917447081940945\tValidation Loss: 0.6917393798000155\n",
            "Epoch 15036  \tTraining Loss: 0.6917446488552775\tValidation Loss: 0.6917393199499758\n",
            "Epoch 15037  \tTraining Loss: 0.691744589515764\tValidation Loss: 0.6917392600992384\n",
            "Epoch 15038  \tTraining Loss: 0.6917445301755536\tValidation Loss: 0.691739200247803\n",
            "Epoch 15039  \tTraining Loss: 0.6917444708346464\tValidation Loss: 0.6917391403956696\n",
            "Epoch 15040  \tTraining Loss: 0.6917444114930423\tValidation Loss: 0.6917390805428383\n",
            "Epoch 15041  \tTraining Loss: 0.6917443521507415\tValidation Loss: 0.6917390206893089\n",
            "Epoch 15042  \tTraining Loss: 0.6917442928077436\tValidation Loss: 0.6917389608350817\n",
            "Epoch 15043  \tTraining Loss: 0.6917442334640489\tValidation Loss: 0.6917389009801561\n",
            "Epoch 15044  \tTraining Loss: 0.6917441741196572\tValidation Loss: 0.6917388411245327\n",
            "Epoch 15045  \tTraining Loss: 0.6917441147745684\tValidation Loss: 0.691738781268211\n",
            "Epoch 15046  \tTraining Loss: 0.6917440554287827\tValidation Loss: 0.6917387214111912\n",
            "Epoch 15047  \tTraining Loss: 0.6917439960823\tValidation Loss: 0.6917386615534731\n",
            "Epoch 15048  \tTraining Loss: 0.69174393673512\tValidation Loss: 0.691738601695057\n",
            "Epoch 15049  \tTraining Loss: 0.691743877387243\tValidation Loss: 0.6917385418359424\n",
            "Epoch 15050  \tTraining Loss: 0.6917438180386687\tValidation Loss: 0.6917384819761296\n",
            "Epoch 15051  \tTraining Loss: 0.6917437586893973\tValidation Loss: 0.6917384221156185\n",
            "Epoch 15052  \tTraining Loss: 0.6917436993394287\tValidation Loss: 0.691738362254409\n",
            "Epoch 15053  \tTraining Loss: 0.6917436399887628\tValidation Loss: 0.6917383023925012\n",
            "Epoch 15054  \tTraining Loss: 0.6917435806373996\tValidation Loss: 0.691738242529895\n",
            "Epoch 15055  \tTraining Loss: 0.6917435212853391\tValidation Loss: 0.6917381826665904\n",
            "Epoch 15056  \tTraining Loss: 0.6917434619325813\tValidation Loss: 0.6917381228025872\n",
            "Epoch 15057  \tTraining Loss: 0.691743402579126\tValidation Loss: 0.6917380629378855\n",
            "Epoch 15058  \tTraining Loss: 0.6917433432249734\tValidation Loss: 0.6917380030724855\n",
            "Epoch 15059  \tTraining Loss: 0.6917432838701233\tValidation Loss: 0.6917379432063867\n",
            "Epoch 15060  \tTraining Loss: 0.6917432245145757\tValidation Loss: 0.6917378833395893\n",
            "Epoch 15061  \tTraining Loss: 0.6917431651583308\tValidation Loss: 0.6917378234720933\n",
            "Epoch 15062  \tTraining Loss: 0.6917431058013881\tValidation Loss: 0.6917377636038987\n",
            "Epoch 15063  \tTraining Loss: 0.691743046443748\tValidation Loss: 0.6917377037350055\n",
            "Epoch 15064  \tTraining Loss: 0.69174298708541\tValidation Loss: 0.6917376438654135\n",
            "Epoch 15065  \tTraining Loss: 0.6917429277263746\tValidation Loss: 0.6917375839951226\n",
            "Epoch 15066  \tTraining Loss: 0.6917428683666416\tValidation Loss: 0.6917375241241331\n",
            "Epoch 15067  \tTraining Loss: 0.6917428090062108\tValidation Loss: 0.6917374642524448\n",
            "Epoch 15068  \tTraining Loss: 0.6917427496450821\tValidation Loss: 0.6917374043800576\n",
            "Epoch 15069  \tTraining Loss: 0.6917426902832557\tValidation Loss: 0.6917373445069716\n",
            "Epoch 15070  \tTraining Loss: 0.6917426309207317\tValidation Loss: 0.6917372846331866\n",
            "Epoch 15071  \tTraining Loss: 0.6917425715575097\tValidation Loss: 0.6917372247587026\n",
            "Epoch 15072  \tTraining Loss: 0.69174251219359\tValidation Loss: 0.6917371648835198\n",
            "Epoch 15073  \tTraining Loss: 0.6917424528289722\tValidation Loss: 0.691737105007638\n",
            "Epoch 15074  \tTraining Loss: 0.6917423934636565\tValidation Loss: 0.691737045131057\n",
            "Epoch 15075  \tTraining Loss: 0.691742334097643\tValidation Loss: 0.691736985253777\n",
            "Epoch 15076  \tTraining Loss: 0.6917422747309314\tValidation Loss: 0.691736925375798\n",
            "Epoch 15077  \tTraining Loss: 0.6917422153635219\tValidation Loss: 0.6917368654971199\n",
            "Epoch 15078  \tTraining Loss: 0.6917421559954142\tValidation Loss: 0.6917368056177426\n",
            "Epoch 15079  \tTraining Loss: 0.6917420966266085\tValidation Loss: 0.6917367457376661\n",
            "Epoch 15080  \tTraining Loss: 0.6917420372571046\tValidation Loss: 0.6917366858568904\n",
            "Epoch 15081  \tTraining Loss: 0.6917419778869026\tValidation Loss: 0.6917366259754155\n",
            "Epoch 15082  \tTraining Loss: 0.6917419185160024\tValidation Loss: 0.6917365660932413\n",
            "Epoch 15083  \tTraining Loss: 0.6917418591444041\tValidation Loss: 0.6917365062103678\n",
            "Epoch 15084  \tTraining Loss: 0.6917417997721074\tValidation Loss: 0.691736446326795\n",
            "Epoch 15085  \tTraining Loss: 0.6917417403991125\tValidation Loss: 0.6917363864425227\n",
            "Epoch 15086  \tTraining Loss: 0.6917416810254192\tValidation Loss: 0.6917363265575511\n",
            "Epoch 15087  \tTraining Loss: 0.6917416216510277\tValidation Loss: 0.69173626667188\n",
            "Epoch 15088  \tTraining Loss: 0.6917415622759377\tValidation Loss: 0.6917362067855095\n",
            "Epoch 15089  \tTraining Loss: 0.6917415029001495\tValidation Loss: 0.6917361468984395\n",
            "Epoch 15090  \tTraining Loss: 0.6917414435236626\tValidation Loss: 0.69173608701067\n",
            "Epoch 15091  \tTraining Loss: 0.6917413841464775\tValidation Loss: 0.691736027122201\n",
            "Epoch 15092  \tTraining Loss: 0.6917413247685937\tValidation Loss: 0.6917359672330323\n",
            "Epoch 15093  \tTraining Loss: 0.6917412653900115\tValidation Loss: 0.6917359073431639\n",
            "Epoch 15094  \tTraining Loss: 0.6917412060107306\tValidation Loss: 0.6917358474525961\n",
            "Epoch 15095  \tTraining Loss: 0.6917411466307511\tValidation Loss: 0.6917357875613285\n",
            "Epoch 15096  \tTraining Loss: 0.6917410872500731\tValidation Loss: 0.6917357276693612\n",
            "Epoch 15097  \tTraining Loss: 0.6917410278686964\tValidation Loss: 0.6917356677766942\n",
            "Epoch 15098  \tTraining Loss: 0.691740968486621\tValidation Loss: 0.6917356078833273\n",
            "Epoch 15099  \tTraining Loss: 0.6917409091038469\tValidation Loss: 0.6917355479892607\n",
            "Epoch 15100  \tTraining Loss: 0.691740849720374\tValidation Loss: 0.6917354880944944\n",
            "Epoch 15101  \tTraining Loss: 0.6917407903362024\tValidation Loss: 0.6917354281990281\n",
            "Epoch 15102  \tTraining Loss: 0.6917407309513319\tValidation Loss: 0.6917353683028619\n",
            "Epoch 15103  \tTraining Loss: 0.6917406715657626\tValidation Loss: 0.6917353084059957\n",
            "Epoch 15104  \tTraining Loss: 0.6917406121794942\tValidation Loss: 0.6917352485084297\n",
            "Epoch 15105  \tTraining Loss: 0.6917405527925272\tValidation Loss: 0.6917351886101637\n",
            "Epoch 15106  \tTraining Loss: 0.6917404934048611\tValidation Loss: 0.6917351287111977\n",
            "Epoch 15107  \tTraining Loss: 0.6917404340164961\tValidation Loss: 0.6917350688115316\n",
            "Epoch 15108  \tTraining Loss: 0.691740374627432\tValidation Loss: 0.6917350089111652\n",
            "Epoch 15109  \tTraining Loss: 0.691740315237669\tValidation Loss: 0.691734949010099\n",
            "Epoch 15110  \tTraining Loss: 0.6917402558472068\tValidation Loss: 0.6917348891083327\n",
            "Epoch 15111  \tTraining Loss: 0.6917401964560455\tValidation Loss: 0.6917348292058659\n",
            "Epoch 15112  \tTraining Loss: 0.691740137064185\tValidation Loss: 0.6917347693026993\n",
            "Epoch 15113  \tTraining Loss: 0.6917400776716254\tValidation Loss: 0.6917347093988322\n",
            "Epoch 15114  \tTraining Loss: 0.6917400182783666\tValidation Loss: 0.6917346494942649\n",
            "Epoch 15115  \tTraining Loss: 0.6917399588844085\tValidation Loss: 0.6917345895889973\n",
            "Epoch 15116  \tTraining Loss: 0.6917398994897515\tValidation Loss: 0.6917345296830293\n",
            "Epoch 15117  \tTraining Loss: 0.6917398400943947\tValidation Loss: 0.6917344697763611\n",
            "Epoch 15118  \tTraining Loss: 0.6917397806983389\tValidation Loss: 0.6917344098689923\n",
            "Epoch 15119  \tTraining Loss: 0.6917397213015836\tValidation Loss: 0.6917343499609233\n",
            "Epoch 15120  \tTraining Loss: 0.6917396619041289\tValidation Loss: 0.6917342900521538\n",
            "Epoch 15121  \tTraining Loss: 0.6917396025059748\tValidation Loss: 0.6917342301426838\n",
            "Epoch 15122  \tTraining Loss: 0.6917395431071213\tValidation Loss: 0.6917341702325133\n",
            "Epoch 15123  \tTraining Loss: 0.6917394837075683\tValidation Loss: 0.6917341103216421\n",
            "Epoch 15124  \tTraining Loss: 0.6917394243073157\tValidation Loss: 0.6917340504100705\n",
            "Epoch 15125  \tTraining Loss: 0.6917393649063636\tValidation Loss: 0.6917339904977983\n",
            "Epoch 15126  \tTraining Loss: 0.691739305504712\tValidation Loss: 0.6917339305848254\n",
            "Epoch 15127  \tTraining Loss: 0.6917392461023606\tValidation Loss: 0.6917338706711518\n",
            "Epoch 15128  \tTraining Loss: 0.6917391866993098\tValidation Loss: 0.6917338107567775\n",
            "Epoch 15129  \tTraining Loss: 0.6917391272955591\tValidation Loss: 0.6917337508417025\n",
            "Epoch 15130  \tTraining Loss: 0.6917390678911087\tValidation Loss: 0.6917336909259267\n",
            "Epoch 15131  \tTraining Loss: 0.6917390084859586\tValidation Loss: 0.6917336310094503\n",
            "Epoch 15132  \tTraining Loss: 0.6917389490801088\tValidation Loss: 0.691733571092273\n",
            "Epoch 15133  \tTraining Loss: 0.6917388896735592\tValidation Loss: 0.6917335111743949\n",
            "Epoch 15134  \tTraining Loss: 0.6917388302663097\tValidation Loss: 0.6917334512558156\n",
            "Epoch 15135  \tTraining Loss: 0.6917387708583602\tValidation Loss: 0.6917333913365356\n",
            "Epoch 15136  \tTraining Loss: 0.691738711449711\tValidation Loss: 0.6917333314165548\n",
            "Epoch 15137  \tTraining Loss: 0.6917386520403618\tValidation Loss: 0.6917332714958728\n",
            "Epoch 15138  \tTraining Loss: 0.6917385926303126\tValidation Loss: 0.6917332115744899\n",
            "Epoch 15139  \tTraining Loss: 0.6917385332195635\tValidation Loss: 0.6917331516524059\n",
            "Epoch 15140  \tTraining Loss: 0.6917384738081143\tValidation Loss: 0.6917330917296208\n",
            "Epoch 15141  \tTraining Loss: 0.691738414395965\tValidation Loss: 0.6917330318061348\n",
            "Epoch 15142  \tTraining Loss: 0.6917383549831156\tValidation Loss: 0.6917329718819475\n",
            "Epoch 15143  \tTraining Loss: 0.6917382955695662\tValidation Loss: 0.6917329119570591\n",
            "Epoch 15144  \tTraining Loss: 0.6917382361553165\tValidation Loss: 0.6917328520314694\n",
            "Epoch 15145  \tTraining Loss: 0.6917381767403666\tValidation Loss: 0.6917327921051786\n",
            "Epoch 15146  \tTraining Loss: 0.6917381173247166\tValidation Loss: 0.6917327321781865\n",
            "Epoch 15147  \tTraining Loss: 0.6917380579083663\tValidation Loss: 0.6917326722504932\n",
            "Epoch 15148  \tTraining Loss: 0.6917379984913158\tValidation Loss: 0.6917326123220984\n",
            "Epoch 15149  \tTraining Loss: 0.6917379390735647\tValidation Loss: 0.6917325523930024\n",
            "Epoch 15150  \tTraining Loss: 0.6917378796551135\tValidation Loss: 0.691732492463205\n",
            "Epoch 15151  \tTraining Loss: 0.691737820235962\tValidation Loss: 0.6917324325327061\n",
            "Epoch 15152  \tTraining Loss: 0.6917377608161098\tValidation Loss: 0.6917323726015058\n",
            "Epoch 15153  \tTraining Loss: 0.6917377013955573\tValidation Loss: 0.691732312669604\n",
            "Epoch 15154  \tTraining Loss: 0.6917376419743044\tValidation Loss: 0.6917322527370008\n",
            "Epoch 15155  \tTraining Loss: 0.691737582552351\tValidation Loss: 0.691732192803696\n",
            "Epoch 15156  \tTraining Loss: 0.6917375231296968\tValidation Loss: 0.6917321328696897\n",
            "Epoch 15157  \tTraining Loss: 0.6917374637063424\tValidation Loss: 0.6917320729349818\n",
            "Epoch 15158  \tTraining Loss: 0.691737404282287\tValidation Loss: 0.6917320129995722\n",
            "Epoch 15159  \tTraining Loss: 0.6917373448575312\tValidation Loss: 0.6917319530634609\n",
            "Epoch 15160  \tTraining Loss: 0.6917372854320748\tValidation Loss: 0.6917318931266481\n",
            "Epoch 15161  \tTraining Loss: 0.6917372260059175\tValidation Loss: 0.6917318331891333\n",
            "Epoch 15162  \tTraining Loss: 0.6917371665790596\tValidation Loss: 0.691731773250917\n",
            "Epoch 15163  \tTraining Loss: 0.6917371071515009\tValidation Loss: 0.6917317133119988\n",
            "Epoch 15164  \tTraining Loss: 0.6917370477232414\tValidation Loss: 0.6917316533723789\n",
            "Epoch 15165  \tTraining Loss: 0.6917369882942812\tValidation Loss: 0.6917315934320569\n",
            "Epoch 15166  \tTraining Loss: 0.6917369288646199\tValidation Loss: 0.6917315334910332\n",
            "Epoch 15167  \tTraining Loss: 0.6917368694342578\tValidation Loss: 0.6917314735493076\n",
            "Epoch 15168  \tTraining Loss: 0.691736810003195\tValidation Loss: 0.6917314136068802\n",
            "Epoch 15169  \tTraining Loss: 0.6917367505714309\tValidation Loss: 0.6917313536637506\n",
            "Epoch 15170  \tTraining Loss: 0.6917366911389659\tValidation Loss: 0.6917312937199189\n",
            "Epoch 15171  \tTraining Loss: 0.6917366317057999\tValidation Loss: 0.6917312337753855\n",
            "Epoch 15172  \tTraining Loss: 0.691736572271933\tValidation Loss: 0.6917311738301499\n",
            "Epoch 15173  \tTraining Loss: 0.6917365128373648\tValidation Loss: 0.6917311138842122\n",
            "Epoch 15174  \tTraining Loss: 0.6917364534020957\tValidation Loss: 0.6917310539375723\n",
            "Epoch 15175  \tTraining Loss: 0.6917363939661253\tValidation Loss: 0.6917309939902303\n",
            "Epoch 15176  \tTraining Loss: 0.6917363345294538\tValidation Loss: 0.6917309340421862\n",
            "Epoch 15177  \tTraining Loss: 0.691736275092081\tValidation Loss: 0.6917308740934398\n",
            "Epoch 15178  \tTraining Loss: 0.691736215654007\tValidation Loss: 0.6917308141439913\n",
            "Epoch 15179  \tTraining Loss: 0.6917361562152319\tValidation Loss: 0.6917307541938403\n",
            "Epoch 15180  \tTraining Loss: 0.6917360967757552\tValidation Loss: 0.691730694242987\n",
            "Epoch 15181  \tTraining Loss: 0.6917360373355773\tValidation Loss: 0.6917306342914314\n",
            "Epoch 15182  \tTraining Loss: 0.691735977894698\tValidation Loss: 0.6917305743391736\n",
            "Epoch 15183  \tTraining Loss: 0.6917359184531173\tValidation Loss: 0.6917305143862132\n",
            "Epoch 15184  \tTraining Loss: 0.6917358590108351\tValidation Loss: 0.6917304544325505\n",
            "Epoch 15185  \tTraining Loss: 0.6917357995678516\tValidation Loss: 0.6917303944781854\n",
            "Epoch 15186  \tTraining Loss: 0.6917357401241666\tValidation Loss: 0.6917303345231175\n",
            "Epoch 15187  \tTraining Loss: 0.69173568067978\tValidation Loss: 0.6917302745673473\n",
            "Epoch 15188  \tTraining Loss: 0.6917356212346918\tValidation Loss: 0.6917302146108745\n",
            "Epoch 15189  \tTraining Loss: 0.6917355617889019\tValidation Loss: 0.691730154653699\n",
            "Epoch 15190  \tTraining Loss: 0.6917355023424107\tValidation Loss: 0.6917300946958211\n",
            "Epoch 15191  \tTraining Loss: 0.6917354428952175\tValidation Loss: 0.6917300347372404\n",
            "Epoch 15192  \tTraining Loss: 0.6917353834473228\tValidation Loss: 0.6917299747779571\n",
            "Epoch 15193  \tTraining Loss: 0.6917353239987263\tValidation Loss: 0.6917299148179711\n",
            "Epoch 15194  \tTraining Loss: 0.6917352645494281\tValidation Loss: 0.6917298548572822\n",
            "Epoch 15195  \tTraining Loss: 0.6917352050994281\tValidation Loss: 0.6917297948958906\n",
            "Epoch 15196  \tTraining Loss: 0.6917351456487263\tValidation Loss: 0.6917297349337962\n",
            "Epoch 15197  \tTraining Loss: 0.6917350861973227\tValidation Loss: 0.6917296749709989\n",
            "Epoch 15198  \tTraining Loss: 0.6917350267452171\tValidation Loss: 0.6917296150074989\n",
            "Epoch 15199  \tTraining Loss: 0.6917349672924097\tValidation Loss: 0.6917295550432958\n",
            "Epoch 15200  \tTraining Loss: 0.6917349078389002\tValidation Loss: 0.6917294950783899\n",
            "Epoch 15201  \tTraining Loss: 0.6917348483846888\tValidation Loss: 0.6917294351127811\n",
            "Epoch 15202  \tTraining Loss: 0.6917347889297755\tValidation Loss: 0.6917293751464693\n",
            "Epoch 15203  \tTraining Loss: 0.69173472947416\tValidation Loss: 0.6917293151794544\n",
            "Epoch 15204  \tTraining Loss: 0.6917346700178425\tValidation Loss: 0.6917292552117363\n",
            "Epoch 15205  \tTraining Loss: 0.691734610560823\tValidation Loss: 0.6917291952433153\n",
            "Epoch 15206  \tTraining Loss: 0.6917345511031012\tValidation Loss: 0.6917291352741911\n",
            "Epoch 15207  \tTraining Loss: 0.6917344916446772\tValidation Loss: 0.6917290753043637\n",
            "Epoch 15208  \tTraining Loss: 0.6917344321855511\tValidation Loss: 0.6917290153338332\n",
            "Epoch 15209  \tTraining Loss: 0.6917343727257228\tValidation Loss: 0.6917289553625996\n",
            "Epoch 15210  \tTraining Loss: 0.6917343132651921\tValidation Loss: 0.6917288953906626\n",
            "Epoch 15211  \tTraining Loss: 0.6917342538039591\tValidation Loss: 0.6917288354180225\n",
            "Epoch 15212  \tTraining Loss: 0.691734194342024\tValidation Loss: 0.6917287754446789\n",
            "Epoch 15213  \tTraining Loss: 0.6917341348793864\tValidation Loss: 0.6917287154706321\n",
            "Epoch 15214  \tTraining Loss: 0.6917340754160464\tValidation Loss: 0.6917286554958818\n",
            "Epoch 15215  \tTraining Loss: 0.6917340159520039\tValidation Loss: 0.6917285955204282\n",
            "Epoch 15216  \tTraining Loss: 0.691733956487259\tValidation Loss: 0.6917285355442712\n",
            "Epoch 15217  \tTraining Loss: 0.6917338970218117\tValidation Loss: 0.6917284755674108\n",
            "Epoch 15218  \tTraining Loss: 0.6917338375556619\tValidation Loss: 0.6917284155898468\n",
            "Epoch 15219  \tTraining Loss: 0.6917337780888094\tValidation Loss: 0.6917283556115792\n",
            "Epoch 15220  \tTraining Loss: 0.6917337186212544\tValidation Loss: 0.6917282956326081\n",
            "Epoch 15221  \tTraining Loss: 0.6917336591529968\tValidation Loss: 0.6917282356529335\n",
            "Epoch 15222  \tTraining Loss: 0.6917335996840365\tValidation Loss: 0.6917281756725553\n",
            "Epoch 15223  \tTraining Loss: 0.6917335402143735\tValidation Loss: 0.6917281156914734\n",
            "Epoch 15224  \tTraining Loss: 0.6917334807440078\tValidation Loss: 0.6917280557096878\n",
            "Epoch 15225  \tTraining Loss: 0.6917334212729394\tValidation Loss: 0.6917279957271985\n",
            "Epoch 15226  \tTraining Loss: 0.6917333618011682\tValidation Loss: 0.6917279357440054\n",
            "Epoch 15227  \tTraining Loss: 0.6917333023286942\tValidation Loss: 0.6917278757601086\n",
            "Epoch 15228  \tTraining Loss: 0.6917332428555174\tValidation Loss: 0.6917278157755081\n",
            "Epoch 15229  \tTraining Loss: 0.6917331833816377\tValidation Loss: 0.6917277557902037\n",
            "Epoch 15230  \tTraining Loss: 0.691733123907055\tValidation Loss: 0.6917276958041954\n",
            "Epoch 15231  \tTraining Loss: 0.6917330644317694\tValidation Loss: 0.6917276358174832\n",
            "Epoch 15232  \tTraining Loss: 0.6917330049557809\tValidation Loss: 0.691727575830067\n",
            "Epoch 15233  \tTraining Loss: 0.6917329454790895\tValidation Loss: 0.691727515841947\n",
            "Epoch 15234  \tTraining Loss: 0.6917328860016949\tValidation Loss: 0.691727455853123\n",
            "Epoch 15235  \tTraining Loss: 0.6917328265235972\tValidation Loss: 0.6917273958635949\n",
            "Epoch 15236  \tTraining Loss: 0.6917327670447966\tValidation Loss: 0.6917273358733629\n",
            "Epoch 15237  \tTraining Loss: 0.6917327075652927\tValidation Loss: 0.6917272758824267\n",
            "Epoch 15238  \tTraining Loss: 0.6917326480850857\tValidation Loss: 0.6917272158907862\n",
            "Epoch 15239  \tTraining Loss: 0.6917325886041755\tValidation Loss: 0.6917271558984418\n",
            "Epoch 15240  \tTraining Loss: 0.6917325291225621\tValidation Loss: 0.6917270959053933\n",
            "Epoch 15241  \tTraining Loss: 0.6917324696402455\tValidation Loss: 0.6917270359116404\n",
            "Epoch 15242  \tTraining Loss: 0.6917324101572255\tValidation Loss: 0.6917269759171834\n",
            "Epoch 15243  \tTraining Loss: 0.6917323506735022\tValidation Loss: 0.6917269159220221\n",
            "Epoch 15244  \tTraining Loss: 0.6917322911890755\tValidation Loss: 0.6917268559261565\n",
            "Epoch 15245  \tTraining Loss: 0.6917322317039456\tValidation Loss: 0.6917267959295864\n",
            "Epoch 15246  \tTraining Loss: 0.6917321722181122\tValidation Loss: 0.6917267359323122\n",
            "Epoch 15247  \tTraining Loss: 0.6917321127315753\tValidation Loss: 0.6917266759343333\n",
            "Epoch 15248  \tTraining Loss: 0.691732053244335\tValidation Loss: 0.6917266159356502\n",
            "Epoch 15249  \tTraining Loss: 0.6917319937563912\tValidation Loss: 0.6917265559362626\n",
            "Epoch 15250  \tTraining Loss: 0.6917319342677437\tValidation Loss: 0.6917264959361705\n",
            "Epoch 15251  \tTraining Loss: 0.6917318747783928\tValidation Loss: 0.691726435935374\n",
            "Epoch 15252  \tTraining Loss: 0.6917318152883382\tValidation Loss: 0.6917263759338729\n",
            "Epoch 15253  \tTraining Loss: 0.69173175579758\tValidation Loss: 0.6917263159316671\n",
            "Epoch 15254  \tTraining Loss: 0.6917316963061181\tValidation Loss: 0.6917262559287568\n",
            "Epoch 15255  \tTraining Loss: 0.6917316368139524\tValidation Loss: 0.6917261959251418\n",
            "Epoch 15256  \tTraining Loss: 0.6917315773210833\tValidation Loss: 0.6917261359208222\n",
            "Epoch 15257  \tTraining Loss: 0.6917315178275101\tValidation Loss: 0.6917260759157979\n",
            "Epoch 15258  \tTraining Loss: 0.6917314583332331\tValidation Loss: 0.6917260159100688\n",
            "Epoch 15259  \tTraining Loss: 0.6917313988382524\tValidation Loss: 0.6917259559036348\n",
            "Epoch 15260  \tTraining Loss: 0.6917313393425678\tValidation Loss: 0.6917258958964964\n",
            "Epoch 15261  \tTraining Loss: 0.6917312798461794\tValidation Loss: 0.6917258358886529\n",
            "Epoch 15262  \tTraining Loss: 0.691731220349087\tValidation Loss: 0.6917257758801045\n",
            "Epoch 15263  \tTraining Loss: 0.6917311608512906\tValidation Loss: 0.6917257158708514\n",
            "Epoch 15264  \tTraining Loss: 0.6917311013527903\tValidation Loss: 0.6917256558608933\n",
            "Epoch 15265  \tTraining Loss: 0.6917310418535859\tValidation Loss: 0.6917255958502302\n",
            "Epoch 15266  \tTraining Loss: 0.6917309823536775\tValidation Loss: 0.6917255358388621\n",
            "Epoch 15267  \tTraining Loss: 0.6917309228530649\tValidation Loss: 0.691725475826789\n",
            "Epoch 15268  \tTraining Loss: 0.6917308633517484\tValidation Loss: 0.6917254158140109\n",
            "Epoch 15269  \tTraining Loss: 0.6917308038497275\tValidation Loss: 0.6917253558005277\n",
            "Epoch 15270  \tTraining Loss: 0.6917307443470025\tValidation Loss: 0.6917252957863395\n",
            "Epoch 15271  \tTraining Loss: 0.6917306848435734\tValidation Loss: 0.691725235771446\n",
            "Epoch 15272  \tTraining Loss: 0.69173062533944\tValidation Loss: 0.6917251757558474\n",
            "Epoch 15273  \tTraining Loss: 0.6917305658346024\tValidation Loss: 0.6917251157395437\n",
            "Epoch 15274  \tTraining Loss: 0.6917305063290603\tValidation Loss: 0.6917250557225345\n",
            "Epoch 15275  \tTraining Loss: 0.6917304468228139\tValidation Loss: 0.6917249957048203\n",
            "Epoch 15276  \tTraining Loss: 0.6917303873158632\tValidation Loss: 0.6917249356864006\n",
            "Epoch 15277  \tTraining Loss: 0.691730327808208\tValidation Loss: 0.6917248756672757\n",
            "Epoch 15278  \tTraining Loss: 0.6917302682998485\tValidation Loss: 0.6917248156474454\n",
            "Epoch 15279  \tTraining Loss: 0.6917302087907844\tValidation Loss: 0.6917247556269097\n",
            "Epoch 15280  \tTraining Loss: 0.691730149281016\tValidation Loss: 0.6917246956056687\n",
            "Epoch 15281  \tTraining Loss: 0.6917300897705428\tValidation Loss: 0.6917246355837221\n",
            "Epoch 15282  \tTraining Loss: 0.6917300302593652\tValidation Loss: 0.6917245755610701\n",
            "Epoch 15283  \tTraining Loss: 0.691729970747483\tValidation Loss: 0.6917245155377126\n",
            "Epoch 15284  \tTraining Loss: 0.6917299112348961\tValidation Loss: 0.6917244555136496\n",
            "Epoch 15285  \tTraining Loss: 0.6917298517216045\tValidation Loss: 0.6917243954888808\n",
            "Epoch 15286  \tTraining Loss: 0.6917297922076083\tValidation Loss: 0.6917243354634067\n",
            "Epoch 15287  \tTraining Loss: 0.6917297326929074\tValidation Loss: 0.6917242754372267\n",
            "Epoch 15288  \tTraining Loss: 0.6917296731775017\tValidation Loss: 0.6917242154103411\n",
            "Epoch 15289  \tTraining Loss: 0.6917296136613912\tValidation Loss: 0.6917241553827499\n",
            "Epoch 15290  \tTraining Loss: 0.6917295541445759\tValidation Loss: 0.691724095354453\n",
            "Epoch 15291  \tTraining Loss: 0.6917294946270557\tValidation Loss: 0.6917240353254502\n",
            "Epoch 15292  \tTraining Loss: 0.6917294351088306\tValidation Loss: 0.6917239752957417\n",
            "Epoch 15293  \tTraining Loss: 0.6917293755899007\tValidation Loss: 0.6917239152653273\n",
            "Epoch 15294  \tTraining Loss: 0.6917293160702657\tValidation Loss: 0.6917238552342071\n",
            "Epoch 15295  \tTraining Loss: 0.6917292565499259\tValidation Loss: 0.6917237952023809\n",
            "Epoch 15296  \tTraining Loss: 0.6917291970288809\tValidation Loss: 0.691723735169849\n",
            "Epoch 15297  \tTraining Loss: 0.691729137507131\tValidation Loss: 0.6917236751366109\n",
            "Epoch 15298  \tTraining Loss: 0.6917290779846759\tValidation Loss: 0.691723615102667\n",
            "Epoch 15299  \tTraining Loss: 0.6917290184615158\tValidation Loss: 0.691723555068017\n",
            "Epoch 15300  \tTraining Loss: 0.6917289589376505\tValidation Loss: 0.6917234950326611\n",
            "Epoch 15301  \tTraining Loss: 0.6917288994130799\tValidation Loss: 0.6917234349965989\n",
            "Epoch 15302  \tTraining Loss: 0.6917288398878043\tValidation Loss: 0.6917233749598308\n",
            "Epoch 15303  \tTraining Loss: 0.6917287803618234\tValidation Loss: 0.6917233149223564\n",
            "Epoch 15304  \tTraining Loss: 0.6917287208351371\tValidation Loss: 0.691723254884176\n",
            "Epoch 15305  \tTraining Loss: 0.6917286613077457\tValidation Loss: 0.6917231948452893\n",
            "Epoch 15306  \tTraining Loss: 0.6917286017796489\tValidation Loss: 0.6917231348056965\n",
            "Epoch 15307  \tTraining Loss: 0.6917285422508469\tValidation Loss: 0.6917230747653973\n",
            "Epoch 15308  \tTraining Loss: 0.6917284827213392\tValidation Loss: 0.6917230147243918\n",
            "Epoch 15309  \tTraining Loss: 0.6917284231911263\tValidation Loss: 0.6917229546826801\n",
            "Epoch 15310  \tTraining Loss: 0.6917283636602076\tValidation Loss: 0.6917228946402618\n",
            "Epoch 15311  \tTraining Loss: 0.6917283041285838\tValidation Loss: 0.6917228345971375\n",
            "Epoch 15312  \tTraining Loss: 0.6917282445962543\tValidation Loss: 0.6917227745533064\n",
            "Epoch 15313  \tTraining Loss: 0.6917281850632193\tValidation Loss: 0.691722714508769\n",
            "Epoch 15314  \tTraining Loss: 0.6917281255294786\tValidation Loss: 0.6917226544635252\n",
            "Epoch 15315  \tTraining Loss: 0.6917280659950323\tValidation Loss: 0.6917225944175748\n",
            "Epoch 15316  \tTraining Loss: 0.6917280064598804\tValidation Loss: 0.6917225343709179\n",
            "Epoch 15317  \tTraining Loss: 0.6917279469240228\tValidation Loss: 0.6917224743235544\n",
            "Epoch 15318  \tTraining Loss: 0.6917278873874596\tValidation Loss: 0.6917224142754844\n",
            "Epoch 15319  \tTraining Loss: 0.6917278278501906\tValidation Loss: 0.6917223542267077\n",
            "Epoch 15320  \tTraining Loss: 0.6917277683122157\tValidation Loss: 0.6917222941772243\n",
            "Epoch 15321  \tTraining Loss: 0.6917277087735351\tValidation Loss: 0.6917222341270342\n",
            "Epoch 15322  \tTraining Loss: 0.6917276492341484\tValidation Loss: 0.6917221740761372\n",
            "Epoch 15323  \tTraining Loss: 0.6917275896940562\tValidation Loss: 0.6917221140245338\n",
            "Epoch 15324  \tTraining Loss: 0.6917275301532578\tValidation Loss: 0.6917220539722234\n",
            "Epoch 15325  \tTraining Loss: 0.6917274706117535\tValidation Loss: 0.6917219939192062\n",
            "Epoch 15326  \tTraining Loss: 0.6917274110695434\tValidation Loss: 0.6917219338654822\n",
            "Epoch 15327  \tTraining Loss: 0.6917273515266272\tValidation Loss: 0.6917218738110513\n",
            "Epoch 15328  \tTraining Loss: 0.691727291983005\tValidation Loss: 0.6917218137559135\n",
            "Epoch 15329  \tTraining Loss: 0.6917272324386767\tValidation Loss: 0.6917217537000687\n",
            "Epoch 15330  \tTraining Loss: 0.6917271728936423\tValidation Loss: 0.691721693643517\n",
            "Epoch 15331  \tTraining Loss: 0.6917271133479017\tValidation Loss: 0.6917216335862582\n",
            "Epoch 15332  \tTraining Loss: 0.691727053801455\tValidation Loss: 0.6917215735282924\n",
            "Epoch 15333  \tTraining Loss: 0.6917269942543022\tValidation Loss: 0.6917215134696195\n",
            "Epoch 15334  \tTraining Loss: 0.691726934706443\tValidation Loss: 0.6917214534102395\n",
            "Epoch 15335  \tTraining Loss: 0.6917268751578777\tValidation Loss: 0.6917213933501524\n",
            "Epoch 15336  \tTraining Loss: 0.6917268156086059\tValidation Loss: 0.6917213332893581\n",
            "Epoch 15337  \tTraining Loss: 0.691726756058628\tValidation Loss: 0.6917212732278567\n",
            "Epoch 15338  \tTraining Loss: 0.6917266965079436\tValidation Loss: 0.6917212131656479\n",
            "Epoch 15339  \tTraining Loss: 0.6917266369565528\tValidation Loss: 0.6917211531027321\n",
            "Epoch 15340  \tTraining Loss: 0.6917265774044556\tValidation Loss: 0.6917210930391089\n",
            "Epoch 15341  \tTraining Loss: 0.691726517851652\tValidation Loss: 0.6917210329747782\n",
            "Epoch 15342  \tTraining Loss: 0.6917264582981418\tValidation Loss: 0.6917209729097402\n",
            "Epoch 15343  \tTraining Loss: 0.6917263987439252\tValidation Loss: 0.691720912843995\n",
            "Epoch 15344  \tTraining Loss: 0.6917263391890021\tValidation Loss: 0.6917208527775422\n",
            "Epoch 15345  \tTraining Loss: 0.6917262796333723\tValidation Loss: 0.691720792710382\n",
            "Epoch 15346  \tTraining Loss: 0.6917262200770359\tValidation Loss: 0.6917207326425144\n",
            "Epoch 15347  \tTraining Loss: 0.6917261605199929\tValidation Loss: 0.6917206725739393\n",
            "Epoch 15348  \tTraining Loss: 0.6917261009622432\tValidation Loss: 0.6917206125046567\n",
            "Epoch 15349  \tTraining Loss: 0.6917260414037867\tValidation Loss: 0.6917205524346663\n",
            "Epoch 15350  \tTraining Loss: 0.6917259818446235\tValidation Loss: 0.6917204923639685\n",
            "Epoch 15351  \tTraining Loss: 0.6917259222847536\tValidation Loss: 0.6917204322925631\n",
            "Epoch 15352  \tTraining Loss: 0.6917258627241768\tValidation Loss: 0.6917203722204499\n",
            "Epoch 15353  \tTraining Loss: 0.6917258031628933\tValidation Loss: 0.6917203121476291\n",
            "Epoch 15354  \tTraining Loss: 0.6917257436009028\tValidation Loss: 0.6917202520741005\n",
            "Epoch 15355  \tTraining Loss: 0.6917256840382056\tValidation Loss: 0.691720191999864\n",
            "Epoch 15356  \tTraining Loss: 0.6917256244748013\tValidation Loss: 0.69172013192492\n",
            "Epoch 15357  \tTraining Loss: 0.69172556491069\tValidation Loss: 0.6917200718492681\n",
            "Epoch 15358  \tTraining Loss: 0.6917255053458718\tValidation Loss: 0.6917200117729083\n",
            "Epoch 15359  \tTraining Loss: 0.6917254457803466\tValidation Loss: 0.6917199516958407\n",
            "Epoch 15360  \tTraining Loss: 0.6917253862141142\tValidation Loss: 0.691719891618065\n",
            "Epoch 15361  \tTraining Loss: 0.6917253266471748\tValidation Loss: 0.6917198315395815\n",
            "Epoch 15362  \tTraining Loss: 0.6917252670795282\tValidation Loss: 0.69171977146039\n",
            "Epoch 15363  \tTraining Loss: 0.6917252075111744\tValidation Loss: 0.6917197113804905\n",
            "Epoch 15364  \tTraining Loss: 0.6917251479421136\tValidation Loss: 0.6917196512998829\n",
            "Epoch 15365  \tTraining Loss: 0.6917250883723456\tValidation Loss: 0.6917195912185673\n",
            "Epoch 15366  \tTraining Loss: 0.69172502880187\tValidation Loss: 0.6917195311365435\n",
            "Epoch 15367  \tTraining Loss: 0.6917249692306875\tValidation Loss: 0.6917194710538117\n",
            "Epoch 15368  \tTraining Loss: 0.6917249096587975\tValidation Loss: 0.6917194109703717\n",
            "Epoch 15369  \tTraining Loss: 0.6917248500862002\tValidation Loss: 0.6917193508862235\n",
            "Epoch 15370  \tTraining Loss: 0.6917247905128954\tValidation Loss: 0.691719290801367\n",
            "Epoch 15371  \tTraining Loss: 0.6917247309388833\tValidation Loss: 0.6917192307158022\n",
            "Epoch 15372  \tTraining Loss: 0.6917246713641637\tValidation Loss: 0.6917191706295293\n",
            "Epoch 15373  \tTraining Loss: 0.6917246117887367\tValidation Loss: 0.691719110542548\n",
            "Epoch 15374  \tTraining Loss: 0.6917245522126021\tValidation Loss: 0.6917190504548584\n",
            "Epoch 15375  \tTraining Loss: 0.6917244926357601\tValidation Loss: 0.6917189903664602\n",
            "Epoch 15376  \tTraining Loss: 0.6917244330582104\tValidation Loss: 0.6917189302773538\n",
            "Epoch 15377  \tTraining Loss: 0.6917243734799532\tValidation Loss: 0.6917188701875389\n",
            "Epoch 15378  \tTraining Loss: 0.6917243139009882\tValidation Loss: 0.6917188100970155\n",
            "Epoch 15379  \tTraining Loss: 0.6917242543213157\tValidation Loss: 0.6917187500057835\n",
            "Epoch 15380  \tTraining Loss: 0.6917241947409353\tValidation Loss: 0.6917186899138432\n",
            "Epoch 15381  \tTraining Loss: 0.6917241351598473\tValidation Loss: 0.6917186298211943\n",
            "Epoch 15382  \tTraining Loss: 0.6917240755780516\tValidation Loss: 0.6917185697278365\n",
            "Epoch 15383  \tTraining Loss: 0.691724015995548\tValidation Loss: 0.6917185096337704\n",
            "Epoch 15384  \tTraining Loss: 0.6917239564123365\tValidation Loss: 0.6917184495389954\n",
            "Epoch 15385  \tTraining Loss: 0.6917238968284173\tValidation Loss: 0.6917183894435118\n",
            "Epoch 15386  \tTraining Loss: 0.69172383724379\tValidation Loss: 0.6917183293473195\n",
            "Epoch 15387  \tTraining Loss: 0.6917237776584549\tValidation Loss: 0.6917182692504185\n",
            "Epoch 15388  \tTraining Loss: 0.691723718072412\tValidation Loss: 0.6917182091528086\n",
            "Epoch 15389  \tTraining Loss: 0.6917236584856609\tValidation Loss: 0.6917181490544899\n",
            "Epoch 15390  \tTraining Loss: 0.6917235988982018\tValidation Loss: 0.6917180889554624\n",
            "Epoch 15391  \tTraining Loss: 0.6917235393100347\tValidation Loss: 0.691718028855726\n",
            "Epoch 15392  \tTraining Loss: 0.6917234797211594\tValidation Loss: 0.6917179687552806\n",
            "Epoch 15393  \tTraining Loss: 0.6917234201315762\tValidation Loss: 0.6917179086541263\n",
            "Epoch 15394  \tTraining Loss: 0.6917233605412846\tValidation Loss: 0.691717848552263\n",
            "Epoch 15395  \tTraining Loss: 0.691723300950285\tValidation Loss: 0.6917177884496908\n",
            "Epoch 15396  \tTraining Loss: 0.6917232413585771\tValidation Loss: 0.6917177283464095\n",
            "Epoch 15397  \tTraining Loss: 0.691723181766161\tValidation Loss: 0.6917176682424191\n",
            "Epoch 15398  \tTraining Loss: 0.6917231221730367\tValidation Loss: 0.6917176081377197\n",
            "Epoch 15399  \tTraining Loss: 0.691723062579204\tValidation Loss: 0.691717548032311\n",
            "Epoch 15400  \tTraining Loss: 0.6917230029846628\tValidation Loss: 0.6917174879261934\n",
            "Epoch 15401  \tTraining Loss: 0.6917229433894134\tValidation Loss: 0.6917174278193663\n",
            "Epoch 15402  \tTraining Loss: 0.6917228837934556\tValidation Loss: 0.69171736771183\n",
            "Epoch 15403  \tTraining Loss: 0.6917228241967893\tValidation Loss: 0.6917173076035846\n",
            "Epoch 15404  \tTraining Loss: 0.6917227645994146\tValidation Loss: 0.6917172474946299\n",
            "Epoch 15405  \tTraining Loss: 0.6917227050013313\tValidation Loss: 0.6917171873849658\n",
            "Epoch 15406  \tTraining Loss: 0.6917226454025396\tValidation Loss: 0.6917171272745924\n",
            "Epoch 15407  \tTraining Loss: 0.6917225858030392\tValidation Loss: 0.6917170671635097\n",
            "Epoch 15408  \tTraining Loss: 0.6917225262028303\tValidation Loss: 0.6917170070517175\n",
            "Epoch 15409  \tTraining Loss: 0.6917224666019127\tValidation Loss: 0.6917169469392157\n",
            "Epoch 15410  \tTraining Loss: 0.6917224070002864\tValidation Loss: 0.6917168868260046\n",
            "Epoch 15411  \tTraining Loss: 0.6917223473979515\tValidation Loss: 0.691716826712084\n",
            "Epoch 15412  \tTraining Loss: 0.6917222877949079\tValidation Loss: 0.6917167665974537\n",
            "Epoch 15413  \tTraining Loss: 0.6917222281911554\tValidation Loss: 0.691716706482114\n",
            "Epoch 15414  \tTraining Loss: 0.6917221685866941\tValidation Loss: 0.6917166463660647\n",
            "Epoch 15415  \tTraining Loss: 0.6917221089815242\tValidation Loss: 0.6917165862493057\n",
            "Epoch 15416  \tTraining Loss: 0.6917220493756452\tValidation Loss: 0.6917165261318371\n",
            "Epoch 15417  \tTraining Loss: 0.6917219897690575\tValidation Loss: 0.6917164660136587\n",
            "Epoch 15418  \tTraining Loss: 0.6917219301617608\tValidation Loss: 0.6917164058947706\n",
            "Epoch 15419  \tTraining Loss: 0.6917218705537551\tValidation Loss: 0.6917163457751727\n",
            "Epoch 15420  \tTraining Loss: 0.6917218109450405\tValidation Loss: 0.6917162856548651\n",
            "Epoch 15421  \tTraining Loss: 0.6917217513356168\tValidation Loss: 0.6917162255338477\n",
            "Epoch 15422  \tTraining Loss: 0.6917216917254841\tValidation Loss: 0.6917161654121203\n",
            "Epoch 15423  \tTraining Loss: 0.6917216321146424\tValidation Loss: 0.6917161052896832\n",
            "Epoch 15424  \tTraining Loss: 0.6917215725030915\tValidation Loss: 0.6917160451665362\n",
            "Epoch 15425  \tTraining Loss: 0.6917215128908314\tValidation Loss: 0.6917159850426791\n",
            "Epoch 15426  \tTraining Loss: 0.6917214532778622\tValidation Loss: 0.6917159249181121\n",
            "Epoch 15427  \tTraining Loss: 0.6917213936641838\tValidation Loss: 0.6917158647928351\n",
            "Epoch 15428  \tTraining Loss: 0.6917213340497961\tValidation Loss: 0.691715804666848\n",
            "Epoch 15429  \tTraining Loss: 0.6917212744346992\tValidation Loss: 0.6917157445401507\n",
            "Epoch 15430  \tTraining Loss: 0.6917212148188929\tValidation Loss: 0.6917156844127436\n",
            "Epoch 15431  \tTraining Loss: 0.6917211552023773\tValidation Loss: 0.6917156242846262\n",
            "Epoch 15432  \tTraining Loss: 0.6917210955851525\tValidation Loss: 0.6917155641557987\n",
            "Epoch 15433  \tTraining Loss: 0.6917210359672181\tValidation Loss: 0.6917155040262609\n",
            "Epoch 15434  \tTraining Loss: 0.6917209763485743\tValidation Loss: 0.6917154438960129\n",
            "Epoch 15435  \tTraining Loss: 0.6917209167292211\tValidation Loss: 0.6917153837650547\n",
            "Epoch 15436  \tTraining Loss: 0.6917208571091583\tValidation Loss: 0.6917153236333862\n",
            "Epoch 15437  \tTraining Loss: 0.6917207974883861\tValidation Loss: 0.6917152635010073\n",
            "Epoch 15438  \tTraining Loss: 0.6917207378669042\tValidation Loss: 0.6917152033679183\n",
            "Epoch 15439  \tTraining Loss: 0.6917206782447128\tValidation Loss: 0.6917151432341185\n",
            "Epoch 15440  \tTraining Loss: 0.6917206186218118\tValidation Loss: 0.6917150830996086\n",
            "Epoch 15441  \tTraining Loss: 0.6917205589982011\tValidation Loss: 0.6917150229643881\n",
            "Epoch 15442  \tTraining Loss: 0.6917204993738807\tValidation Loss: 0.6917149628284572\n",
            "Epoch 15443  \tTraining Loss: 0.6917204397488506\tValidation Loss: 0.6917149026918157\n",
            "Epoch 15444  \tTraining Loss: 0.6917203801231108\tValidation Loss: 0.6917148425544637\n",
            "Epoch 15445  \tTraining Loss: 0.691720320496661\tValidation Loss: 0.6917147824164012\n",
            "Epoch 15446  \tTraining Loss: 0.6917202608695016\tValidation Loss: 0.691714722277628\n",
            "Epoch 15447  \tTraining Loss: 0.6917202012416324\tValidation Loss: 0.6917146621381443\n",
            "Epoch 15448  \tTraining Loss: 0.6917201416130531\tValidation Loss: 0.6917146019979499\n",
            "Epoch 15449  \tTraining Loss: 0.691720081983764\tValidation Loss: 0.6917145418570447\n",
            "Epoch 15450  \tTraining Loss: 0.6917200223537648\tValidation Loss: 0.6917144817154288\n",
            "Epoch 15451  \tTraining Loss: 0.6917199627230559\tValidation Loss: 0.6917144215731021\n",
            "Epoch 15452  \tTraining Loss: 0.6917199030916369\tValidation Loss: 0.6917143614300646\n",
            "Epoch 15453  \tTraining Loss: 0.6917198434595078\tValidation Loss: 0.6917143012863165\n",
            "Epoch 15454  \tTraining Loss: 0.6917197838266685\tValidation Loss: 0.6917142411418573\n",
            "Epoch 15455  \tTraining Loss: 0.6917197241931194\tValidation Loss: 0.6917141809966872\n",
            "Epoch 15456  \tTraining Loss: 0.6917196645588599\tValidation Loss: 0.6917141208508064\n",
            "Epoch 15457  \tTraining Loss: 0.6917196049238905\tValidation Loss: 0.6917140607042145\n",
            "Epoch 15458  \tTraining Loss: 0.6917195452882107\tValidation Loss: 0.6917140005569118\n",
            "Epoch 15459  \tTraining Loss: 0.6917194856518206\tValidation Loss: 0.6917139404088979\n",
            "Epoch 15460  \tTraining Loss: 0.6917194260147204\tValidation Loss: 0.6917138802601729\n",
            "Epoch 15461  \tTraining Loss: 0.6917193663769098\tValidation Loss: 0.691713820110737\n",
            "Epoch 15462  \tTraining Loss: 0.6917193067383889\tValidation Loss: 0.6917137599605898\n",
            "Epoch 15463  \tTraining Loss: 0.6917192470991577\tValidation Loss: 0.6917136998097316\n",
            "Epoch 15464  \tTraining Loss: 0.691719187459216\tValidation Loss: 0.6917136396581623\n",
            "Epoch 15465  \tTraining Loss: 0.6917191278185639\tValidation Loss: 0.6917135795058817\n",
            "Epoch 15466  \tTraining Loss: 0.6917190681772014\tValidation Loss: 0.6917135193528899\n",
            "Epoch 15467  \tTraining Loss: 0.6917190085351284\tValidation Loss: 0.6917134591991868\n",
            "Epoch 15468  \tTraining Loss: 0.6917189488923448\tValidation Loss: 0.6917133990447725\n",
            "Epoch 15469  \tTraining Loss: 0.6917188892488507\tValidation Loss: 0.6917133388896467\n",
            "Epoch 15470  \tTraining Loss: 0.6917188296046459\tValidation Loss: 0.6917132787338097\n",
            "Epoch 15471  \tTraining Loss: 0.6917187699597307\tValidation Loss: 0.6917132185772613\n",
            "Epoch 15472  \tTraining Loss: 0.6917187103141046\tValidation Loss: 0.6917131584200014\n",
            "Epoch 15473  \tTraining Loss: 0.691718650667768\tValidation Loss: 0.6917130982620301\n",
            "Epoch 15474  \tTraining Loss: 0.6917185910207206\tValidation Loss: 0.6917130381033474\n",
            "Epoch 15475  \tTraining Loss: 0.6917185313729626\tValidation Loss: 0.691712977943953\n",
            "Epoch 15476  \tTraining Loss: 0.6917184717244937\tValidation Loss: 0.6917129177838471\n",
            "Epoch 15477  \tTraining Loss: 0.6917184120753139\tValidation Loss: 0.6917128576230297\n",
            "Epoch 15478  \tTraining Loss: 0.6917183524254235\tValidation Loss: 0.6917127974615007\n",
            "Epoch 15479  \tTraining Loss: 0.691718292774822\tValidation Loss: 0.6917127372992601\n",
            "Epoch 15480  \tTraining Loss: 0.6917182331235097\tValidation Loss: 0.6917126771363077\n",
            "Epoch 15481  \tTraining Loss: 0.6917181734714865\tValidation Loss: 0.6917126169726437\n",
            "Epoch 15482  \tTraining Loss: 0.6917181138187521\tValidation Loss: 0.6917125568082679\n",
            "Epoch 15483  \tTraining Loss: 0.691718054165307\tValidation Loss: 0.6917124966431804\n",
            "Epoch 15484  \tTraining Loss: 0.6917179945111507\tValidation Loss: 0.6917124364773811\n",
            "Epoch 15485  \tTraining Loss: 0.6917179348562833\tValidation Loss: 0.6917123763108699\n",
            "Epoch 15486  \tTraining Loss: 0.6917178752007048\tValidation Loss: 0.6917123161436469\n",
            "Epoch 15487  \tTraining Loss: 0.6917178155444154\tValidation Loss: 0.691712255975712\n",
            "Epoch 15488  \tTraining Loss: 0.6917177558874146\tValidation Loss: 0.6917121958070651\n",
            "Epoch 15489  \tTraining Loss: 0.6917176962297026\tValidation Loss: 0.6917121356377064\n",
            "Epoch 15490  \tTraining Loss: 0.6917176365712794\tValidation Loss: 0.6917120754676356\n",
            "Epoch 15491  \tTraining Loss: 0.6917175769121451\tValidation Loss: 0.6917120152968528\n",
            "Epoch 15492  \tTraining Loss: 0.6917175172522994\tValidation Loss: 0.691711955125358\n",
            "Epoch 15493  \tTraining Loss: 0.6917174575917422\tValidation Loss: 0.6917118949531511\n",
            "Epoch 15494  \tTraining Loss: 0.6917173979304738\tValidation Loss: 0.6917118347802321\n",
            "Epoch 15495  \tTraining Loss: 0.691717338268494\tValidation Loss: 0.6917117746066009\n",
            "Epoch 15496  \tTraining Loss: 0.6917172786058028\tValidation Loss: 0.6917117144322577\n",
            "Epoch 15497  \tTraining Loss: 0.6917172189424002\tValidation Loss: 0.6917116542572022\n",
            "Epoch 15498  \tTraining Loss: 0.691717159278286\tValidation Loss: 0.6917115940814343\n",
            "Epoch 15499  \tTraining Loss: 0.6917170996134603\tValidation Loss: 0.6917115339049543\n",
            "Epoch 15500  \tTraining Loss: 0.6917170399479231\tValidation Loss: 0.691711473727762\n",
            "Epoch 15501  \tTraining Loss: 0.6917169802816744\tValidation Loss: 0.6917114135498573\n",
            "Epoch 15502  \tTraining Loss: 0.6917169206147139\tValidation Loss: 0.6917113533712403\n",
            "Epoch 15503  \tTraining Loss: 0.6917168609470418\tValidation Loss: 0.691711293191911\n",
            "Epoch 15504  \tTraining Loss: 0.6917168012786581\tValidation Loss: 0.6917112330118691\n",
            "Epoch 15505  \tTraining Loss: 0.6917167416095628\tValidation Loss: 0.6917111728311148\n",
            "Epoch 15506  \tTraining Loss: 0.6917166819397554\tValidation Loss: 0.6917111126496481\n",
            "Epoch 15507  \tTraining Loss: 0.6917166222692366\tValidation Loss: 0.6917110524674688\n",
            "Epoch 15508  \tTraining Loss: 0.6917165625980056\tValidation Loss: 0.691710992284577\n",
            "Epoch 15509  \tTraining Loss: 0.6917165029260631\tValidation Loss: 0.6917109321009727\n",
            "Epoch 15510  \tTraining Loss: 0.6917164432534086\tValidation Loss: 0.6917108719166556\n",
            "Epoch 15511  \tTraining Loss: 0.6917163835800422\tValidation Loss: 0.6917108117316259\n",
            "Epoch 15512  \tTraining Loss: 0.6917163239059639\tValidation Loss: 0.6917107515458836\n",
            "Epoch 15513  \tTraining Loss: 0.6917162642311736\tValidation Loss: 0.6917106913594285\n",
            "Epoch 15514  \tTraining Loss: 0.6917162045556714\tValidation Loss: 0.6917106311722607\n",
            "Epoch 15515  \tTraining Loss: 0.6917161448794571\tValidation Loss: 0.6917105709843803\n",
            "Epoch 15516  \tTraining Loss: 0.6917160852025307\tValidation Loss: 0.6917105107957869\n",
            "Epoch 15517  \tTraining Loss: 0.6917160255248923\tValidation Loss: 0.6917104506064807\n",
            "Epoch 15518  \tTraining Loss: 0.6917159658465417\tValidation Loss: 0.6917103904164618\n",
            "Epoch 15519  \tTraining Loss: 0.6917159061674789\tValidation Loss: 0.6917103302257298\n",
            "Epoch 15520  \tTraining Loss: 0.6917158464877039\tValidation Loss: 0.6917102700342849\n",
            "Epoch 15521  \tTraining Loss: 0.6917157868072168\tValidation Loss: 0.691710209842127\n",
            "Epoch 15522  \tTraining Loss: 0.6917157271260174\tValidation Loss: 0.6917101496492563\n",
            "Epoch 15523  \tTraining Loss: 0.6917156674441056\tValidation Loss: 0.6917100894556725\n",
            "Epoch 15524  \tTraining Loss: 0.6917156077614816\tValidation Loss: 0.6917100292613756\n",
            "Epoch 15525  \tTraining Loss: 0.6917155480781452\tValidation Loss: 0.6917099690663656\n",
            "Epoch 15526  \tTraining Loss: 0.6917154883940965\tValidation Loss: 0.6917099088706427\n",
            "Epoch 15527  \tTraining Loss: 0.6917154287093352\tValidation Loss: 0.6917098486742064\n",
            "Epoch 15528  \tTraining Loss: 0.6917153690238615\tValidation Loss: 0.691709788477057\n",
            "Epoch 15529  \tTraining Loss: 0.6917153093376756\tValidation Loss: 0.6917097282791944\n",
            "Epoch 15530  \tTraining Loss: 0.6917152496507769\tValidation Loss: 0.6917096680806186\n",
            "Epoch 15531  \tTraining Loss: 0.6917151899631657\tValidation Loss: 0.6917096078813295\n",
            "Epoch 15532  \tTraining Loss: 0.6917151302748419\tValidation Loss: 0.6917095476813271\n",
            "Epoch 15533  \tTraining Loss: 0.6917150705858056\tValidation Loss: 0.6917094874806115\n",
            "Epoch 15534  \tTraining Loss: 0.6917150108960566\tValidation Loss: 0.6917094272791822\n",
            "Epoch 15535  \tTraining Loss: 0.6917149512055949\tValidation Loss: 0.6917093670770398\n",
            "Epoch 15536  \tTraining Loss: 0.6917148915144206\tValidation Loss: 0.6917093068741839\n",
            "Epoch 15537  \tTraining Loss: 0.6917148318225335\tValidation Loss: 0.6917092466706144\n",
            "Epoch 15538  \tTraining Loss: 0.6917147721299335\tValidation Loss: 0.6917091864663315\n",
            "Epoch 15539  \tTraining Loss: 0.6917147124366209\tValidation Loss: 0.6917091262613352\n",
            "Epoch 15540  \tTraining Loss: 0.6917146527425952\tValidation Loss: 0.6917090660556253\n",
            "Epoch 15541  \tTraining Loss: 0.691714593047857\tValidation Loss: 0.6917090058492017\n",
            "Epoch 15542  \tTraining Loss: 0.6917145333524056\tValidation Loss: 0.6917089456420646\n",
            "Epoch 15543  \tTraining Loss: 0.6917144736562415\tValidation Loss: 0.6917088854342137\n",
            "Epoch 15544  \tTraining Loss: 0.6917144139593643\tValidation Loss: 0.6917088252256492\n",
            "Epoch 15545  \tTraining Loss: 0.6917143542617741\tValidation Loss: 0.691708765016371\n",
            "Epoch 15546  \tTraining Loss: 0.6917142945634709\tValidation Loss: 0.6917087048063792\n",
            "Epoch 15547  \tTraining Loss: 0.6917142348644547\tValidation Loss: 0.6917086445956734\n",
            "Epoch 15548  \tTraining Loss: 0.6917141751647253\tValidation Loss: 0.691708584384254\n",
            "Epoch 15549  \tTraining Loss: 0.691714115464283\tValidation Loss: 0.6917085241721206\n",
            "Epoch 15550  \tTraining Loss: 0.6917140557631273\tValidation Loss: 0.6917084639592734\n",
            "Epoch 15551  \tTraining Loss: 0.6917139960612584\tValidation Loss: 0.6917084037457123\n",
            "Epoch 15552  \tTraining Loss: 0.6917139363586765\tValidation Loss: 0.6917083435314372\n",
            "Epoch 15553  \tTraining Loss: 0.6917138766553812\tValidation Loss: 0.6917082833164482\n",
            "Epoch 15554  \tTraining Loss: 0.6917138169513726\tValidation Loss: 0.6917082231007453\n",
            "Epoch 15555  \tTraining Loss: 0.6917137572466507\tValidation Loss: 0.6917081628843282\n",
            "Epoch 15556  \tTraining Loss: 0.6917136975412155\tValidation Loss: 0.6917081026671972\n",
            "Epoch 15557  \tTraining Loss: 0.6917136378350669\tValidation Loss: 0.691708042449352\n",
            "Epoch 15558  \tTraining Loss: 0.6917135781282049\tValidation Loss: 0.6917079822307928\n",
            "Epoch 15559  \tTraining Loss: 0.6917135184206294\tValidation Loss: 0.6917079220115192\n",
            "Epoch 15560  \tTraining Loss: 0.6917134587123406\tValidation Loss: 0.6917078617915318\n",
            "Epoch 15561  \tTraining Loss: 0.6917133990033381\tValidation Loss: 0.6917078015708299\n",
            "Epoch 15562  \tTraining Loss: 0.691713339293622\tValidation Loss: 0.6917077413494138\n",
            "Epoch 15563  \tTraining Loss: 0.6917132795831925\tValidation Loss: 0.6917076811272834\n",
            "Epoch 15564  \tTraining Loss: 0.6917132198720494\tValidation Loss: 0.6917076209044387\n",
            "Epoch 15565  \tTraining Loss: 0.6917131601601927\tValidation Loss: 0.6917075606808798\n",
            "Epoch 15566  \tTraining Loss: 0.6917131004476221\tValidation Loss: 0.6917075004566063\n",
            "Epoch 15567  \tTraining Loss: 0.691713040734338\tValidation Loss: 0.6917074402316187\n",
            "Epoch 15568  \tTraining Loss: 0.6917129810203401\tValidation Loss: 0.6917073800059165\n",
            "Epoch 15569  \tTraining Loss: 0.6917129213056284\tValidation Loss: 0.6917073197794997\n",
            "Epoch 15570  \tTraining Loss: 0.6917128615902031\tValidation Loss: 0.6917072595523687\n",
            "Epoch 15571  \tTraining Loss: 0.6917128018740636\tValidation Loss: 0.691707199324523\n",
            "Epoch 15572  \tTraining Loss: 0.6917127421572105\tValidation Loss: 0.6917071390959627\n",
            "Epoch 15573  \tTraining Loss: 0.6917126824396435\tValidation Loss: 0.6917070788666878\n",
            "Epoch 15574  \tTraining Loss: 0.6917126227213625\tValidation Loss: 0.6917070186366985\n",
            "Epoch 15575  \tTraining Loss: 0.6917125630023675\tValidation Loss: 0.6917069584059943\n",
            "Epoch 15576  \tTraining Loss: 0.6917125032826587\tValidation Loss: 0.6917068981745754\n",
            "Epoch 15577  \tTraining Loss: 0.6917124435622357\tValidation Loss: 0.6917068379424419\n",
            "Epoch 15578  \tTraining Loss: 0.6917123838410987\tValidation Loss: 0.6917067777095937\n",
            "Epoch 15579  \tTraining Loss: 0.6917123241192477\tValidation Loss: 0.6917067174760306\n",
            "Epoch 15580  \tTraining Loss: 0.6917122643966824\tValidation Loss: 0.6917066572417527\n",
            "Epoch 15581  \tTraining Loss: 0.6917122046734031\tValidation Loss: 0.6917065970067602\n",
            "Epoch 15582  \tTraining Loss: 0.6917121449494096\tValidation Loss: 0.6917065367710525\n",
            "Epoch 15583  \tTraining Loss: 0.6917120852247017\tValidation Loss: 0.69170647653463\n",
            "Epoch 15584  \tTraining Loss: 0.6917120254992799\tValidation Loss: 0.6917064162974926\n",
            "Epoch 15585  \tTraining Loss: 0.6917119657731435\tValidation Loss: 0.6917063560596404\n",
            "Epoch 15586  \tTraining Loss: 0.691711906046293\tValidation Loss: 0.6917062958210729\n",
            "Epoch 15587  \tTraining Loss: 0.6917118463187281\tValidation Loss: 0.6917062355817905\n",
            "Epoch 15588  \tTraining Loss: 0.6917117865904487\tValidation Loss: 0.6917061753417931\n",
            "Epoch 15589  \tTraining Loss: 0.6917117268614551\tValidation Loss: 0.6917061151010804\n",
            "Epoch 15590  \tTraining Loss: 0.6917116671317468\tValidation Loss: 0.6917060548596528\n",
            "Epoch 15591  \tTraining Loss: 0.6917116074013243\tValidation Loss: 0.6917059946175098\n",
            "Epoch 15592  \tTraining Loss: 0.691711547670187\tValidation Loss: 0.6917059343746519\n",
            "Epoch 15593  \tTraining Loss: 0.6917114879383354\tValidation Loss: 0.6917058741310786\n",
            "Epoch 15594  \tTraining Loss: 0.6917114282057693\tValidation Loss: 0.6917058138867901\n",
            "Epoch 15595  \tTraining Loss: 0.6917113684724885\tValidation Loss: 0.6917057536417862\n",
            "Epoch 15596  \tTraining Loss: 0.691711308738493\tValidation Loss: 0.6917056933960671\n",
            "Epoch 15597  \tTraining Loss: 0.6917112490037829\tValidation Loss: 0.6917056331496327\n",
            "Epoch 15598  \tTraining Loss: 0.691711189268358\tValidation Loss: 0.6917055729024829\n",
            "Epoch 15599  \tTraining Loss: 0.6917111295322184\tValidation Loss: 0.6917055126546177\n",
            "Epoch 15600  \tTraining Loss: 0.691711069795364\tValidation Loss: 0.691705452406037\n",
            "Epoch 15601  \tTraining Loss: 0.691711010057795\tValidation Loss: 0.6917053921567408\n",
            "Epoch 15602  \tTraining Loss: 0.6917109503195111\tValidation Loss: 0.6917053319067291\n",
            "Epoch 15603  \tTraining Loss: 0.6917108905805122\tValidation Loss: 0.691705271656002\n",
            "Epoch 15604  \tTraining Loss: 0.6917108308407987\tValidation Loss: 0.6917052114045592\n",
            "Epoch 15605  \tTraining Loss: 0.69171077110037\tValidation Loss: 0.6917051511524009\n",
            "Epoch 15606  \tTraining Loss: 0.6917107113592265\tValidation Loss: 0.6917050908995269\n",
            "Epoch 15607  \tTraining Loss: 0.6917106516173679\tValidation Loss: 0.6917050306459372\n",
            "Epoch 15608  \tTraining Loss: 0.6917105918747943\tValidation Loss: 0.6917049703916319\n",
            "Epoch 15609  \tTraining Loss: 0.6917105321315057\tValidation Loss: 0.6917049101366108\n",
            "Epoch 15610  \tTraining Loss: 0.691710472387502\tValidation Loss: 0.691704849880874\n",
            "Epoch 15611  \tTraining Loss: 0.6917104126427832\tValidation Loss: 0.6917047896244214\n",
            "Epoch 15612  \tTraining Loss: 0.6917103528973493\tValidation Loss: 0.691704729367253\n",
            "Epoch 15613  \tTraining Loss: 0.6917102931512\tValidation Loss: 0.6917046691093688\n",
            "Epoch 15614  \tTraining Loss: 0.6917102334043357\tValidation Loss: 0.6917046088507686\n",
            "Epoch 15615  \tTraining Loss: 0.6917101736567561\tValidation Loss: 0.6917045485914526\n",
            "Epoch 15616  \tTraining Loss: 0.6917101139084613\tValidation Loss: 0.6917044883314204\n",
            "Epoch 15617  \tTraining Loss: 0.6917100541594511\tValidation Loss: 0.6917044280706726\n",
            "Epoch 15618  \tTraining Loss: 0.6917099944097256\tValidation Loss: 0.6917043678092086\n",
            "Epoch 15619  \tTraining Loss: 0.6917099346592847\tValidation Loss: 0.6917043075470285\n",
            "Epoch 15620  \tTraining Loss: 0.6917098749081284\tValidation Loss: 0.6917042472841324\n",
            "Epoch 15621  \tTraining Loss: 0.6917098151562566\tValidation Loss: 0.6917041870205202\n",
            "Epoch 15622  \tTraining Loss: 0.6917097554036695\tValidation Loss: 0.6917041267561919\n",
            "Epoch 15623  \tTraining Loss: 0.6917096956503668\tValidation Loss: 0.6917040664911474\n",
            "Epoch 15624  \tTraining Loss: 0.6917096358963485\tValidation Loss: 0.6917040062253866\n",
            "Epoch 15625  \tTraining Loss: 0.6917095761416148\tValidation Loss: 0.6917039459589097\n",
            "Epoch 15626  \tTraining Loss: 0.6917095163861653\tValidation Loss: 0.6917038856917165\n",
            "Epoch 15627  \tTraining Loss: 0.6917094566300004\tValidation Loss: 0.6917038254238072\n",
            "Epoch 15628  \tTraining Loss: 0.6917093968731196\tValidation Loss: 0.6917037651551813\n",
            "Epoch 15629  \tTraining Loss: 0.6917093371155232\tValidation Loss: 0.6917037048858392\n",
            "Epoch 15630  \tTraining Loss: 0.6917092773572111\tValidation Loss: 0.6917036446157806\n",
            "Epoch 15631  \tTraining Loss: 0.6917092175981832\tValidation Loss: 0.6917035843450057\n",
            "Epoch 15632  \tTraining Loss: 0.6917091578384397\tValidation Loss: 0.6917035240735143\n",
            "Epoch 15633  \tTraining Loss: 0.6917090980779801\tValidation Loss: 0.6917034638013064\n",
            "Epoch 15634  \tTraining Loss: 0.6917090383168049\tValidation Loss: 0.691703403528382\n",
            "Epoch 15635  \tTraining Loss: 0.6917089785549135\tValidation Loss: 0.691703343254741\n",
            "Epoch 15636  \tTraining Loss: 0.6917089187923064\tValidation Loss: 0.6917032829803835\n",
            "Epoch 15637  \tTraining Loss: 0.6917088590289833\tValidation Loss: 0.6917032227053095\n",
            "Epoch 15638  \tTraining Loss: 0.6917087992649442\tValidation Loss: 0.6917031624295187\n",
            "Epoch 15639  \tTraining Loss: 0.6917087395001892\tValidation Loss: 0.6917031021530112\n",
            "Epoch 15640  \tTraining Loss: 0.6917086797347181\tValidation Loss: 0.6917030418757871\n",
            "Epoch 15641  \tTraining Loss: 0.6917086199685308\tValidation Loss: 0.6917029815978463\n",
            "Epoch 15642  \tTraining Loss: 0.6917085602016274\tValidation Loss: 0.6917029213191886\n",
            "Epoch 15643  \tTraining Loss: 0.691708500434008\tValidation Loss: 0.6917028610398144\n",
            "Epoch 15644  \tTraining Loss: 0.6917084406656724\tValidation Loss: 0.691702800759723\n",
            "Epoch 15645  \tTraining Loss: 0.6917083808966206\tValidation Loss: 0.6917027404789149\n",
            "Epoch 15646  \tTraining Loss: 0.6917083211268524\tValidation Loss: 0.6917026801973899\n",
            "Epoch 15647  \tTraining Loss: 0.691708261356368\tValidation Loss: 0.691702619915148\n",
            "Epoch 15648  \tTraining Loss: 0.6917082015851674\tValidation Loss: 0.6917025596321892\n",
            "Epoch 15649  \tTraining Loss: 0.6917081418132504\tValidation Loss: 0.6917024993485134\n",
            "Epoch 15650  \tTraining Loss: 0.691708082040617\tValidation Loss: 0.6917024390641204\n",
            "Epoch 15651  \tTraining Loss: 0.6917080222672671\tValidation Loss: 0.6917023787790106\n",
            "Epoch 15652  \tTraining Loss: 0.691707962493201\tValidation Loss: 0.6917023184931836\n",
            "Epoch 15653  \tTraining Loss: 0.6917079027184183\tValidation Loss: 0.6917022582066396\n",
            "Epoch 15654  \tTraining Loss: 0.6917078429429191\tValidation Loss: 0.6917021979193783\n",
            "Epoch 15655  \tTraining Loss: 0.6917077831667033\tValidation Loss: 0.6917021376313999\n",
            "Epoch 15656  \tTraining Loss: 0.691707723389771\tValidation Loss: 0.6917020773427043\n",
            "Epoch 15657  \tTraining Loss: 0.6917076636121223\tValidation Loss: 0.6917020170532914\n",
            "Epoch 15658  \tTraining Loss: 0.6917076038337567\tValidation Loss: 0.6917019567631614\n",
            "Epoch 15659  \tTraining Loss: 0.6917075440546746\tValidation Loss: 0.691701896472314\n",
            "Epoch 15660  \tTraining Loss: 0.6917074842748757\tValidation Loss: 0.6917018361807491\n",
            "Epoch 15661  \tTraining Loss: 0.6917074244943602\tValidation Loss: 0.691701775888467\n",
            "Epoch 15662  \tTraining Loss: 0.6917073647131279\tValidation Loss: 0.6917017155954676\n",
            "Epoch 15663  \tTraining Loss: 0.6917073049311788\tValidation Loss: 0.6917016553017507\n",
            "Epoch 15664  \tTraining Loss: 0.6917072451485129\tValidation Loss: 0.6917015950073162\n",
            "Epoch 15665  \tTraining Loss: 0.69170718536513\tValidation Loss: 0.6917015347121643\n",
            "Epoch 15666  \tTraining Loss: 0.6917071255810303\tValidation Loss: 0.691701474416295\n",
            "Epoch 15667  \tTraining Loss: 0.6917070657962138\tValidation Loss: 0.691701414119708\n",
            "Epoch 15668  \tTraining Loss: 0.6917070060106802\tValidation Loss: 0.6917013538224035\n",
            "Epoch 15669  \tTraining Loss: 0.6917069462244297\tValidation Loss: 0.6917012935243814\n",
            "Epoch 15670  \tTraining Loss: 0.6917068864374621\tValidation Loss: 0.6917012332256415\n",
            "Epoch 15671  \tTraining Loss: 0.6917068266497777\tValidation Loss: 0.691701172926184\n",
            "Epoch 15672  \tTraining Loss: 0.691706766861376\tValidation Loss: 0.6917011126260089\n",
            "Epoch 15673  \tTraining Loss: 0.6917067070722572\tValidation Loss: 0.6917010523251159\n",
            "Epoch 15674  \tTraining Loss: 0.6917066472824214\tValidation Loss: 0.6917009920235052\n",
            "Epoch 15675  \tTraining Loss: 0.6917065874918682\tValidation Loss: 0.6917009317211767\n",
            "Epoch 15676  \tTraining Loss: 0.691706527700598\tValidation Loss: 0.6917008714181304\n",
            "Epoch 15677  \tTraining Loss: 0.6917064679086105\tValidation Loss: 0.6917008111143662\n",
            "Epoch 15678  \tTraining Loss: 0.6917064081159057\tValidation Loss: 0.6917007508098841\n",
            "Epoch 15679  \tTraining Loss: 0.6917063483224837\tValidation Loss: 0.6917006905046842\n",
            "Epoch 15680  \tTraining Loss: 0.6917062885283441\tValidation Loss: 0.691700630198766\n",
            "Epoch 15681  \tTraining Loss: 0.6917062287334874\tValidation Loss: 0.6917005698921301\n",
            "Epoch 15682  \tTraining Loss: 0.6917061689379131\tValidation Loss: 0.6917005095847761\n",
            "Epoch 15683  \tTraining Loss: 0.6917061091416216\tValidation Loss: 0.691700449276704\n",
            "Epoch 15684  \tTraining Loss: 0.6917060493446126\tValidation Loss: 0.6917003889679137\n",
            "Epoch 15685  \tTraining Loss: 0.6917059895468859\tValidation Loss: 0.6917003286584056\n",
            "Epoch 15686  \tTraining Loss: 0.6917059297484418\tValidation Loss: 0.6917002683481791\n",
            "Epoch 15687  \tTraining Loss: 0.6917058699492802\tValidation Loss: 0.6917002080372345\n",
            "Epoch 15688  \tTraining Loss: 0.6917058101494009\tValidation Loss: 0.6917001477255718\n",
            "Epoch 15689  \tTraining Loss: 0.691705750348804\tValidation Loss: 0.6917000874131907\n",
            "Epoch 15690  \tTraining Loss: 0.6917056905474894\tValidation Loss: 0.6917000271000913\n",
            "Epoch 15691  \tTraining Loss: 0.6917056307454572\tValidation Loss: 0.6916999667862735\n",
            "Epoch 15692  \tTraining Loss: 0.6917055709427072\tValidation Loss: 0.6916999064717376\n",
            "Epoch 15693  \tTraining Loss: 0.6917055111392395\tValidation Loss: 0.6916998461564833\n",
            "Epoch 15694  \tTraining Loss: 0.691705451335054\tValidation Loss: 0.6916997858405104\n",
            "Epoch 15695  \tTraining Loss: 0.6917053915301508\tValidation Loss: 0.6916997255238193\n",
            "Epoch 15696  \tTraining Loss: 0.6917053317245296\tValidation Loss: 0.6916996652064096\n",
            "Epoch 15697  \tTraining Loss: 0.6917052719181906\tValidation Loss: 0.6916996048882814\n",
            "Epoch 15698  \tTraining Loss: 0.6917052121111336\tValidation Loss: 0.6916995445694346\n",
            "Epoch 15699  \tTraining Loss: 0.6917051523033586\tValidation Loss: 0.6916994842498695\n",
            "Epoch 15700  \tTraining Loss: 0.6917050924948659\tValidation Loss: 0.6916994239295855\n",
            "Epoch 15701  \tTraining Loss: 0.6917050326856549\tValidation Loss: 0.6916993636085831\n",
            "Epoch 15702  \tTraining Loss: 0.6917049728757261\tValidation Loss: 0.6916993032868619\n",
            "Epoch 15703  \tTraining Loss: 0.691704913065079\tValidation Loss: 0.6916992429644222\n",
            "Epoch 15704  \tTraining Loss: 0.6917048532537138\tValidation Loss: 0.6916991826412636\n",
            "Epoch 15705  \tTraining Loss: 0.6917047934416306\tValidation Loss: 0.6916991223173862\n",
            "Epoch 15706  \tTraining Loss: 0.6917047336288291\tValidation Loss: 0.6916990619927902\n",
            "Epoch 15707  \tTraining Loss: 0.6917046738153095\tValidation Loss: 0.6916990016674752\n",
            "Epoch 15708  \tTraining Loss: 0.6917046140010717\tValidation Loss: 0.6916989413414416\n",
            "Epoch 15709  \tTraining Loss: 0.6917045541861155\tValidation Loss: 0.6916988810146889\n",
            "Epoch 15710  \tTraining Loss: 0.6917044943704411\tValidation Loss: 0.6916988206872173\n",
            "Epoch 15711  \tTraining Loss: 0.6917044345540484\tValidation Loss: 0.6916987603590269\n",
            "Epoch 15712  \tTraining Loss: 0.6917043747369372\tValidation Loss: 0.6916987000301175\n",
            "Epoch 15713  \tTraining Loss: 0.6917043149191076\tValidation Loss: 0.6916986397004891\n",
            "Epoch 15714  \tTraining Loss: 0.6917042551005598\tValidation Loss: 0.6916985793701416\n",
            "Epoch 15715  \tTraining Loss: 0.6917041952812933\tValidation Loss: 0.691698519039075\n",
            "Epoch 15716  \tTraining Loss: 0.6917041354613084\tValidation Loss: 0.6916984587072893\n",
            "Epoch 15717  \tTraining Loss: 0.6917040756406049\tValidation Loss: 0.6916983983747845\n",
            "Epoch 15718  \tTraining Loss: 0.691704015819183\tValidation Loss: 0.6916983380415606\n",
            "Epoch 15719  \tTraining Loss: 0.6917039559970424\tValidation Loss: 0.6916982777076175\n",
            "Epoch 15720  \tTraining Loss: 0.6917038961741832\tValidation Loss: 0.6916982173729551\n",
            "Epoch 15721  \tTraining Loss: 0.6917038363506053\tValidation Loss: 0.6916981570375734\n",
            "Epoch 15722  \tTraining Loss: 0.6917037765263087\tValidation Loss: 0.6916980967014726\n",
            "Epoch 15723  \tTraining Loss: 0.6917037167012934\tValidation Loss: 0.6916980363646524\n",
            "Epoch 15724  \tTraining Loss: 0.6917036568755593\tValidation Loss: 0.6916979760271128\n",
            "Epoch 15725  \tTraining Loss: 0.6917035970491066\tValidation Loss: 0.6916979156888537\n",
            "Epoch 15726  \tTraining Loss: 0.691703537221935\tValidation Loss: 0.6916978553498755\n",
            "Epoch 15727  \tTraining Loss: 0.6917034773940445\tValidation Loss: 0.6916977950101776\n",
            "Epoch 15728  \tTraining Loss: 0.6917034175654351\tValidation Loss: 0.6916977346697603\n",
            "Epoch 15729  \tTraining Loss: 0.6917033577361069\tValidation Loss: 0.6916976743286235\n",
            "Epoch 15730  \tTraining Loss: 0.6917032979060597\tValidation Loss: 0.6916976139867671\n",
            "Epoch 15731  \tTraining Loss: 0.6917032380752935\tValidation Loss: 0.6916975536441912\n",
            "Epoch 15732  \tTraining Loss: 0.6917031782438082\tValidation Loss: 0.6916974933008957\n",
            "Epoch 15733  \tTraining Loss: 0.691703118411604\tValidation Loss: 0.6916974329568805\n",
            "Epoch 15734  \tTraining Loss: 0.6917030585786806\tValidation Loss: 0.6916973726121457\n",
            "Epoch 15735  \tTraining Loss: 0.6917029987450383\tValidation Loss: 0.6916973122666913\n",
            "Epoch 15736  \tTraining Loss: 0.6917029389106767\tValidation Loss: 0.6916972519205169\n",
            "Epoch 15737  \tTraining Loss: 0.691702879075596\tValidation Loss: 0.6916971915736229\n",
            "Epoch 15738  \tTraining Loss: 0.691702819239796\tValidation Loss: 0.6916971312260091\n",
            "Epoch 15739  \tTraining Loss: 0.6917027594032769\tValidation Loss: 0.6916970708776754\n",
            "Epoch 15740  \tTraining Loss: 0.6917026995660385\tValidation Loss: 0.6916970105286219\n",
            "Epoch 15741  \tTraining Loss: 0.6917026397280807\tValidation Loss: 0.6916969501788485\n",
            "Epoch 15742  \tTraining Loss: 0.6917025798894036\tValidation Loss: 0.6916968898283552\n",
            "Epoch 15743  \tTraining Loss: 0.6917025200500072\tValidation Loss: 0.691696829477142\n",
            "Epoch 15744  \tTraining Loss: 0.6917024602098913\tValidation Loss: 0.6916967691252087\n",
            "Epoch 15745  \tTraining Loss: 0.6917024003690562\tValidation Loss: 0.6916967087725554\n",
            "Epoch 15746  \tTraining Loss: 0.6917023405275015\tValidation Loss: 0.6916966484191821\n",
            "Epoch 15747  \tTraining Loss: 0.6917022806852273\tValidation Loss: 0.6916965880650886\n",
            "Epoch 15748  \tTraining Loss: 0.6917022208422334\tValidation Loss: 0.6916965277102752\n",
            "Epoch 15749  \tTraining Loss: 0.6917021609985202\tValidation Loss: 0.6916964673547414\n",
            "Epoch 15750  \tTraining Loss: 0.6917021011540873\tValidation Loss: 0.6916964069984877\n",
            "Epoch 15751  \tTraining Loss: 0.6917020413089349\tValidation Loss: 0.6916963466415137\n",
            "Epoch 15752  \tTraining Loss: 0.6917019814630627\tValidation Loss: 0.6916962862838195\n",
            "Epoch 15753  \tTraining Loss: 0.6917019216164708\tValidation Loss: 0.691696225925405\n",
            "Epoch 15754  \tTraining Loss: 0.6917018617691594\tValidation Loss: 0.6916961655662701\n",
            "Epoch 15755  \tTraining Loss: 0.691701801921128\tValidation Loss: 0.6916961052064149\n",
            "Epoch 15756  \tTraining Loss: 0.6917017420723769\tValidation Loss: 0.6916960448458395\n",
            "Epoch 15757  \tTraining Loss: 0.6917016822229061\tValidation Loss: 0.6916959844845436\n",
            "Epoch 15758  \tTraining Loss: 0.6917016223727153\tValidation Loss: 0.6916959241225272\n",
            "Epoch 15759  \tTraining Loss: 0.6917015625218047\tValidation Loss: 0.6916958637597903\n",
            "Epoch 15760  \tTraining Loss: 0.691701502670174\tValidation Loss: 0.6916958033963332\n",
            "Epoch 15761  \tTraining Loss: 0.6917014428178235\tValidation Loss: 0.6916957430321553\n",
            "Epoch 15762  \tTraining Loss: 0.6917013829647531\tValidation Loss: 0.6916956826672569\n",
            "Epoch 15763  \tTraining Loss: 0.6917013231109626\tValidation Loss: 0.6916956223016381\n",
            "Epoch 15764  \tTraining Loss: 0.6917012632564521\tValidation Loss: 0.6916955619352984\n",
            "Epoch 15765  \tTraining Loss: 0.6917012034012217\tValidation Loss: 0.6916955015682382\n",
            "Epoch 15766  \tTraining Loss: 0.6917011435452709\tValidation Loss: 0.6916954412004573\n",
            "Epoch 15767  \tTraining Loss: 0.6917010836886001\tValidation Loss: 0.6916953808319557\n",
            "Epoch 15768  \tTraining Loss: 0.691701023831209\tValidation Loss: 0.6916953204627333\n",
            "Epoch 15769  \tTraining Loss: 0.6917009639730978\tValidation Loss: 0.6916952600927903\n",
            "Epoch 15770  \tTraining Loss: 0.6917009041142664\tValidation Loss: 0.6916951997221262\n",
            "Epoch 15771  \tTraining Loss: 0.6917008442547147\tValidation Loss: 0.6916951393507416\n",
            "Epoch 15772  \tTraining Loss: 0.6917007843944427\tValidation Loss: 0.6916950789786359\n",
            "Epoch 15773  \tTraining Loss: 0.6917007245334504\tValidation Loss: 0.6916950186058094\n",
            "Epoch 15774  \tTraining Loss: 0.6917006646717377\tValidation Loss: 0.6916949582322619\n",
            "Epoch 15775  \tTraining Loss: 0.6917006048093046\tValidation Loss: 0.6916948978579935\n",
            "Epoch 15776  \tTraining Loss: 0.6917005449461512\tValidation Loss: 0.691694837483004\n",
            "Epoch 15777  \tTraining Loss: 0.6917004850822771\tValidation Loss: 0.6916947771072937\n",
            "Epoch 15778  \tTraining Loss: 0.6917004252176827\tValidation Loss: 0.6916947167308621\n",
            "Epoch 15779  \tTraining Loss: 0.6917003653523677\tValidation Loss: 0.6916946563537094\n",
            "Epoch 15780  \tTraining Loss: 0.6917003054863322\tValidation Loss: 0.6916945959758357\n",
            "Epoch 15781  \tTraining Loss: 0.6917002456195761\tValidation Loss: 0.6916945355972409\n",
            "Epoch 15782  \tTraining Loss: 0.6917001857520992\tValidation Loss: 0.6916944752179247\n",
            "Epoch 15783  \tTraining Loss: 0.691700125883902\tValidation Loss: 0.6916944148378875\n",
            "Epoch 15784  \tTraining Loss: 0.6917000660149837\tValidation Loss: 0.6916943544571289\n",
            "Epoch 15785  \tTraining Loss: 0.691700006145345\tValidation Loss: 0.6916942940756491\n",
            "Epoch 15786  \tTraining Loss: 0.6916999462749854\tValidation Loss: 0.691694233693448\n",
            "Epoch 15787  \tTraining Loss: 0.6916998864039051\tValidation Loss: 0.6916941733105255\n",
            "Epoch 15788  \tTraining Loss: 0.691699826532104\tValidation Loss: 0.6916941129268817\n",
            "Epoch 15789  \tTraining Loss: 0.691699766659582\tValidation Loss: 0.6916940525425164\n",
            "Epoch 15790  \tTraining Loss: 0.6916997067863391\tValidation Loss: 0.6916939921574298\n",
            "Epoch 15791  \tTraining Loss: 0.6916996469123753\tValidation Loss: 0.6916939317716215\n",
            "Epoch 15792  \tTraining Loss: 0.6916995870376905\tValidation Loss: 0.6916938713850918\n",
            "Epoch 15793  \tTraining Loss: 0.6916995271622848\tValidation Loss: 0.6916938109978407\n",
            "Epoch 15794  \tTraining Loss: 0.6916994672861582\tValidation Loss: 0.6916937506098679\n",
            "Epoch 15795  \tTraining Loss: 0.6916994074093102\tValidation Loss: 0.6916936902211734\n",
            "Epoch 15796  \tTraining Loss: 0.6916993475317416\tValidation Loss: 0.6916936298317574\n",
            "Epoch 15797  \tTraining Loss: 0.6916992876534516\tValidation Loss: 0.6916935694416197\n",
            "Epoch 15798  \tTraining Loss: 0.6916992277744405\tValidation Loss: 0.6916935090507604\n",
            "Epoch 15799  \tTraining Loss: 0.6916991678947083\tValidation Loss: 0.6916934486591794\n",
            "Epoch 15800  \tTraining Loss: 0.6916991080142548\tValidation Loss: 0.6916933882668765\n",
            "Epoch 15801  \tTraining Loss: 0.69169904813308\tValidation Loss: 0.6916933278738518\n",
            "Epoch 15802  \tTraining Loss: 0.6916989882511841\tValidation Loss: 0.6916932674801055\n",
            "Epoch 15803  \tTraining Loss: 0.6916989283685671\tValidation Loss: 0.6916932070856372\n",
            "Epoch 15804  \tTraining Loss: 0.6916988684852283\tValidation Loss: 0.691693146690447\n",
            "Epoch 15805  \tTraining Loss: 0.6916988086011685\tValidation Loss: 0.691693086294535\n",
            "Epoch 15806  \tTraining Loss: 0.6916987487163871\tValidation Loss: 0.6916930258979009\n",
            "Epoch 15807  \tTraining Loss: 0.6916986888308845\tValidation Loss: 0.6916929655005448\n",
            "Epoch 15808  \tTraining Loss: 0.6916986289446602\tValidation Loss: 0.6916929051024668\n",
            "Epoch 15809  \tTraining Loss: 0.6916985690577145\tValidation Loss: 0.6916928447036668\n",
            "Epoch 15810  \tTraining Loss: 0.6916985091700473\tValidation Loss: 0.6916927843041447\n",
            "Epoch 15811  \tTraining Loss: 0.6916984492816585\tValidation Loss: 0.6916927239039004\n",
            "Epoch 15812  \tTraining Loss: 0.6916983893925481\tValidation Loss: 0.691692663502934\n",
            "Epoch 15813  \tTraining Loss: 0.691698329502716\tValidation Loss: 0.6916926031012456\n",
            "Epoch 15814  \tTraining Loss: 0.6916982696121624\tValidation Loss: 0.6916925426988347\n",
            "Epoch 15815  \tTraining Loss: 0.6916982097208871\tValidation Loss: 0.6916924822957017\n",
            "Epoch 15816  \tTraining Loss: 0.6916981498288901\tValidation Loss: 0.6916924218918465\n",
            "Epoch 15817  \tTraining Loss: 0.6916980899361713\tValidation Loss: 0.6916923614872691\n",
            "Epoch 15818  \tTraining Loss: 0.6916980300427307\tValidation Loss: 0.6916923010819692\n",
            "Epoch 15819  \tTraining Loss: 0.6916979701485683\tValidation Loss: 0.6916922406759469\n",
            "Epoch 15820  \tTraining Loss: 0.6916979102536841\tValidation Loss: 0.6916921802692023\n",
            "Epoch 15821  \tTraining Loss: 0.691697850358078\tValidation Loss: 0.6916921198617353\n",
            "Epoch 15822  \tTraining Loss: 0.69169779046175\tValidation Loss: 0.6916920594535457\n",
            "Epoch 15823  \tTraining Loss: 0.6916977305647\tValidation Loss: 0.6916919990446339\n",
            "Epoch 15824  \tTraining Loss: 0.6916976706669281\tValidation Loss: 0.6916919386349993\n",
            "Epoch 15825  \tTraining Loss: 0.6916976107684341\tValidation Loss: 0.6916918782246423\n",
            "Epoch 15826  \tTraining Loss: 0.6916975508692181\tValidation Loss: 0.6916918178135627\n",
            "Epoch 15827  \tTraining Loss: 0.69169749096928\tValidation Loss: 0.6916917574017605\n",
            "Epoch 15828  \tTraining Loss: 0.6916974310686198\tValidation Loss: 0.6916916969892356\n",
            "Epoch 15829  \tTraining Loss: 0.6916973711672375\tValidation Loss: 0.6916916365759881\n",
            "Epoch 15830  \tTraining Loss: 0.691697311265133\tValidation Loss: 0.6916915761620178\n",
            "Epoch 15831  \tTraining Loss: 0.6916972513623064\tValidation Loss: 0.6916915157473248\n",
            "Epoch 15832  \tTraining Loss: 0.6916971914587574\tValidation Loss: 0.6916914553319091\n",
            "Epoch 15833  \tTraining Loss: 0.6916971315544863\tValidation Loss: 0.6916913949157705\n",
            "Epoch 15834  \tTraining Loss: 0.6916970716494928\tValidation Loss: 0.6916913344989091\n",
            "Epoch 15835  \tTraining Loss: 0.691697011743777\tValidation Loss: 0.6916912740813249\n",
            "Epoch 15836  \tTraining Loss: 0.6916969518373387\tValidation Loss: 0.6916912136630177\n",
            "Epoch 15837  \tTraining Loss: 0.6916968919301782\tValidation Loss: 0.6916911532439877\n",
            "Epoch 15838  \tTraining Loss: 0.6916968320222953\tValidation Loss: 0.6916910928242346\n",
            "Epoch 15839  \tTraining Loss: 0.6916967721136897\tValidation Loss: 0.6916910324037585\n",
            "Epoch 15840  \tTraining Loss: 0.6916967122043618\tValidation Loss: 0.6916909719825595\n",
            "Epoch 15841  \tTraining Loss: 0.6916966522943114\tValidation Loss: 0.6916909115606373\n",
            "Epoch 15842  \tTraining Loss: 0.6916965923835383\tValidation Loss: 0.6916908511379922\n",
            "Epoch 15843  \tTraining Loss: 0.6916965324720428\tValidation Loss: 0.6916907907146239\n",
            "Epoch 15844  \tTraining Loss: 0.6916964725598245\tValidation Loss: 0.6916907302905324\n",
            "Epoch 15845  \tTraining Loss: 0.6916964126468836\tValidation Loss: 0.6916906698657177\n",
            "Epoch 15846  \tTraining Loss: 0.69169635273322\tValidation Loss: 0.6916906094401799\n",
            "Epoch 15847  \tTraining Loss: 0.6916962928188338\tValidation Loss: 0.6916905490139187\n",
            "Epoch 15848  \tTraining Loss: 0.6916962329037247\tValidation Loss: 0.6916904885869343\n",
            "Epoch 15849  \tTraining Loss: 0.6916961729878929\tValidation Loss: 0.6916904281592267\n",
            "Epoch 15850  \tTraining Loss: 0.6916961130713383\tValidation Loss: 0.6916903677307956\n",
            "Epoch 15851  \tTraining Loss: 0.6916960531540607\tValidation Loss: 0.6916903073016412\n",
            "Epoch 15852  \tTraining Loss: 0.6916959932360605\tValidation Loss: 0.6916902468717634\n",
            "Epoch 15853  \tTraining Loss: 0.6916959333173371\tValidation Loss: 0.6916901864411622\n",
            "Epoch 15854  \tTraining Loss: 0.6916958733978908\tValidation Loss: 0.6916901260098375\n",
            "Epoch 15855  \tTraining Loss: 0.6916958134777217\tValidation Loss: 0.6916900655777892\n",
            "Epoch 15856  \tTraining Loss: 0.6916957535568296\tValidation Loss: 0.6916900051450176\n",
            "Epoch 15857  \tTraining Loss: 0.6916956936352142\tValidation Loss: 0.6916899447115222\n",
            "Epoch 15858  \tTraining Loss: 0.691695633712876\tValidation Loss: 0.6916898842773034\n",
            "Epoch 15859  \tTraining Loss: 0.6916955737898145\tValidation Loss: 0.6916898238423607\n",
            "Epoch 15860  \tTraining Loss: 0.6916955138660299\tValidation Loss: 0.6916897634066945\n",
            "Epoch 15861  \tTraining Loss: 0.6916954539415222\tValidation Loss: 0.6916897029703046\n",
            "Epoch 15862  \tTraining Loss: 0.6916953940162913\tValidation Loss: 0.6916896425331911\n",
            "Epoch 15863  \tTraining Loss: 0.6916953340903371\tValidation Loss: 0.6916895820953537\n",
            "Epoch 15864  \tTraining Loss: 0.6916952741636598\tValidation Loss: 0.6916895216567925\n",
            "Epoch 15865  \tTraining Loss: 0.691695214236259\tValidation Loss: 0.6916894612175076\n",
            "Epoch 15866  \tTraining Loss: 0.6916951543081349\tValidation Loss: 0.6916894007774987\n",
            "Epoch 15867  \tTraining Loss: 0.6916950943792876\tValidation Loss: 0.6916893403367661\n",
            "Epoch 15868  \tTraining Loss: 0.6916950344497167\tValidation Loss: 0.6916892798953095\n",
            "Epoch 15869  \tTraining Loss: 0.6916949745194225\tValidation Loss: 0.691689219453129\n",
            "Epoch 15870  \tTraining Loss: 0.6916949145884047\tValidation Loss: 0.6916891590102244\n",
            "Epoch 15871  \tTraining Loss: 0.6916948546566635\tValidation Loss: 0.691689098566596\n",
            "Epoch 15872  \tTraining Loss: 0.6916947947241987\tValidation Loss: 0.6916890381222434\n",
            "Epoch 15873  \tTraining Loss: 0.6916947347910105\tValidation Loss: 0.6916889776771667\n",
            "Epoch 15874  \tTraining Loss: 0.6916946748570986\tValidation Loss: 0.6916889172313659\n",
            "Epoch 15875  \tTraining Loss: 0.6916946149224632\tValidation Loss: 0.6916888567848412\n",
            "Epoch 15876  \tTraining Loss: 0.6916945549871041\tValidation Loss: 0.6916887963375921\n",
            "Epoch 15877  \tTraining Loss: 0.6916944950510211\tValidation Loss: 0.6916887358896189\n",
            "Epoch 15878  \tTraining Loss: 0.6916944351142146\tValidation Loss: 0.6916886754409214\n",
            "Epoch 15879  \tTraining Loss: 0.6916943751766843\tValidation Loss: 0.6916886149914996\n",
            "Epoch 15880  \tTraining Loss: 0.6916943152384302\tValidation Loss: 0.6916885545413536\n",
            "Epoch 15881  \tTraining Loss: 0.6916942552994524\tValidation Loss: 0.6916884940904833\n",
            "Epoch 15882  \tTraining Loss: 0.6916941953597505\tValidation Loss: 0.6916884336388887\n",
            "Epoch 15883  \tTraining Loss: 0.6916941354193249\tValidation Loss: 0.6916883731865695\n",
            "Epoch 15884  \tTraining Loss: 0.6916940754781754\tValidation Loss: 0.6916883127335259\n",
            "Epoch 15885  \tTraining Loss: 0.6916940155363019\tValidation Loss: 0.691688252279758\n",
            "Epoch 15886  \tTraining Loss: 0.6916939555937045\tValidation Loss: 0.6916881918252655\n",
            "Epoch 15887  \tTraining Loss: 0.691693895650383\tValidation Loss: 0.6916881313700486\n",
            "Epoch 15888  \tTraining Loss: 0.6916938357063375\tValidation Loss: 0.6916880709141071\n",
            "Epoch 15889  \tTraining Loss: 0.6916937757615679\tValidation Loss: 0.6916880104574411\n",
            "Epoch 15890  \tTraining Loss: 0.6916937158160742\tValidation Loss: 0.6916879500000503\n",
            "Epoch 15891  \tTraining Loss: 0.6916936558698562\tValidation Loss: 0.691687889541935\n",
            "Epoch 15892  \tTraining Loss: 0.6916935959229144\tValidation Loss: 0.691687829083095\n",
            "Epoch 15893  \tTraining Loss: 0.6916935359752482\tValidation Loss: 0.6916877686235303\n",
            "Epoch 15894  \tTraining Loss: 0.6916934760268576\tValidation Loss: 0.6916877081632409\n",
            "Epoch 15895  \tTraining Loss: 0.6916934160777429\tValidation Loss: 0.6916876477022268\n",
            "Epoch 15896  \tTraining Loss: 0.6916933561279038\tValidation Loss: 0.6916875872404876\n",
            "Epoch 15897  \tTraining Loss: 0.6916932961773407\tValidation Loss: 0.6916875267780237\n",
            "Epoch 15898  \tTraining Loss: 0.6916932362260529\tValidation Loss: 0.6916874663148351\n",
            "Epoch 15899  \tTraining Loss: 0.6916931762740408\tValidation Loss: 0.6916874058509215\n",
            "Epoch 15900  \tTraining Loss: 0.6916931163213041\tValidation Loss: 0.691687345386283\n",
            "Epoch 15901  \tTraining Loss: 0.6916930563678432\tValidation Loss: 0.6916872849209194\n",
            "Epoch 15902  \tTraining Loss: 0.6916929964136577\tValidation Loss: 0.6916872244548309\n",
            "Epoch 15903  \tTraining Loss: 0.6916929364587477\tValidation Loss: 0.6916871639880174\n",
            "Epoch 15904  \tTraining Loss: 0.6916928765031132\tValidation Loss: 0.6916871035204788\n",
            "Epoch 15905  \tTraining Loss: 0.691692816546754\tValidation Loss: 0.6916870430522152\n",
            "Epoch 15906  \tTraining Loss: 0.6916927565896702\tValidation Loss: 0.6916869825832264\n",
            "Epoch 15907  \tTraining Loss: 0.6916926966318617\tValidation Loss: 0.6916869221135125\n",
            "Epoch 15908  \tTraining Loss: 0.6916926366733286\tValidation Loss: 0.6916868616430734\n",
            "Epoch 15909  \tTraining Loss: 0.6916925767140707\tValidation Loss: 0.6916868011719091\n",
            "Epoch 15910  \tTraining Loss: 0.6916925167540882\tValidation Loss: 0.6916867407000195\n",
            "Epoch 15911  \tTraining Loss: 0.6916924567933808\tValidation Loss: 0.6916866802274046\n",
            "Epoch 15912  \tTraining Loss: 0.6916923968319485\tValidation Loss: 0.6916866197540645\n",
            "Epoch 15913  \tTraining Loss: 0.6916923368697915\tValidation Loss: 0.691686559279999\n",
            "Epoch 15914  \tTraining Loss: 0.6916922769069095\tValidation Loss: 0.691686498805208\n",
            "Epoch 15915  \tTraining Loss: 0.6916922169433026\tValidation Loss: 0.6916864383296918\n",
            "Epoch 15916  \tTraining Loss: 0.6916921569789709\tValidation Loss: 0.6916863778534502\n",
            "Epoch 15917  \tTraining Loss: 0.6916920970139141\tValidation Loss: 0.691686317376483\n",
            "Epoch 15918  \tTraining Loss: 0.6916920370481323\tValidation Loss: 0.6916862568987903\n",
            "Epoch 15919  \tTraining Loss: 0.6916919770816256\tValidation Loss: 0.6916861964203721\n",
            "Epoch 15920  \tTraining Loss: 0.6916919171143936\tValidation Loss: 0.6916861359412283\n",
            "Epoch 15921  \tTraining Loss: 0.6916918571464367\tValidation Loss: 0.691686075461359\n",
            "Epoch 15922  \tTraining Loss: 0.6916917971777544\tValidation Loss: 0.691686014980764\n",
            "Epoch 15923  \tTraining Loss: 0.6916917372083471\tValidation Loss: 0.6916859544994433\n",
            "Epoch 15924  \tTraining Loss: 0.6916916772382147\tValidation Loss: 0.691685894017397\n",
            "Epoch 15925  \tTraining Loss: 0.6916916172673567\tValidation Loss: 0.6916858335346249\n",
            "Epoch 15926  \tTraining Loss: 0.6916915572957738\tValidation Loss: 0.6916857730511271\n",
            "Epoch 15927  \tTraining Loss: 0.6916914973234655\tValidation Loss: 0.6916857125669035\n",
            "Epoch 15928  \tTraining Loss: 0.6916914373504318\tValidation Loss: 0.6916856520819541\n",
            "Epoch 15929  \tTraining Loss: 0.6916913773766727\tValidation Loss: 0.6916855915962788\n",
            "Epoch 15930  \tTraining Loss: 0.6916913174021883\tValidation Loss: 0.6916855311098776\n",
            "Epoch 15931  \tTraining Loss: 0.6916912574269783\tValidation Loss: 0.6916854706227505\n",
            "Epoch 15932  \tTraining Loss: 0.6916911974510431\tValidation Loss: 0.6916854101348976\n",
            "Epoch 15933  \tTraining Loss: 0.6916911374743823\tValidation Loss: 0.6916853496463186\n",
            "Epoch 15934  \tTraining Loss: 0.6916910774969959\tValidation Loss: 0.6916852891570135\n",
            "Epoch 15935  \tTraining Loss: 0.691691017518884\tValidation Loss: 0.6916852286669826\n",
            "Epoch 15936  \tTraining Loss: 0.6916909575400465\tValidation Loss: 0.6916851681762254\n",
            "Epoch 15937  \tTraining Loss: 0.6916908975604833\tValidation Loss: 0.6916851076847422\n",
            "Epoch 15938  \tTraining Loss: 0.6916908375801945\tValidation Loss: 0.6916850471925329\n",
            "Epoch 15939  \tTraining Loss: 0.6916907775991801\tValidation Loss: 0.6916849866995973\n",
            "Epoch 15940  \tTraining Loss: 0.6916907176174398\tValidation Loss: 0.6916849262059356\n",
            "Epoch 15941  \tTraining Loss: 0.691690657634974\tValidation Loss: 0.6916848657115477\n",
            "Epoch 15942  \tTraining Loss: 0.6916905976517822\tValidation Loss: 0.6916848052164335\n",
            "Epoch 15943  \tTraining Loss: 0.6916905376678646\tValidation Loss: 0.691684744720593\n",
            "Epoch 15944  \tTraining Loss: 0.6916904776832212\tValidation Loss: 0.6916846842240262\n",
            "Epoch 15945  \tTraining Loss: 0.6916904176978519\tValidation Loss: 0.691684623726733\n",
            "Epoch 15946  \tTraining Loss: 0.6916903577117567\tValidation Loss: 0.6916845632287134\n",
            "Epoch 15947  \tTraining Loss: 0.6916902977249355\tValidation Loss: 0.6916845027299674\n",
            "Epoch 15948  \tTraining Loss: 0.6916902377373884\tValidation Loss: 0.691684442230495\n",
            "Epoch 15949  \tTraining Loss: 0.6916901777491151\tValidation Loss: 0.6916843817302961\n",
            "Epoch 15950  \tTraining Loss: 0.6916901177601159\tValidation Loss: 0.6916843212293705\n",
            "Epoch 15951  \tTraining Loss: 0.6916900577703907\tValidation Loss: 0.6916842607277185\n",
            "Epoch 15952  \tTraining Loss: 0.6916899977799394\tValidation Loss: 0.69168420022534\n",
            "Epoch 15953  \tTraining Loss: 0.6916899377887618\tValidation Loss: 0.6916841397222347\n",
            "Epoch 15954  \tTraining Loss: 0.6916898777968581\tValidation Loss: 0.691684079218403\n",
            "Epoch 15955  \tTraining Loss: 0.6916898178042281\tValidation Loss: 0.6916840187138444\n",
            "Epoch 15956  \tTraining Loss: 0.691689757810872\tValidation Loss: 0.6916839582085592\n",
            "Epoch 15957  \tTraining Loss: 0.6916896978167896\tValidation Loss: 0.6916838977025472\n",
            "Epoch 15958  \tTraining Loss: 0.6916896378219809\tValidation Loss: 0.6916838371958085\n",
            "Epoch 15959  \tTraining Loss: 0.6916895778264457\tValidation Loss: 0.691683776688343\n",
            "Epoch 15960  \tTraining Loss: 0.6916895178301843\tValidation Loss: 0.6916837161801506\n",
            "Epoch 15961  \tTraining Loss: 0.6916894578331966\tValidation Loss: 0.6916836556712314\n",
            "Epoch 15962  \tTraining Loss: 0.6916893978354822\tValidation Loss: 0.6916835951615853\n",
            "Epoch 15963  \tTraining Loss: 0.6916893378370415\tValidation Loss: 0.6916835346512121\n",
            "Epoch 15964  \tTraining Loss: 0.6916892778378743\tValidation Loss: 0.6916834741401121\n",
            "Epoch 15965  \tTraining Loss: 0.6916892178379805\tValidation Loss: 0.691683413628285\n",
            "Epoch 15966  \tTraining Loss: 0.6916891578373602\tValidation Loss: 0.6916833531157309\n",
            "Epoch 15967  \tTraining Loss: 0.6916890978360133\tValidation Loss: 0.6916832926024499\n",
            "Epoch 15968  \tTraining Loss: 0.6916890378339396\tValidation Loss: 0.6916832320884417\n",
            "Epoch 15969  \tTraining Loss: 0.6916889778311395\tValidation Loss: 0.6916831715737064\n",
            "Epoch 15970  \tTraining Loss: 0.6916889178276127\tValidation Loss: 0.6916831110582438\n",
            "Epoch 15971  \tTraining Loss: 0.691688857823359\tValidation Loss: 0.6916830505420541\n",
            "Epoch 15972  \tTraining Loss: 0.6916887978183786\tValidation Loss: 0.6916829900251373\n",
            "Epoch 15973  \tTraining Loss: 0.6916887378126714\tValidation Loss: 0.6916829295074932\n",
            "Epoch 15974  \tTraining Loss: 0.6916886778062374\tValidation Loss: 0.6916828689891218\n",
            "Epoch 15975  \tTraining Loss: 0.6916886177990766\tValidation Loss: 0.6916828084700231\n",
            "Epoch 15976  \tTraining Loss: 0.6916885577911889\tValidation Loss: 0.6916827479501969\n",
            "Epoch 15977  \tTraining Loss: 0.6916884977825742\tValidation Loss: 0.6916826874296436\n",
            "Epoch 15978  \tTraining Loss: 0.6916884377732327\tValidation Loss: 0.6916826269083627\n",
            "Epoch 15979  \tTraining Loss: 0.691688377763164\tValidation Loss: 0.6916825663863543\n",
            "Epoch 15980  \tTraining Loss: 0.6916883177523685\tValidation Loss: 0.6916825058636186\n",
            "Epoch 15981  \tTraining Loss: 0.6916882577408457\tValidation Loss: 0.6916824453401553\n",
            "Epoch 15982  \tTraining Loss: 0.691688197728596\tValidation Loss: 0.6916823848159646\n",
            "Epoch 15983  \tTraining Loss: 0.6916881377156192\tValidation Loss: 0.6916823242910461\n",
            "Epoch 15984  \tTraining Loss: 0.6916880777019152\tValidation Loss: 0.6916822637654001\n",
            "Epoch 15985  \tTraining Loss: 0.6916880176874841\tValidation Loss: 0.6916822032390266\n",
            "Epoch 15986  \tTraining Loss: 0.6916879576723258\tValidation Loss: 0.6916821427119253\n",
            "Epoch 15987  \tTraining Loss: 0.69168789765644\tValidation Loss: 0.6916820821840962\n",
            "Epoch 15988  \tTraining Loss: 0.6916878376398272\tValidation Loss: 0.6916820216555396\n",
            "Epoch 15989  \tTraining Loss: 0.691687777622487\tValidation Loss: 0.6916819611262552\n",
            "Epoch 15990  \tTraining Loss: 0.6916877176044194\tValidation Loss: 0.6916819005962429\n",
            "Epoch 15991  \tTraining Loss: 0.6916876575856246\tValidation Loss: 0.6916818400655028\n",
            "Epoch 15992  \tTraining Loss: 0.6916875975661022\tValidation Loss: 0.691681779534035\n",
            "Epoch 15993  \tTraining Loss: 0.6916875375458525\tValidation Loss: 0.6916817190018392\n",
            "Epoch 15994  \tTraining Loss: 0.6916874775248752\tValidation Loss: 0.6916816584689154\n",
            "Epoch 15995  \tTraining Loss: 0.6916874175031705\tValidation Loss: 0.6916815979352637\n",
            "Epoch 15996  \tTraining Loss: 0.6916873574807384\tValidation Loss: 0.6916815374008841\n",
            "Epoch 15997  \tTraining Loss: 0.6916872974575785\tValidation Loss: 0.6916814768657765\n",
            "Epoch 15998  \tTraining Loss: 0.6916872374336912\tValidation Loss: 0.6916814163299407\n",
            "Epoch 15999  \tTraining Loss: 0.691687177409076\tValidation Loss: 0.6916813557933771\n",
            "Epoch 16000  \tTraining Loss: 0.6916871173837333\tValidation Loss: 0.6916812952560851\n",
            "Epoch 16001  \tTraining Loss: 0.6916870573576629\tValidation Loss: 0.691681234718065\n",
            "Epoch 16002  \tTraining Loss: 0.6916869973308648\tValidation Loss: 0.6916811741793168\n",
            "Epoch 16003  \tTraining Loss: 0.691686937303339\tValidation Loss: 0.6916811136398405\n",
            "Epoch 16004  \tTraining Loss: 0.6916868772750852\tValidation Loss: 0.6916810530996358\n",
            "Epoch 16005  \tTraining Loss: 0.6916868172461037\tValidation Loss: 0.6916809925587029\n",
            "Epoch 16006  \tTraining Loss: 0.6916867572163944\tValidation Loss: 0.6916809320170417\n",
            "Epoch 16007  \tTraining Loss: 0.6916866971859571\tValidation Loss: 0.6916808714746521\n",
            "Epoch 16008  \tTraining Loss: 0.691686637154792\tValidation Loss: 0.6916808109315342\n",
            "Epoch 16009  \tTraining Loss: 0.6916865771228988\tValidation Loss: 0.691680750387688\n",
            "Epoch 16010  \tTraining Loss: 0.6916865170902777\tValidation Loss: 0.6916806898431132\n",
            "Epoch 16011  \tTraining Loss: 0.6916864570569285\tValidation Loss: 0.69168062929781\n",
            "Epoch 16012  \tTraining Loss: 0.6916863970228514\tValidation Loss: 0.6916805687517783\n",
            "Epoch 16013  \tTraining Loss: 0.691686336988046\tValidation Loss: 0.691680508205018\n",
            "Epoch 16014  \tTraining Loss: 0.6916862769525127\tValidation Loss: 0.6916804476575293\n",
            "Epoch 16015  \tTraining Loss: 0.6916862169162513\tValidation Loss: 0.691680387109312\n",
            "Epoch 16016  \tTraining Loss: 0.6916861568792615\tValidation Loss: 0.691680326560366\n",
            "Epoch 16017  \tTraining Loss: 0.6916860968415436\tValidation Loss: 0.6916802660106914\n",
            "Epoch 16018  \tTraining Loss: 0.6916860368030975\tValidation Loss: 0.6916802054602882\n",
            "Epoch 16019  \tTraining Loss: 0.691685976763923\tValidation Loss: 0.6916801449091561\n",
            "Epoch 16020  \tTraining Loss: 0.6916859167240202\tValidation Loss: 0.6916800843572953\n",
            "Epoch 16021  \tTraining Loss: 0.6916858566833891\tValidation Loss: 0.6916800238047057\n",
            "Epoch 16022  \tTraining Loss: 0.6916857966420297\tValidation Loss: 0.6916799632513874\n",
            "Epoch 16023  \tTraining Loss: 0.6916857365999417\tValidation Loss: 0.6916799026973403\n",
            "Epoch 16024  \tTraining Loss: 0.6916856765571254\tValidation Loss: 0.6916798421425642\n",
            "Epoch 16025  \tTraining Loss: 0.6916856165135806\tValidation Loss: 0.6916797815870591\n",
            "Epoch 16026  \tTraining Loss: 0.6916855564693074\tValidation Loss: 0.6916797210308253\n",
            "Epoch 16027  \tTraining Loss: 0.6916854964243057\tValidation Loss: 0.6916796604738624\n",
            "Epoch 16028  \tTraining Loss: 0.6916854363785752\tValidation Loss: 0.6916795999161706\n",
            "Epoch 16029  \tTraining Loss: 0.6916853763321161\tValidation Loss: 0.6916795393577496\n",
            "Epoch 16030  \tTraining Loss: 0.6916853162849285\tValidation Loss: 0.6916794787985997\n",
            "Epoch 16031  \tTraining Loss: 0.6916852562370123\tValidation Loss: 0.6916794182387206\n",
            "Epoch 16032  \tTraining Loss: 0.6916851961883672\tValidation Loss: 0.6916793576781123\n",
            "Epoch 16033  \tTraining Loss: 0.6916851361389934\tValidation Loss: 0.691679297116775\n",
            "Epoch 16034  \tTraining Loss: 0.6916850760888908\tValidation Loss: 0.6916792365547084\n",
            "Epoch 16035  \tTraining Loss: 0.6916850160380597\tValidation Loss: 0.6916791759919128\n",
            "Epoch 16036  \tTraining Loss: 0.6916849559864995\tValidation Loss: 0.6916791154283877\n",
            "Epoch 16037  \tTraining Loss: 0.6916848959342106\tValidation Loss: 0.6916790548641334\n",
            "Epoch 16038  \tTraining Loss: 0.6916848358811927\tValidation Loss: 0.6916789942991497\n",
            "Epoch 16039  \tTraining Loss: 0.6916847758274458\tValidation Loss: 0.6916789337334367\n",
            "Epoch 16040  \tTraining Loss: 0.6916847157729701\tValidation Loss: 0.6916788731669944\n",
            "Epoch 16041  \tTraining Loss: 0.6916846557177654\tValidation Loss: 0.6916788125998227\n",
            "Epoch 16042  \tTraining Loss: 0.6916845956618316\tValidation Loss: 0.6916787520319213\n",
            "Epoch 16043  \tTraining Loss: 0.6916845356051688\tValidation Loss: 0.6916786914632906\n",
            "Epoch 16044  \tTraining Loss: 0.6916844755477769\tValidation Loss: 0.6916786308939304\n",
            "Epoch 16045  \tTraining Loss: 0.6916844154896558\tValidation Loss: 0.6916785703238406\n",
            "Epoch 16046  \tTraining Loss: 0.6916843554308056\tValidation Loss: 0.6916785097530213\n",
            "Epoch 16047  \tTraining Loss: 0.6916842953712263\tValidation Loss: 0.6916784491814723\n",
            "Epoch 16048  \tTraining Loss: 0.6916842353109177\tValidation Loss: 0.6916783886091937\n",
            "Epoch 16049  \tTraining Loss: 0.6916841752498799\tValidation Loss: 0.6916783280361855\n",
            "Epoch 16050  \tTraining Loss: 0.6916841151881128\tValidation Loss: 0.6916782674624475\n",
            "Epoch 16051  \tTraining Loss: 0.6916840551256164\tValidation Loss: 0.6916782068879798\n",
            "Epoch 16052  \tTraining Loss: 0.6916839950623905\tValidation Loss: 0.6916781463127823\n",
            "Epoch 16053  \tTraining Loss: 0.6916839349984354\tValidation Loss: 0.6916780857368551\n",
            "Epoch 16054  \tTraining Loss: 0.6916838749337508\tValidation Loss: 0.6916780251601979\n",
            "Epoch 16055  \tTraining Loss: 0.6916838148683367\tValidation Loss: 0.6916779645828111\n",
            "Epoch 16056  \tTraining Loss: 0.6916837548021934\tValidation Loss: 0.6916779040046942\n",
            "Epoch 16057  \tTraining Loss: 0.6916836947353204\tValidation Loss: 0.6916778434258474\n",
            "Epoch 16058  \tTraining Loss: 0.6916836346677178\tValidation Loss: 0.6916777828462707\n",
            "Epoch 16059  \tTraining Loss: 0.6916835745993857\tValidation Loss: 0.6916777222659639\n",
            "Epoch 16060  \tTraining Loss: 0.6916835145303241\tValidation Loss: 0.6916776616849272\n",
            "Epoch 16061  \tTraining Loss: 0.6916834544605327\tValidation Loss: 0.6916776011031603\n",
            "Epoch 16062  \tTraining Loss: 0.6916833943900117\tValidation Loss: 0.6916775405206635\n",
            "Epoch 16063  \tTraining Loss: 0.691683334318761\tValidation Loss: 0.6916774799374366\n",
            "Epoch 16064  \tTraining Loss: 0.6916832742467806\tValidation Loss: 0.6916774193534794\n",
            "Epoch 16065  \tTraining Loss: 0.6916832141740703\tValidation Loss: 0.6916773587687921\n",
            "Epoch 16066  \tTraining Loss: 0.6916831541006303\tValidation Loss: 0.6916772981833745\n",
            "Epoch 16067  \tTraining Loss: 0.6916830940264603\tValidation Loss: 0.6916772375972268\n",
            "Epoch 16068  \tTraining Loss: 0.6916830339515607\tValidation Loss: 0.6916771770103487\n",
            "Epoch 16069  \tTraining Loss: 0.691682973875931\tValidation Loss: 0.6916771164227402\n",
            "Epoch 16070  \tTraining Loss: 0.6916829137995716\tValidation Loss: 0.6916770558344015\n",
            "Epoch 16071  \tTraining Loss: 0.6916828537224821\tValidation Loss: 0.6916769952453324\n",
            "Epoch 16072  \tTraining Loss: 0.6916827936446626\tValidation Loss: 0.6916769346555329\n",
            "Epoch 16073  \tTraining Loss: 0.691682733566113\tValidation Loss: 0.691676874065003\n",
            "Epoch 16074  \tTraining Loss: 0.6916826734868334\tValidation Loss: 0.6916768134737427\n",
            "Epoch 16075  \tTraining Loss: 0.6916826134068237\tValidation Loss: 0.6916767528817517\n",
            "Epoch 16076  \tTraining Loss: 0.6916825533260839\tValidation Loss: 0.6916766922890303\n",
            "Epoch 16077  \tTraining Loss: 0.691682493244614\tValidation Loss: 0.6916766316955782\n",
            "Epoch 16078  \tTraining Loss: 0.6916824331624138\tValidation Loss: 0.6916765711013957\n",
            "Epoch 16079  \tTraining Loss: 0.6916823730794834\tValidation Loss: 0.6916765105064825\n",
            "Epoch 16080  \tTraining Loss: 0.6916823129958228\tValidation Loss: 0.6916764499108385\n",
            "Epoch 16081  \tTraining Loss: 0.6916822529114318\tValidation Loss: 0.691676389314464\n",
            "Epoch 16082  \tTraining Loss: 0.6916821928263107\tValidation Loss: 0.6916763287173587\n",
            "Epoch 16083  \tTraining Loss: 0.6916821327404592\tValidation Loss: 0.6916762681195227\n",
            "Epoch 16084  \tTraining Loss: 0.6916820726538772\tValidation Loss: 0.6916762075209558\n",
            "Epoch 16085  \tTraining Loss: 0.6916820125665648\tValidation Loss: 0.6916761469216581\n",
            "Epoch 16086  \tTraining Loss: 0.6916819524785219\tValidation Loss: 0.6916760863216296\n",
            "Epoch 16087  \tTraining Loss: 0.6916818923897486\tValidation Loss: 0.6916760257208703\n",
            "Epoch 16088  \tTraining Loss: 0.6916818323002449\tValidation Loss: 0.69167596511938\n",
            "Epoch 16089  \tTraining Loss: 0.6916817722100106\tValidation Loss: 0.6916759045171588\n",
            "Epoch 16090  \tTraining Loss: 0.6916817121190456\tValidation Loss: 0.6916758439142064\n",
            "Epoch 16091  \tTraining Loss: 0.6916816520273501\tValidation Loss: 0.6916757833105233\n",
            "Epoch 16092  \tTraining Loss: 0.6916815919349238\tValidation Loss: 0.691675722706109\n",
            "Epoch 16093  \tTraining Loss: 0.6916815318417671\tValidation Loss: 0.6916756621009637\n",
            "Epoch 16094  \tTraining Loss: 0.6916814717478794\tValidation Loss: 0.6916756014950872\n",
            "Epoch 16095  \tTraining Loss: 0.6916814116532611\tValidation Loss: 0.6916755408884796\n",
            "Epoch 16096  \tTraining Loss: 0.691681351557912\tValidation Loss: 0.6916754802811408\n",
            "Epoch 16097  \tTraining Loss: 0.6916812914618321\tValidation Loss: 0.6916754196730708\n",
            "Epoch 16098  \tTraining Loss: 0.6916812313650215\tValidation Loss: 0.6916753590642696\n",
            "Epoch 16099  \tTraining Loss: 0.6916811712674799\tValidation Loss: 0.6916752984547371\n",
            "Epoch 16100  \tTraining Loss: 0.6916811111692074\tValidation Loss: 0.6916752378444733\n",
            "Epoch 16101  \tTraining Loss: 0.6916810510702038\tValidation Loss: 0.6916751772334783\n",
            "Epoch 16102  \tTraining Loss: 0.6916809909704695\tValidation Loss: 0.6916751166217517\n",
            "Epoch 16103  \tTraining Loss: 0.6916809308700042\tValidation Loss: 0.6916750560092939\n",
            "Epoch 16104  \tTraining Loss: 0.6916808707688077\tValidation Loss: 0.6916749953961044\n",
            "Epoch 16105  \tTraining Loss: 0.6916808106668803\tValidation Loss: 0.6916749347821839\n",
            "Epoch 16106  \tTraining Loss: 0.6916807505642215\tValidation Loss: 0.6916748741675316\n",
            "Epoch 16107  \tTraining Loss: 0.6916806904608319\tValidation Loss: 0.6916748135521478\n",
            "Epoch 16108  \tTraining Loss: 0.691680630356711\tValidation Loss: 0.6916747529360324\n",
            "Epoch 16109  \tTraining Loss: 0.6916805702518589\tValidation Loss: 0.6916746923191857\n",
            "Epoch 16110  \tTraining Loss: 0.6916805101462756\tValidation Loss: 0.6916746317016071\n",
            "Epoch 16111  \tTraining Loss: 0.6916804500399611\tValidation Loss: 0.6916745710832969\n",
            "Epoch 16112  \tTraining Loss: 0.6916803899329153\tValidation Loss: 0.691674510464255\n",
            "Epoch 16113  \tTraining Loss: 0.6916803298251382\tValidation Loss: 0.6916744498444815\n",
            "Epoch 16114  \tTraining Loss: 0.6916802697166295\tValidation Loss: 0.6916743892239762\n",
            "Epoch 16115  \tTraining Loss: 0.6916802096073897\tValidation Loss: 0.691674328602739\n",
            "Epoch 16116  \tTraining Loss: 0.6916801494974184\tValidation Loss: 0.6916742679807701\n",
            "Epoch 16117  \tTraining Loss: 0.6916800893867155\tValidation Loss: 0.6916742073580694\n",
            "Epoch 16118  \tTraining Loss: 0.6916800292752813\tValidation Loss: 0.6916741467346369\n",
            "Epoch 16119  \tTraining Loss: 0.6916799691631156\tValidation Loss: 0.6916740861104722\n",
            "Epoch 16120  \tTraining Loss: 0.6916799090502183\tValidation Loss: 0.6916740254855758\n",
            "Epoch 16121  \tTraining Loss: 0.6916798489365893\tValidation Loss: 0.6916739648599474\n",
            "Epoch 16122  \tTraining Loss: 0.6916797888222288\tValidation Loss: 0.691673904233587\n",
            "Epoch 16123  \tTraining Loss: 0.6916797287071366\tValidation Loss: 0.6916738436064944\n",
            "Epoch 16124  \tTraining Loss: 0.6916796685913128\tValidation Loss: 0.69167378297867\n",
            "Epoch 16125  \tTraining Loss: 0.6916796084747572\tValidation Loss: 0.6916737223501134\n",
            "Epoch 16126  \tTraining Loss: 0.6916795483574699\tValidation Loss: 0.6916736617208246\n",
            "Epoch 16127  \tTraining Loss: 0.6916794882394508\tValidation Loss: 0.6916736010908038\n",
            "Epoch 16128  \tTraining Loss: 0.6916794281206999\tValidation Loss: 0.6916735404600505\n",
            "Epoch 16129  \tTraining Loss: 0.6916793680012171\tValidation Loss: 0.6916734798285652\n",
            "Epoch 16130  \tTraining Loss: 0.6916793078810025\tValidation Loss: 0.6916734191963476\n",
            "Epoch 16131  \tTraining Loss: 0.691679247760056\tValidation Loss: 0.6916733585633977\n",
            "Epoch 16132  \tTraining Loss: 0.6916791876383774\tValidation Loss: 0.6916732979297157\n",
            "Epoch 16133  \tTraining Loss: 0.691679127515967\tValidation Loss: 0.6916732372953011\n",
            "Epoch 16134  \tTraining Loss: 0.6916790673928247\tValidation Loss: 0.6916731766601542\n",
            "Epoch 16135  \tTraining Loss: 0.6916790072689502\tValidation Loss: 0.6916731160242747\n",
            "Epoch 16136  \tTraining Loss: 0.6916789471443436\tValidation Loss: 0.691673055387663\n",
            "Epoch 16137  \tTraining Loss: 0.6916788870190052\tValidation Loss: 0.6916729947503186\n",
            "Epoch 16138  \tTraining Loss: 0.6916788268929343\tValidation Loss: 0.6916729341122418\n",
            "Epoch 16139  \tTraining Loss: 0.6916787667661313\tValidation Loss: 0.6916728734734325\n",
            "Epoch 16140  \tTraining Loss: 0.6916787066385962\tValidation Loss: 0.6916728128338906\n",
            "Epoch 16141  \tTraining Loss: 0.6916786465103288\tValidation Loss: 0.691672752193616\n",
            "Epoch 16142  \tTraining Loss: 0.6916785863813291\tValidation Loss: 0.6916726915526088\n",
            "Epoch 16143  \tTraining Loss: 0.6916785262515972\tValidation Loss: 0.6916726309108691\n",
            "Epoch 16144  \tTraining Loss: 0.6916784661211329\tValidation Loss: 0.6916725702683965\n",
            "Epoch 16145  \tTraining Loss: 0.6916784059899364\tValidation Loss: 0.6916725096251912\n",
            "Epoch 16146  \tTraining Loss: 0.6916783458580072\tValidation Loss: 0.6916724489812531\n",
            "Epoch 16147  \tTraining Loss: 0.691678285725346\tValidation Loss: 0.6916723883365823\n",
            "Epoch 16148  \tTraining Loss: 0.691678225591952\tValidation Loss: 0.6916723276911786\n",
            "Epoch 16149  \tTraining Loss: 0.6916781654578256\tValidation Loss: 0.691672267045042\n",
            "Epoch 16150  \tTraining Loss: 0.6916781053229667\tValidation Loss: 0.6916722063981727\n",
            "Epoch 16151  \tTraining Loss: 0.6916780451873752\tValidation Loss: 0.6916721457505702\n",
            "Epoch 16152  \tTraining Loss: 0.6916779850510512\tValidation Loss: 0.6916720851022349\n",
            "Epoch 16153  \tTraining Loss: 0.6916779249139945\tValidation Loss: 0.6916720244531666\n",
            "Epoch 16154  \tTraining Loss: 0.6916778647762052\tValidation Loss: 0.6916719638033652\n",
            "Epoch 16155  \tTraining Loss: 0.6916778046376831\tValidation Loss: 0.6916719031528309\n",
            "Epoch 16156  \tTraining Loss: 0.6916777444984283\tValidation Loss: 0.6916718425015633\n",
            "Epoch 16157  \tTraining Loss: 0.6916776843584408\tValidation Loss: 0.6916717818495628\n",
            "Epoch 16158  \tTraining Loss: 0.6916776242177206\tValidation Loss: 0.691671721196829\n",
            "Epoch 16159  \tTraining Loss: 0.6916775640762676\tValidation Loss: 0.6916716605433619\n",
            "Epoch 16160  \tTraining Loss: 0.6916775039340817\tValidation Loss: 0.6916715998891619\n",
            "Epoch 16161  \tTraining Loss: 0.6916774437911628\tValidation Loss: 0.6916715392342284\n",
            "Epoch 16162  \tTraining Loss: 0.6916773836475111\tValidation Loss: 0.6916714785785617\n",
            "Epoch 16163  \tTraining Loss: 0.6916773235031265\tValidation Loss: 0.6916714179221617\n",
            "Epoch 16164  \tTraining Loss: 0.6916772633580088\tValidation Loss: 0.6916713572650283\n",
            "Epoch 16165  \tTraining Loss: 0.6916772032121582\tValidation Loss: 0.6916712966071615\n",
            "Epoch 16166  \tTraining Loss: 0.6916771430655745\tValidation Loss: 0.6916712359485615\n",
            "Epoch 16167  \tTraining Loss: 0.6916770829182577\tValidation Loss: 0.691671175289228\n",
            "Epoch 16168  \tTraining Loss: 0.6916770227702078\tValidation Loss: 0.6916711146291608\n",
            "Epoch 16169  \tTraining Loss: 0.6916769626214248\tValidation Loss: 0.6916710539683603\n",
            "Epoch 16170  \tTraining Loss: 0.6916769024719086\tValidation Loss: 0.6916709933068261\n",
            "Epoch 16171  \tTraining Loss: 0.6916768423216592\tValidation Loss: 0.6916709326445585\n",
            "Epoch 16172  \tTraining Loss: 0.6916767821706766\tValidation Loss: 0.6916708719815573\n",
            "Epoch 16173  \tTraining Loss: 0.6916767220189607\tValidation Loss: 0.6916708113178224\n",
            "Epoch 16174  \tTraining Loss: 0.6916766618665114\tValidation Loss: 0.6916707506533538\n",
            "Epoch 16175  \tTraining Loss: 0.691676601713329\tValidation Loss: 0.6916706899881515\n",
            "Epoch 16176  \tTraining Loss: 0.691676541559413\tValidation Loss: 0.6916706293222157\n",
            "Epoch 16177  \tTraining Loss: 0.6916764814047638\tValidation Loss: 0.6916705686555458\n",
            "Epoch 16178  \tTraining Loss: 0.6916764212493811\tValidation Loss: 0.6916705079881422\n",
            "Epoch 16179  \tTraining Loss: 0.6916763610932648\tValidation Loss: 0.6916704473200048\n",
            "Epoch 16180  \tTraining Loss: 0.6916763009364151\tValidation Loss: 0.6916703866511337\n",
            "Epoch 16181  \tTraining Loss: 0.6916762407788319\tValidation Loss: 0.6916703259815286\n",
            "Epoch 16182  \tTraining Loss: 0.6916761806205152\tValidation Loss: 0.6916702653111895\n",
            "Epoch 16183  \tTraining Loss: 0.6916761204614646\tValidation Loss: 0.6916702046401166\n",
            "Epoch 16184  \tTraining Loss: 0.6916760603016808\tValidation Loss: 0.6916701439683096\n",
            "Epoch 16185  \tTraining Loss: 0.6916760001411631\tValidation Loss: 0.6916700832957686\n",
            "Epoch 16186  \tTraining Loss: 0.6916759399799118\tValidation Loss: 0.6916700226224936\n",
            "Epoch 16187  \tTraining Loss: 0.6916758798179267\tValidation Loss: 0.6916699619484845\n",
            "Epoch 16188  \tTraining Loss: 0.6916758196552079\tValidation Loss: 0.6916699012737412\n",
            "Epoch 16189  \tTraining Loss: 0.6916757594917552\tValidation Loss: 0.691669840598264\n",
            "Epoch 16190  \tTraining Loss: 0.6916756993275688\tValidation Loss: 0.6916697799220523\n",
            "Epoch 16191  \tTraining Loss: 0.6916756391626485\tValidation Loss: 0.6916697192451066\n",
            "Epoch 16192  \tTraining Loss: 0.6916755789969943\tValidation Loss: 0.6916696585674266\n",
            "Epoch 16193  \tTraining Loss: 0.6916755188306062\tValidation Loss: 0.6916695978890124\n",
            "Epoch 16194  \tTraining Loss: 0.6916754586634842\tValidation Loss: 0.6916695372098638\n",
            "Epoch 16195  \tTraining Loss: 0.6916753984956281\tValidation Loss: 0.6916694765299809\n",
            "Epoch 16196  \tTraining Loss: 0.6916753383270381\tValidation Loss: 0.6916694158493636\n",
            "Epoch 16197  \tTraining Loss: 0.691675278157714\tValidation Loss: 0.6916693551680119\n",
            "Epoch 16198  \tTraining Loss: 0.6916752179876559\tValidation Loss: 0.691669294485926\n",
            "Epoch 16199  \tTraining Loss: 0.6916751578168634\tValidation Loss: 0.6916692338031053\n",
            "Epoch 16200  \tTraining Loss: 0.691675097645337\tValidation Loss: 0.6916691731195502\n",
            "Epoch 16201  \tTraining Loss: 0.6916750374730763\tValidation Loss: 0.6916691124352606\n",
            "Epoch 16202  \tTraining Loss: 0.6916749773000815\tValidation Loss: 0.6916690517502365\n",
            "Epoch 16203  \tTraining Loss: 0.6916749171263527\tValidation Loss: 0.6916689910644777\n",
            "Epoch 16204  \tTraining Loss: 0.6916748569518891\tValidation Loss: 0.6916689303779844\n",
            "Epoch 16205  \tTraining Loss: 0.6916747967766915\tValidation Loss: 0.6916688696907562\n",
            "Epoch 16206  \tTraining Loss: 0.6916747366007595\tValidation Loss: 0.6916688090027935\n",
            "Epoch 16207  \tTraining Loss: 0.6916746764240932\tValidation Loss: 0.6916687483140961\n",
            "Epoch 16208  \tTraining Loss: 0.6916746162466924\tValidation Loss: 0.691668687624664\n",
            "Epoch 16209  \tTraining Loss: 0.6916745560685571\tValidation Loss: 0.6916686269344969\n",
            "Epoch 16210  \tTraining Loss: 0.6916744958896874\tValidation Loss: 0.6916685662435951\n",
            "Epoch 16211  \tTraining Loss: 0.6916744357100832\tValidation Loss: 0.6916685055519585\n",
            "Epoch 16212  \tTraining Loss: 0.6916743755297445\tValidation Loss: 0.691668444859587\n",
            "Epoch 16213  \tTraining Loss: 0.6916743153486713\tValidation Loss: 0.6916683841664806\n",
            "Epoch 16214  \tTraining Loss: 0.6916742551668634\tValidation Loss: 0.6916683234726392\n",
            "Epoch 16215  \tTraining Loss: 0.6916741949843208\tValidation Loss: 0.6916682627780628\n",
            "Epoch 16216  \tTraining Loss: 0.6916741348010437\tValidation Loss: 0.6916682020827515\n",
            "Epoch 16217  \tTraining Loss: 0.6916740746170318\tValidation Loss: 0.6916681413867051\n",
            "Epoch 16218  \tTraining Loss: 0.6916740144322852\tValidation Loss: 0.6916680806899236\n",
            "Epoch 16219  \tTraining Loss: 0.6916739542468038\tValidation Loss: 0.691668019992407\n",
            "Epoch 16220  \tTraining Loss: 0.6916738940605875\tValidation Loss: 0.6916679592941553\n",
            "Epoch 16221  \tTraining Loss: 0.6916738338736366\tValidation Loss: 0.6916678985951683\n",
            "Epoch 16222  \tTraining Loss: 0.6916737736859507\tValidation Loss: 0.6916678378954463\n",
            "Epoch 16223  \tTraining Loss: 0.6916737134975299\tValidation Loss: 0.691667777194989\n",
            "Epoch 16224  \tTraining Loss: 0.6916736533083743\tValidation Loss: 0.6916677164937963\n",
            "Epoch 16225  \tTraining Loss: 0.6916735931184836\tValidation Loss: 0.6916676557918684\n",
            "Epoch 16226  \tTraining Loss: 0.6916735329278579\tValidation Loss: 0.6916675950892053\n",
            "Epoch 16227  \tTraining Loss: 0.6916734727364974\tValidation Loss: 0.6916675343858065\n",
            "Epoch 16228  \tTraining Loss: 0.6916734125444016\tValidation Loss: 0.6916674736816726\n",
            "Epoch 16229  \tTraining Loss: 0.6916733523515709\tValidation Loss: 0.6916674129768032\n",
            "Epoch 16230  \tTraining Loss: 0.6916732921580049\tValidation Loss: 0.6916673522711984\n",
            "Epoch 16231  \tTraining Loss: 0.6916732319637039\tValidation Loss: 0.6916672915648581\n",
            "Epoch 16232  \tTraining Loss: 0.6916731717686676\tValidation Loss: 0.6916672308577821\n",
            "Epoch 16233  \tTraining Loss: 0.6916731115728961\tValidation Loss: 0.6916671701499706\n",
            "Epoch 16234  \tTraining Loss: 0.6916730513763895\tValidation Loss: 0.6916671094414237\n",
            "Epoch 16235  \tTraining Loss: 0.6916729911791474\tValidation Loss: 0.691667048732141\n",
            "Epoch 16236  \tTraining Loss: 0.69167293098117\tValidation Loss: 0.6916669880221228\n",
            "Epoch 16237  \tTraining Loss: 0.6916728707824574\tValidation Loss: 0.6916669273113689\n",
            "Epoch 16238  \tTraining Loss: 0.6916728105830093\tValidation Loss: 0.6916668665998791\n",
            "Epoch 16239  \tTraining Loss: 0.6916727503828259\tValidation Loss: 0.6916668058876537\n",
            "Epoch 16240  \tTraining Loss: 0.6916726901819069\tValidation Loss: 0.6916667451746925\n",
            "Epoch 16241  \tTraining Loss: 0.6916726299802526\tValidation Loss: 0.6916666844609954\n",
            "Epoch 16242  \tTraining Loss: 0.6916725697778626\tValidation Loss: 0.6916666237465625\n",
            "Epoch 16243  \tTraining Loss: 0.6916725095747371\tValidation Loss: 0.6916665630313938\n",
            "Epoch 16244  \tTraining Loss: 0.6916724493708762\tValidation Loss: 0.6916665023154893\n",
            "Epoch 16245  \tTraining Loss: 0.6916723891662796\tValidation Loss: 0.6916664415988486\n",
            "Epoch 16246  \tTraining Loss: 0.6916723289609473\tValidation Loss: 0.6916663808814721\n",
            "Epoch 16247  \tTraining Loss: 0.6916722687548794\tValidation Loss: 0.6916663201633596\n",
            "Epoch 16248  \tTraining Loss: 0.6916722085480759\tValidation Loss: 0.6916662594445111\n",
            "Epoch 16249  \tTraining Loss: 0.6916721483405365\tValidation Loss: 0.6916661987249263\n",
            "Epoch 16250  \tTraining Loss: 0.6916720881322612\tValidation Loss: 0.6916661380046055\n",
            "Epoch 16251  \tTraining Loss: 0.6916720279232503\tValidation Loss: 0.6916660772835488\n",
            "Epoch 16252  \tTraining Loss: 0.6916719677135035\tValidation Loss: 0.6916660165617556\n",
            "Epoch 16253  \tTraining Loss: 0.6916719075030209\tValidation Loss: 0.6916659558392264\n",
            "Epoch 16254  \tTraining Loss: 0.6916718472918023\tValidation Loss: 0.6916658951159609\n",
            "Epoch 16255  \tTraining Loss: 0.6916717870798479\tValidation Loss: 0.6916658343919593\n",
            "Epoch 16256  \tTraining Loss: 0.6916717268671574\tValidation Loss: 0.6916657736672212\n",
            "Epoch 16257  \tTraining Loss: 0.6916716666537309\tValidation Loss: 0.691665712941747\n",
            "Epoch 16258  \tTraining Loss: 0.6916716064395685\tValidation Loss: 0.6916656522155362\n",
            "Epoch 16259  \tTraining Loss: 0.6916715462246699\tValidation Loss: 0.6916655914885891\n",
            "Epoch 16260  \tTraining Loss: 0.6916714860090352\tValidation Loss: 0.6916655307609056\n",
            "Epoch 16261  \tTraining Loss: 0.6916714257926646\tValidation Loss: 0.6916654700324857\n",
            "Epoch 16262  \tTraining Loss: 0.6916713655755576\tValidation Loss: 0.6916654093033292\n",
            "Epoch 16263  \tTraining Loss: 0.6916713053577145\tValidation Loss: 0.6916653485734361\n",
            "Epoch 16264  \tTraining Loss: 0.6916712451391351\tValidation Loss: 0.6916652878428068\n",
            "Epoch 16265  \tTraining Loss: 0.6916711849198195\tValidation Loss: 0.6916652271114406\n",
            "Epoch 16266  \tTraining Loss: 0.6916711246997677\tValidation Loss: 0.691665166379338\n",
            "Epoch 16267  \tTraining Loss: 0.6916710644789794\tValidation Loss: 0.6916651056464985\n",
            "Epoch 16268  \tTraining Loss: 0.6916710042574549\tValidation Loss: 0.6916650449129226\n",
            "Epoch 16269  \tTraining Loss: 0.6916709440351939\tValidation Loss: 0.6916649841786099\n",
            "Epoch 16270  \tTraining Loss: 0.6916708838121965\tValidation Loss: 0.6916649234435603\n",
            "Epoch 16271  \tTraining Loss: 0.6916708235884628\tValidation Loss: 0.6916648627077742\n",
            "Epoch 16272  \tTraining Loss: 0.6916707633639925\tValidation Loss: 0.6916648019712511\n",
            "Epoch 16273  \tTraining Loss: 0.6916707031387856\tValidation Loss: 0.6916647412339912\n",
            "Epoch 16274  \tTraining Loss: 0.6916706429128422\tValidation Loss: 0.6916646804959945\n",
            "Epoch 16275  \tTraining Loss: 0.6916705826861622\tValidation Loss: 0.6916646197572608\n",
            "Epoch 16276  \tTraining Loss: 0.6916705224587457\tValidation Loss: 0.6916645590177903\n",
            "Epoch 16277  \tTraining Loss: 0.6916704622305925\tValidation Loss: 0.6916644982775827\n",
            "Epoch 16278  \tTraining Loss: 0.6916704020017025\tValidation Loss: 0.6916644375366381\n",
            "Epoch 16279  \tTraining Loss: 0.6916703417720759\tValidation Loss: 0.6916643767949565\n",
            "Epoch 16280  \tTraining Loss: 0.6916702815417126\tValidation Loss: 0.6916643160525379\n",
            "Epoch 16281  \tTraining Loss: 0.6916702213106124\tValidation Loss: 0.6916642553093822\n",
            "Epoch 16282  \tTraining Loss: 0.6916701610787755\tValidation Loss: 0.6916641945654893\n",
            "Epoch 16283  \tTraining Loss: 0.6916701008462016\tValidation Loss: 0.6916641338208595\n",
            "Epoch 16284  \tTraining Loss: 0.6916700406128911\tValidation Loss: 0.6916640730754923\n",
            "Epoch 16285  \tTraining Loss: 0.6916699803788434\tValidation Loss: 0.6916640123293878\n",
            "Epoch 16286  \tTraining Loss: 0.6916699201440589\tValidation Loss: 0.6916639515825461\n",
            "Epoch 16287  \tTraining Loss: 0.6916698599085375\tValidation Loss: 0.6916638908349672\n",
            "Epoch 16288  \tTraining Loss: 0.6916697996722789\tValidation Loss: 0.6916638300866508\n",
            "Epoch 16289  \tTraining Loss: 0.6916697394352834\tValidation Loss: 0.6916637693375972\n",
            "Epoch 16290  \tTraining Loss: 0.6916696791975507\tValidation Loss: 0.6916637085878061\n",
            "Epoch 16291  \tTraining Loss: 0.6916696189590811\tValidation Loss: 0.6916636478372779\n",
            "Epoch 16292  \tTraining Loss: 0.6916695587198741\tValidation Loss: 0.691663587086012\n",
            "Epoch 16293  \tTraining Loss: 0.6916694984799302\tValidation Loss: 0.6916635263340086\n",
            "Epoch 16294  \tTraining Loss: 0.6916694382392491\tValidation Loss: 0.6916634655812678\n",
            "Epoch 16295  \tTraining Loss: 0.6916693779978307\tValidation Loss: 0.6916634048277893\n",
            "Epoch 16296  \tTraining Loss: 0.6916693177556749\tValidation Loss: 0.6916633440735733\n",
            "Epoch 16297  \tTraining Loss: 0.6916692575127819\tValidation Loss: 0.6916632833186197\n",
            "Epoch 16298  \tTraining Loss: 0.6916691972691514\tValidation Loss: 0.6916632225629286\n",
            "Epoch 16299  \tTraining Loss: 0.6916691370247838\tValidation Loss: 0.6916631618064996\n",
            "Epoch 16300  \tTraining Loss: 0.6916690767796787\tValidation Loss: 0.691663101049333\n",
            "Epoch 16301  \tTraining Loss: 0.6916690165338362\tValidation Loss: 0.6916630402914287\n",
            "Epoch 16302  \tTraining Loss: 0.6916689562872562\tValidation Loss: 0.6916629795327865\n",
            "Epoch 16303  \tTraining Loss: 0.6916688960399388\tValidation Loss: 0.6916629187734067\n",
            "Epoch 16304  \tTraining Loss: 0.6916688357918838\tValidation Loss: 0.691662858013289\n",
            "Epoch 16305  \tTraining Loss: 0.6916687755430911\tValidation Loss: 0.6916627972524334\n",
            "Epoch 16306  \tTraining Loss: 0.691668715293561\tValidation Loss: 0.6916627364908398\n",
            "Epoch 16307  \tTraining Loss: 0.6916686550432932\tValidation Loss: 0.6916626757285085\n",
            "Epoch 16308  \tTraining Loss: 0.6916685947922878\tValidation Loss: 0.6916626149654391\n",
            "Epoch 16309  \tTraining Loss: 0.6916685345405447\tValidation Loss: 0.6916625542016317\n",
            "Epoch 16310  \tTraining Loss: 0.6916684742880638\tValidation Loss: 0.6916624934370863\n",
            "Epoch 16311  \tTraining Loss: 0.6916684140348452\tValidation Loss: 0.6916624326718029\n",
            "Epoch 16312  \tTraining Loss: 0.6916683537808889\tValidation Loss: 0.6916623719057814\n",
            "Epoch 16313  \tTraining Loss: 0.6916682935261946\tValidation Loss: 0.6916623111390217\n",
            "Epoch 16314  \tTraining Loss: 0.6916682332707625\tValidation Loss: 0.6916622503715237\n",
            "Epoch 16315  \tTraining Loss: 0.6916681730145926\tValidation Loss: 0.6916621896032878\n",
            "Epoch 16316  \tTraining Loss: 0.6916681127576848\tValidation Loss: 0.6916621288343137\n",
            "Epoch 16317  \tTraining Loss: 0.691668052500039\tValidation Loss: 0.6916620680646012\n",
            "Epoch 16318  \tTraining Loss: 0.6916679922416552\tValidation Loss: 0.6916620072941504\n",
            "Epoch 16319  \tTraining Loss: 0.6916679319825334\tValidation Loss: 0.6916619465229613\n",
            "Epoch 16320  \tTraining Loss: 0.6916678717226736\tValidation Loss: 0.6916618857510339\n",
            "Epoch 16321  \tTraining Loss: 0.6916678114620757\tValidation Loss: 0.6916618249783681\n",
            "Epoch 16322  \tTraining Loss: 0.6916677512007398\tValidation Loss: 0.691661764204964\n",
            "Epoch 16323  \tTraining Loss: 0.6916676909386654\tValidation Loss: 0.6916617034308213\n",
            "Epoch 16324  \tTraining Loss: 0.6916676306758531\tValidation Loss: 0.6916616426559402\n",
            "Epoch 16325  \tTraining Loss: 0.6916675704123025\tValidation Loss: 0.6916615818803207\n",
            "Epoch 16326  \tTraining Loss: 0.6916675101480136\tValidation Loss: 0.6916615211039624\n",
            "Epoch 16327  \tTraining Loss: 0.6916674498829867\tValidation Loss: 0.6916614603268657\n",
            "Epoch 16328  \tTraining Loss: 0.6916673896172213\tValidation Loss: 0.6916613995490303\n",
            "Epoch 16329  \tTraining Loss: 0.6916673293507175\tValidation Loss: 0.6916613387704564\n",
            "Epoch 16330  \tTraining Loss: 0.6916672690834755\tValidation Loss: 0.6916612779911437\n",
            "Epoch 16331  \tTraining Loss: 0.6916672088154949\tValidation Loss: 0.6916612172110924\n",
            "Epoch 16332  \tTraining Loss: 0.691667148546776\tValidation Loss: 0.6916611564303023\n",
            "Epoch 16333  \tTraining Loss: 0.6916670882773186\tValidation Loss: 0.6916610956487736\n",
            "Epoch 16334  \tTraining Loss: 0.6916670280071229\tValidation Loss: 0.6916610348665059\n",
            "Epoch 16335  \tTraining Loss: 0.6916669677361883\tValidation Loss: 0.6916609740834995\n",
            "Epoch 16336  \tTraining Loss: 0.6916669074645152\tValidation Loss: 0.6916609132997542\n",
            "Epoch 16337  \tTraining Loss: 0.6916668471921036\tValidation Loss: 0.69166085251527\n",
            "Epoch 16338  \tTraining Loss: 0.6916667869189534\tValidation Loss: 0.6916607917300469\n",
            "Epoch 16339  \tTraining Loss: 0.6916667266450646\tValidation Loss: 0.6916607309440848\n",
            "Epoch 16340  \tTraining Loss: 0.6916666663704369\tValidation Loss: 0.6916606701573838\n",
            "Epoch 16341  \tTraining Loss: 0.6916666060950706\tValidation Loss: 0.6916606093699437\n",
            "Epoch 16342  \tTraining Loss: 0.6916665458189655\tValidation Loss: 0.6916605485817645\n",
            "Epoch 16343  \tTraining Loss: 0.6916664855421216\tValidation Loss: 0.6916604877928463\n",
            "Epoch 16344  \tTraining Loss: 0.6916664252645388\tValidation Loss: 0.6916604270031889\n",
            "Epoch 16345  \tTraining Loss: 0.6916663649862174\tValidation Loss: 0.6916603662127925\n",
            "Epoch 16346  \tTraining Loss: 0.6916663047071568\tValidation Loss: 0.6916603054216567\n",
            "Epoch 16347  \tTraining Loss: 0.6916662444273575\tValidation Loss: 0.6916602446297819\n",
            "Epoch 16348  \tTraining Loss: 0.691666184146819\tValidation Loss: 0.6916601838371679\n",
            "Epoch 16349  \tTraining Loss: 0.6916661238655417\tValidation Loss: 0.6916601230438144\n",
            "Epoch 16350  \tTraining Loss: 0.6916660635835253\tValidation Loss: 0.6916600622497217\n",
            "Epoch 16351  \tTraining Loss: 0.69166600330077\tValidation Loss: 0.6916600014548896\n",
            "Epoch 16352  \tTraining Loss: 0.6916659430172755\tValidation Loss: 0.6916599406593182\n",
            "Epoch 16353  \tTraining Loss: 0.6916658827330417\tValidation Loss: 0.6916598798630074\n",
            "Epoch 16354  \tTraining Loss: 0.6916658224480691\tValidation Loss: 0.6916598190659571\n",
            "Epoch 16355  \tTraining Loss: 0.6916657621623571\tValidation Loss: 0.6916597582681673\n",
            "Epoch 16356  \tTraining Loss: 0.6916657018759059\tValidation Loss: 0.6916596974696381\n",
            "Epoch 16357  \tTraining Loss: 0.6916656415887155\tValidation Loss: 0.6916596366703693\n",
            "Epoch 16358  \tTraining Loss: 0.6916655813007857\tValidation Loss: 0.6916595758703609\n",
            "Epoch 16359  \tTraining Loss: 0.6916655210121168\tValidation Loss: 0.6916595150696131\n",
            "Epoch 16360  \tTraining Loss: 0.6916654607227083\tValidation Loss: 0.6916594542681254\n",
            "Epoch 16361  \tTraining Loss: 0.6916654004325605\tValidation Loss: 0.6916593934658982\n",
            "Epoch 16362  \tTraining Loss: 0.6916653401416734\tValidation Loss: 0.6916593326629313\n",
            "Epoch 16363  \tTraining Loss: 0.6916652798500469\tValidation Loss: 0.6916592718592246\n",
            "Epoch 16364  \tTraining Loss: 0.6916652195576807\tValidation Loss: 0.6916592110547781\n",
            "Epoch 16365  \tTraining Loss: 0.691665159264575\tValidation Loss: 0.6916591502495919\n",
            "Epoch 16366  \tTraining Loss: 0.6916650989707298\tValidation Loss: 0.6916590894436659\n",
            "Epoch 16367  \tTraining Loss: 0.6916650386761453\tValidation Loss: 0.691659028637\n",
            "Epoch 16368  \tTraining Loss: 0.6916649783808209\tValidation Loss: 0.6916589678295942\n",
            "Epoch 16369  \tTraining Loss: 0.6916649180847568\tValidation Loss: 0.6916589070214485\n",
            "Epoch 16370  \tTraining Loss: 0.6916648577879532\tValidation Loss: 0.6916588462125628\n",
            "Epoch 16371  \tTraining Loss: 0.6916647974904099\tValidation Loss: 0.6916587854029372\n",
            "Epoch 16372  \tTraining Loss: 0.6916647371921267\tValidation Loss: 0.6916587245925715\n",
            "Epoch 16373  \tTraining Loss: 0.6916646768931038\tValidation Loss: 0.6916586637814659\n",
            "Epoch 16374  \tTraining Loss: 0.6916646165933412\tValidation Loss: 0.69165860296962\n",
            "Epoch 16375  \tTraining Loss: 0.6916645562928386\tValidation Loss: 0.6916585421570342\n",
            "Epoch 16376  \tTraining Loss: 0.6916644959915964\tValidation Loss: 0.6916584813437081\n",
            "Epoch 16377  \tTraining Loss: 0.691664435689614\tValidation Loss: 0.6916584205296419\n",
            "Epoch 16378  \tTraining Loss: 0.6916643753868917\tValidation Loss: 0.6916583597148355\n",
            "Epoch 16379  \tTraining Loss: 0.6916643150834297\tValidation Loss: 0.6916582988992888\n",
            "Epoch 16380  \tTraining Loss: 0.6916642547792274\tValidation Loss: 0.6916582380830019\n",
            "Epoch 16381  \tTraining Loss: 0.6916641944742852\tValidation Loss: 0.6916581772659746\n",
            "Epoch 16382  \tTraining Loss: 0.6916641341686028\tValidation Loss: 0.691658116448207\n",
            "Epoch 16383  \tTraining Loss: 0.6916640738621805\tValidation Loss: 0.691658055629699\n",
            "Epoch 16384  \tTraining Loss: 0.691664013555018\tValidation Loss: 0.6916579948104506\n",
            "Epoch 16385  \tTraining Loss: 0.6916639532471154\tValidation Loss: 0.6916579339904618\n",
            "Epoch 16386  \tTraining Loss: 0.6916638929384725\tValidation Loss: 0.6916578731697325\n",
            "Epoch 16387  \tTraining Loss: 0.6916638326290894\tValidation Loss: 0.6916578123482627\n",
            "Epoch 16388  \tTraining Loss: 0.691663772318966\tValidation Loss: 0.6916577515260526\n",
            "Epoch 16389  \tTraining Loss: 0.6916637120081024\tValidation Loss: 0.6916576907031016\n",
            "Epoch 16390  \tTraining Loss: 0.6916636516964985\tValidation Loss: 0.69165762987941\n",
            "Epoch 16391  \tTraining Loss: 0.6916635913841541\tValidation Loss: 0.6916575690549781\n",
            "Epoch 16392  \tTraining Loss: 0.6916635310710694\tValidation Loss: 0.6916575082298053\n",
            "Epoch 16393  \tTraining Loss: 0.6916634707572443\tValidation Loss: 0.6916574474038918\n",
            "Epoch 16394  \tTraining Loss: 0.6916634104426787\tValidation Loss: 0.6916573865772377\n",
            "Epoch 16395  \tTraining Loss: 0.6916633501273727\tValidation Loss: 0.6916573257498427\n",
            "Epoch 16396  \tTraining Loss: 0.691663289811326\tValidation Loss: 0.691657264921707\n",
            "Epoch 16397  \tTraining Loss: 0.6916632294945388\tValidation Loss: 0.6916572040928305\n",
            "Epoch 16398  \tTraining Loss: 0.6916631691770111\tValidation Loss: 0.691657143263213\n",
            "Epoch 16399  \tTraining Loss: 0.6916631088587428\tValidation Loss: 0.6916570824328548\n",
            "Epoch 16400  \tTraining Loss: 0.6916630485397337\tValidation Loss: 0.6916570216017555\n",
            "Epoch 16401  \tTraining Loss: 0.691662988219984\tValidation Loss: 0.6916569607699153\n",
            "Epoch 16402  \tTraining Loss: 0.6916629278994936\tValidation Loss: 0.6916568999373344\n",
            "Epoch 16403  \tTraining Loss: 0.6916628675782625\tValidation Loss: 0.6916568391040121\n",
            "Epoch 16404  \tTraining Loss: 0.6916628072562905\tValidation Loss: 0.691656778269949\n",
            "Epoch 16405  \tTraining Loss: 0.6916627469335778\tValidation Loss: 0.6916567174351448\n",
            "Epoch 16406  \tTraining Loss: 0.6916626866101242\tValidation Loss: 0.6916566565995993\n",
            "Epoch 16407  \tTraining Loss: 0.6916626262859297\tValidation Loss: 0.6916565957633128\n",
            "Epoch 16408  \tTraining Loss: 0.6916625659609943\tValidation Loss: 0.6916565349262851\n",
            "Epoch 16409  \tTraining Loss: 0.691662505635318\tValidation Loss: 0.6916564740885162\n",
            "Epoch 16410  \tTraining Loss: 0.6916624453089006\tValidation Loss: 0.691656413250006\n",
            "Epoch 16411  \tTraining Loss: 0.6916623849817424\tValidation Loss: 0.6916563524107546\n",
            "Epoch 16412  \tTraining Loss: 0.6916623246538429\tValidation Loss: 0.6916562915707619\n",
            "Epoch 16413  \tTraining Loss: 0.6916622643252025\tValidation Loss: 0.6916562307300278\n",
            "Epoch 16414  \tTraining Loss: 0.6916622039958209\tValidation Loss: 0.6916561698885524\n",
            "Epoch 16415  \tTraining Loss: 0.6916621436656982\tValidation Loss: 0.6916561090463356\n",
            "Epoch 16416  \tTraining Loss: 0.6916620833348344\tValidation Loss: 0.6916560482033773\n",
            "Epoch 16417  \tTraining Loss: 0.6916620230032293\tValidation Loss: 0.6916559873596776\n",
            "Epoch 16418  \tTraining Loss: 0.691661962670883\tValidation Loss: 0.6916559265152364\n",
            "Epoch 16419  \tTraining Loss: 0.6916619023377955\tValidation Loss: 0.6916558656700537\n",
            "Epoch 16420  \tTraining Loss: 0.6916618420039666\tValidation Loss: 0.6916558048241293\n",
            "Epoch 16421  \tTraining Loss: 0.6916617816693963\tValidation Loss: 0.6916557439774634\n",
            "Epoch 16422  \tTraining Loss: 0.6916617213340849\tValidation Loss: 0.6916556831300559\n",
            "Epoch 16423  \tTraining Loss: 0.6916616609980318\tValidation Loss: 0.6916556222819067\n",
            "Epoch 16424  \tTraining Loss: 0.6916616006612374\tValidation Loss: 0.6916555614330158\n",
            "Epoch 16425  \tTraining Loss: 0.6916615403237015\tValidation Loss: 0.6916555005833832\n",
            "Epoch 16426  \tTraining Loss: 0.6916614799854242\tValidation Loss: 0.6916554397330088\n",
            "Epoch 16427  \tTraining Loss: 0.6916614196464053\tValidation Loss: 0.6916553788818927\n",
            "Epoch 16428  \tTraining Loss: 0.6916613593066449\tValidation Loss: 0.6916553180300347\n",
            "Epoch 16429  \tTraining Loss: 0.6916612989661426\tValidation Loss: 0.6916552571774349\n",
            "Epoch 16430  \tTraining Loss: 0.691661238624899\tValidation Loss: 0.6916551963240931\n",
            "Epoch 16431  \tTraining Loss: 0.6916611782829136\tValidation Loss: 0.6916551354700096\n",
            "Epoch 16432  \tTraining Loss: 0.6916611179401865\tValidation Loss: 0.6916550746151839\n",
            "Epoch 16433  \tTraining Loss: 0.6916610575967178\tValidation Loss: 0.6916550137596164\n",
            "Epoch 16434  \tTraining Loss: 0.6916609972525072\tValidation Loss: 0.6916549529033069\n",
            "Epoch 16435  \tTraining Loss: 0.6916609369075549\tValidation Loss: 0.6916548920462552\n",
            "Epoch 16436  \tTraining Loss: 0.6916608765618607\tValidation Loss: 0.6916548311884615\n",
            "Epoch 16437  \tTraining Loss: 0.6916608162154247\tValidation Loss: 0.6916547703299257\n",
            "Epoch 16438  \tTraining Loss: 0.6916607558682468\tValidation Loss: 0.6916547094706478\n",
            "Epoch 16439  \tTraining Loss: 0.691660695520327\tValidation Loss: 0.6916546486106276\n",
            "Epoch 16440  \tTraining Loss: 0.6916606351716653\tValidation Loss: 0.6916545877498653\n",
            "Epoch 16441  \tTraining Loss: 0.6916605748222615\tValidation Loss: 0.6916545268883608\n",
            "Epoch 16442  \tTraining Loss: 0.6916605144721157\tValidation Loss: 0.691654466026114\n",
            "Epoch 16443  \tTraining Loss: 0.6916604541212279\tValidation Loss: 0.6916544051631248\n",
            "Epoch 16444  \tTraining Loss: 0.691660393769598\tValidation Loss: 0.6916543442993934\n",
            "Epoch 16445  \tTraining Loss: 0.691660333417226\tValidation Loss: 0.6916542834349195\n",
            "Epoch 16446  \tTraining Loss: 0.6916602730641117\tValidation Loss: 0.6916542225697033\n",
            "Epoch 16447  \tTraining Loss: 0.6916602127102554\tValidation Loss: 0.6916541617037447\n",
            "Epoch 16448  \tTraining Loss: 0.6916601523556568\tValidation Loss: 0.6916541008370435\n",
            "Epoch 16449  \tTraining Loss: 0.6916600920003161\tValidation Loss: 0.6916540399696001\n",
            "Epoch 16450  \tTraining Loss: 0.6916600316442328\tValidation Loss: 0.6916539791014137\n",
            "Epoch 16451  \tTraining Loss: 0.6916599712874074\tValidation Loss: 0.691653918232485\n",
            "Epoch 16452  \tTraining Loss: 0.6916599109298396\tValidation Loss: 0.6916538573628138\n",
            "Epoch 16453  \tTraining Loss: 0.6916598505715295\tValidation Loss: 0.6916537964923999\n",
            "Epoch 16454  \tTraining Loss: 0.6916597902124768\tValidation Loss: 0.6916537356212433\n",
            "Epoch 16455  \tTraining Loss: 0.6916597298526819\tValidation Loss: 0.6916536747493439\n",
            "Epoch 16456  \tTraining Loss: 0.6916596694921443\tValidation Loss: 0.691653613876702\n",
            "Epoch 16457  \tTraining Loss: 0.6916596091308643\tValidation Loss: 0.6916535530033172\n",
            "Epoch 16458  \tTraining Loss: 0.6916595487688418\tValidation Loss: 0.6916534921291897\n",
            "Epoch 16459  \tTraining Loss: 0.6916594884060767\tValidation Loss: 0.6916534312543194\n",
            "Epoch 16460  \tTraining Loss: 0.691659428042569\tValidation Loss: 0.6916533703787061\n",
            "Epoch 16461  \tTraining Loss: 0.6916593676783187\tValidation Loss: 0.69165330950235\n",
            "Epoch 16462  \tTraining Loss: 0.6916593073133256\tValidation Loss: 0.6916532486252509\n",
            "Epoch 16463  \tTraining Loss: 0.6916592469475898\tValidation Loss: 0.6916531877474091\n",
            "Epoch 16464  \tTraining Loss: 0.6916591865811115\tValidation Loss: 0.6916531268688242\n",
            "Epoch 16465  \tTraining Loss: 0.6916591262138901\tValidation Loss: 0.6916530659894962\n",
            "Epoch 16466  \tTraining Loss: 0.6916590658459262\tValidation Loss: 0.6916530051094253\n",
            "Epoch 16467  \tTraining Loss: 0.6916590054772194\tValidation Loss: 0.6916529442286112\n",
            "Epoch 16468  \tTraining Loss: 0.6916589451077695\tValidation Loss: 0.691652883347054\n",
            "Epoch 16469  \tTraining Loss: 0.6916588847375769\tValidation Loss: 0.6916528224647537\n",
            "Epoch 16470  \tTraining Loss: 0.6916588243666413\tValidation Loss: 0.6916527615817102\n",
            "Epoch 16471  \tTraining Loss: 0.6916587639949627\tValidation Loss: 0.6916527006979236\n",
            "Epoch 16472  \tTraining Loss: 0.6916587036225412\tValidation Loss: 0.6916526398133936\n",
            "Epoch 16473  \tTraining Loss: 0.6916586432493766\tValidation Loss: 0.6916525789281204\n",
            "Epoch 16474  \tTraining Loss: 0.6916585828754689\tValidation Loss: 0.6916525180421038\n",
            "Epoch 16475  \tTraining Loss: 0.6916585225008183\tValidation Loss: 0.6916524571553441\n",
            "Epoch 16476  \tTraining Loss: 0.6916584621254244\tValidation Loss: 0.6916523962678408\n",
            "Epoch 16477  \tTraining Loss: 0.6916584017492874\tValidation Loss: 0.6916523353795941\n",
            "Epoch 16478  \tTraining Loss: 0.6916583413724071\tValidation Loss: 0.6916522744906043\n",
            "Epoch 16479  \tTraining Loss: 0.6916582809947837\tValidation Loss: 0.6916522136008707\n",
            "Epoch 16480  \tTraining Loss: 0.691658220616417\tValidation Loss: 0.6916521527103937\n",
            "Epoch 16481  \tTraining Loss: 0.6916581602373071\tValidation Loss: 0.6916520918191732\n",
            "Epoch 16482  \tTraining Loss: 0.6916580998574536\tValidation Loss: 0.6916520309272091\n",
            "Epoch 16483  \tTraining Loss: 0.691658039476857\tValidation Loss: 0.6916519700345015\n",
            "Epoch 16484  \tTraining Loss: 0.6916579790955169\tValidation Loss: 0.6916519091410502\n",
            "Epoch 16485  \tTraining Loss: 0.6916579187134334\tValidation Loss: 0.6916518482468552\n",
            "Epoch 16486  \tTraining Loss: 0.6916578583306066\tValidation Loss: 0.6916517873519166\n",
            "Epoch 16487  \tTraining Loss: 0.6916577979470361\tValidation Loss: 0.6916517264562343\n",
            "Epoch 16488  \tTraining Loss: 0.6916577375627221\tValidation Loss: 0.6916516655598081\n",
            "Epoch 16489  \tTraining Loss: 0.6916576771776646\tValidation Loss: 0.6916516046626382\n",
            "Epoch 16490  \tTraining Loss: 0.6916576167918635\tValidation Loss: 0.6916515437647245\n",
            "Epoch 16491  \tTraining Loss: 0.6916575564053188\tValidation Loss: 0.6916514828660669\n",
            "Epoch 16492  \tTraining Loss: 0.6916574960180305\tValidation Loss: 0.6916514219666655\n",
            "Epoch 16493  \tTraining Loss: 0.6916574356299984\tValidation Loss: 0.6916513610665203\n",
            "Epoch 16494  \tTraining Loss: 0.6916573752412226\tValidation Loss: 0.6916513001656308\n",
            "Epoch 16495  \tTraining Loss: 0.6916573148517032\tValidation Loss: 0.6916512392639976\n",
            "Epoch 16496  \tTraining Loss: 0.6916572544614398\tValidation Loss: 0.6916511783616202\n",
            "Epoch 16497  \tTraining Loss: 0.6916571940704327\tValidation Loss: 0.6916511174584988\n",
            "Epoch 16498  \tTraining Loss: 0.6916571336786818\tValidation Loss: 0.6916510565546334\n",
            "Epoch 16499  \tTraining Loss: 0.6916570732861871\tValidation Loss: 0.6916509956500239\n",
            "Epoch 16500  \tTraining Loss: 0.6916570128929481\tValidation Loss: 0.6916509347446702\n",
            "Epoch 16501  \tTraining Loss: 0.6916569524989655\tValidation Loss: 0.6916508738385724\n",
            "Epoch 16502  \tTraining Loss: 0.691656892104239\tValidation Loss: 0.6916508129317304\n",
            "Epoch 16503  \tTraining Loss: 0.6916568317087682\tValidation Loss: 0.691650752024144\n",
            "Epoch 16504  \tTraining Loss: 0.6916567713125535\tValidation Loss: 0.6916506911158136\n",
            "Epoch 16505  \tTraining Loss: 0.6916567109155947\tValidation Loss: 0.6916506302067387\n",
            "Epoch 16506  \tTraining Loss: 0.6916566505178917\tValidation Loss: 0.6916505692969195\n",
            "Epoch 16507  \tTraining Loss: 0.6916565901194447\tValidation Loss: 0.6916505083863559\n",
            "Epoch 16508  \tTraining Loss: 0.6916565297202536\tValidation Loss: 0.6916504474750481\n",
            "Epoch 16509  \tTraining Loss: 0.6916564693203181\tValidation Loss: 0.6916503865629956\n",
            "Epoch 16510  \tTraining Loss: 0.6916564089196384\tValidation Loss: 0.6916503256501988\n",
            "Epoch 16511  \tTraining Loss: 0.6916563485182144\tValidation Loss: 0.6916502647366574\n",
            "Epoch 16512  \tTraining Loss: 0.6916562881160463\tValidation Loss: 0.6916502038223715\n",
            "Epoch 16513  \tTraining Loss: 0.6916562277131337\tValidation Loss: 0.6916501429073411\n",
            "Epoch 16514  \tTraining Loss: 0.6916561673094768\tValidation Loss: 0.691650081991566\n",
            "Epoch 16515  \tTraining Loss: 0.6916561069050754\tValidation Loss: 0.6916500210750465\n",
            "Epoch 16516  \tTraining Loss: 0.6916560464999297\tValidation Loss: 0.691649960157782\n",
            "Epoch 16517  \tTraining Loss: 0.6916559860940394\tValidation Loss: 0.691649899239773\n",
            "Epoch 16518  \tTraining Loss: 0.6916559256874047\tValidation Loss: 0.6916498383210193\n",
            "Epoch 16519  \tTraining Loss: 0.6916558652800254\tValidation Loss: 0.6916497774015208\n",
            "Epoch 16520  \tTraining Loss: 0.6916558048719016\tValidation Loss: 0.6916497164812776\n",
            "Epoch 16521  \tTraining Loss: 0.6916557444630332\tValidation Loss: 0.6916496555602896\n",
            "Epoch 16522  \tTraining Loss: 0.6916556840534202\tValidation Loss: 0.6916495946385566\n",
            "Epoch 16523  \tTraining Loss: 0.6916556236430624\tValidation Loss: 0.6916495337160788\n",
            "Epoch 16524  \tTraining Loss: 0.69165556323196\tValidation Loss: 0.6916494727928562\n",
            "Epoch 16525  \tTraining Loss: 0.6916555028201129\tValidation Loss: 0.6916494118688884\n",
            "Epoch 16526  \tTraining Loss: 0.6916554424075209\tValidation Loss: 0.6916493509441758\n",
            "Epoch 16527  \tTraining Loss: 0.6916553819941842\tValidation Loss: 0.6916492900187181\n",
            "Epoch 16528  \tTraining Loss: 0.6916553215801027\tValidation Loss: 0.6916492290925154\n",
            "Epoch 16529  \tTraining Loss: 0.6916552611652763\tValidation Loss: 0.6916491681655677\n",
            "Epoch 16530  \tTraining Loss: 0.6916552007497051\tValidation Loss: 0.6916491072378746\n",
            "Epoch 16531  \tTraining Loss: 0.6916551403333889\tValidation Loss: 0.6916490463094367\n",
            "Epoch 16532  \tTraining Loss: 0.6916550799163277\tValidation Loss: 0.6916489853802534\n",
            "Epoch 16533  \tTraining Loss: 0.6916550194985216\tValidation Loss: 0.691648924450325\n",
            "Epoch 16534  \tTraining Loss: 0.6916549590799703\tValidation Loss: 0.6916488635196514\n",
            "Epoch 16535  \tTraining Loss: 0.6916548986606742\tValidation Loss: 0.6916488025882325\n",
            "Epoch 16536  \tTraining Loss: 0.6916548382406328\tValidation Loss: 0.6916487416560682\n",
            "Epoch 16537  \tTraining Loss: 0.6916547778198464\tValidation Loss: 0.6916486807231587\n",
            "Epoch 16538  \tTraining Loss: 0.6916547173983147\tValidation Loss: 0.6916486197895038\n",
            "Epoch 16539  \tTraining Loss: 0.691654656976038\tValidation Loss: 0.6916485588551033\n",
            "Epoch 16540  \tTraining Loss: 0.6916545965530161\tValidation Loss: 0.6916484979199576\n",
            "Epoch 16541  \tTraining Loss: 0.6916545361292489\tValidation Loss: 0.6916484369840664\n",
            "Epoch 16542  \tTraining Loss: 0.6916544757047363\tValidation Loss: 0.6916483760474297\n",
            "Epoch 16543  \tTraining Loss: 0.6916544152794785\tValidation Loss: 0.6916483151100474\n",
            "Epoch 16544  \tTraining Loss: 0.6916543548534753\tValidation Loss: 0.6916482541719197\n",
            "Epoch 16545  \tTraining Loss: 0.6916542944267268\tValidation Loss: 0.6916481932330463\n",
            "Epoch 16546  \tTraining Loss: 0.6916542339992329\tValidation Loss: 0.6916481322934273\n",
            "Epoch 16547  \tTraining Loss: 0.6916541735709933\tValidation Loss: 0.6916480713530625\n",
            "Epoch 16548  \tTraining Loss: 0.6916541131420084\tValidation Loss: 0.691648010411952\n",
            "Epoch 16549  \tTraining Loss: 0.6916540527122781\tValidation Loss: 0.6916479494700959\n",
            "Epoch 16550  \tTraining Loss: 0.691653992281802\tValidation Loss: 0.6916478885274941\n",
            "Epoch 16551  \tTraining Loss: 0.6916539318505806\tValidation Loss: 0.6916478275841466\n",
            "Epoch 16552  \tTraining Loss: 0.6916538714186135\tValidation Loss: 0.6916477666400531\n",
            "Epoch 16553  \tTraining Loss: 0.6916538109859006\tValidation Loss: 0.6916477056952137\n",
            "Epoch 16554  \tTraining Loss: 0.6916537505524422\tValidation Loss: 0.6916476447496286\n",
            "Epoch 16555  \tTraining Loss: 0.691653690118238\tValidation Loss: 0.6916475838032974\n",
            "Epoch 16556  \tTraining Loss: 0.691653629683288\tValidation Loss: 0.6916475228562203\n",
            "Epoch 16557  \tTraining Loss: 0.6916535692475924\tValidation Loss: 0.6916474619083973\n",
            "Epoch 16558  \tTraining Loss: 0.6916535088111508\tValidation Loss: 0.6916474009598282\n",
            "Epoch 16559  \tTraining Loss: 0.6916534483739635\tValidation Loss: 0.6916473400105131\n",
            "Epoch 16560  \tTraining Loss: 0.6916533879360304\tValidation Loss: 0.691647279060452\n",
            "Epoch 16561  \tTraining Loss: 0.6916533274973512\tValidation Loss: 0.6916472181096446\n",
            "Epoch 16562  \tTraining Loss: 0.6916532670579262\tValidation Loss: 0.6916471571580912\n",
            "Epoch 16563  \tTraining Loss: 0.6916532066177552\tValidation Loss: 0.6916470962057916\n",
            "Epoch 16564  \tTraining Loss: 0.6916531461768382\tValidation Loss: 0.6916470352527457\n",
            "Epoch 16565  \tTraining Loss: 0.6916530857351751\tValidation Loss: 0.6916469742989538\n",
            "Epoch 16566  \tTraining Loss: 0.6916530252927661\tValidation Loss: 0.6916469133444154\n",
            "Epoch 16567  \tTraining Loss: 0.6916529648496108\tValidation Loss: 0.6916468523891307\n",
            "Epoch 16568  \tTraining Loss: 0.6916529044057095\tValidation Loss: 0.6916467914330998\n",
            "Epoch 16569  \tTraining Loss: 0.6916528439610619\tValidation Loss: 0.6916467304763225\n",
            "Epoch 16570  \tTraining Loss: 0.6916527835156683\tValidation Loss: 0.6916466695187987\n",
            "Epoch 16571  \tTraining Loss: 0.6916527230695283\tValidation Loss: 0.6916466085605285\n",
            "Epoch 16572  \tTraining Loss: 0.6916526626226421\tValidation Loss: 0.6916465476015118\n",
            "Epoch 16573  \tTraining Loss: 0.6916526021750096\tValidation Loss: 0.6916464866417488\n",
            "Epoch 16574  \tTraining Loss: 0.6916525417266308\tValidation Loss: 0.6916464256812391\n",
            "Epoch 16575  \tTraining Loss: 0.6916524812775056\tValidation Loss: 0.6916463647199829\n",
            "Epoch 16576  \tTraining Loss: 0.6916524208276339\tValidation Loss: 0.69164630375798\n",
            "Epoch 16577  \tTraining Loss: 0.691652360377016\tValidation Loss: 0.6916462427952306\n",
            "Epoch 16578  \tTraining Loss: 0.6916522999256516\tValidation Loss: 0.6916461818317345\n",
            "Epoch 16579  \tTraining Loss: 0.6916522394735406\tValidation Loss: 0.6916461208674917\n",
            "Epoch 16580  \tTraining Loss: 0.6916521790206831\tValidation Loss: 0.6916460599025022\n",
            "Epoch 16581  \tTraining Loss: 0.6916521185670791\tValidation Loss: 0.6916459989367658\n",
            "Epoch 16582  \tTraining Loss: 0.6916520581127285\tValidation Loss: 0.6916459379702827\n",
            "Epoch 16583  \tTraining Loss: 0.6916519976576312\tValidation Loss: 0.6916458770030528\n",
            "Epoch 16584  \tTraining Loss: 0.6916519372017873\tValidation Loss: 0.691645816035076\n",
            "Epoch 16585  \tTraining Loss: 0.6916518767451967\tValidation Loss: 0.6916457550663525\n",
            "Epoch 16586  \tTraining Loss: 0.6916518162878593\tValidation Loss: 0.6916456940968819\n",
            "Epoch 16587  \tTraining Loss: 0.6916517558297752\tValidation Loss: 0.6916456331266644\n",
            "Epoch 16588  \tTraining Loss: 0.6916516953709444\tValidation Loss: 0.6916455721556999\n",
            "Epoch 16589  \tTraining Loss: 0.6916516349113668\tValidation Loss: 0.6916455111839883\n",
            "Epoch 16590  \tTraining Loss: 0.6916515744510422\tValidation Loss: 0.6916454502115299\n",
            "Epoch 16591  \tTraining Loss: 0.6916515139899708\tValidation Loss: 0.6916453892383243\n",
            "Epoch 16592  \tTraining Loss: 0.6916514535281526\tValidation Loss: 0.6916453282643715\n",
            "Epoch 16593  \tTraining Loss: 0.6916513930655872\tValidation Loss: 0.6916452672896716\n",
            "Epoch 16594  \tTraining Loss: 0.6916513326022751\tValidation Loss: 0.6916452063142245\n",
            "Epoch 16595  \tTraining Loss: 0.6916512721382156\tValidation Loss: 0.6916451453380302\n",
            "Epoch 16596  \tTraining Loss: 0.6916512116734094\tValidation Loss: 0.6916450843610887\n",
            "Epoch 16597  \tTraining Loss: 0.691651151207856\tValidation Loss: 0.6916450233834\n",
            "Epoch 16598  \tTraining Loss: 0.6916510907415555\tValidation Loss: 0.6916449624049639\n",
            "Epoch 16599  \tTraining Loss: 0.6916510302745078\tValidation Loss: 0.6916449014257804\n",
            "Epoch 16600  \tTraining Loss: 0.691650969806713\tValidation Loss: 0.6916448404458496\n",
            "Epoch 16601  \tTraining Loss: 0.691650909338171\tValidation Loss: 0.6916447794651713\n",
            "Epoch 16602  \tTraining Loss: 0.6916508488688817\tValidation Loss: 0.6916447184837458\n",
            "Epoch 16603  \tTraining Loss: 0.6916507883988451\tValidation Loss: 0.6916446575015727\n",
            "Epoch 16604  \tTraining Loss: 0.6916507279280613\tValidation Loss: 0.6916445965186521\n",
            "Epoch 16605  \tTraining Loss: 0.69165066745653\tValidation Loss: 0.6916445355349838\n",
            "Epoch 16606  \tTraining Loss: 0.6916506069842514\tValidation Loss: 0.6916444745505682\n",
            "Epoch 16607  \tTraining Loss: 0.6916505465112254\tValidation Loss: 0.6916444135654048\n",
            "Epoch 16608  \tTraining Loss: 0.6916504860374519\tValidation Loss: 0.6916443525794939\n",
            "Epoch 16609  \tTraining Loss: 0.691650425562931\tValidation Loss: 0.6916442915928354\n",
            "Epoch 16610  \tTraining Loss: 0.6916503650876626\tValidation Loss: 0.691644230605429\n",
            "Epoch 16611  \tTraining Loss: 0.6916503046116466\tValidation Loss: 0.6916441696172749\n",
            "Epoch 16612  \tTraining Loss: 0.6916502441348831\tValidation Loss: 0.6916441086283731\n",
            "Epoch 16613  \tTraining Loss: 0.691650183657372\tValidation Loss: 0.6916440476387237\n",
            "Epoch 16614  \tTraining Loss: 0.6916501231791132\tValidation Loss: 0.6916439866483262\n",
            "Epoch 16615  \tTraining Loss: 0.6916500627001068\tValidation Loss: 0.6916439256571809\n",
            "Epoch 16616  \tTraining Loss: 0.6916500022203527\tValidation Loss: 0.6916438646652878\n",
            "Epoch 16617  \tTraining Loss: 0.6916499417398507\tValidation Loss: 0.6916438036726468\n",
            "Epoch 16618  \tTraining Loss: 0.6916498812586012\tValidation Loss: 0.6916437426792578\n",
            "Epoch 16619  \tTraining Loss: 0.6916498207766038\tValidation Loss: 0.6916436816851207\n",
            "Epoch 16620  \tTraining Loss: 0.6916497602938584\tValidation Loss: 0.6916436206902358\n",
            "Epoch 16621  \tTraining Loss: 0.6916496998103653\tValidation Loss: 0.6916435596946027\n",
            "Epoch 16622  \tTraining Loss: 0.6916496393261242\tValidation Loss: 0.6916434986982215\n",
            "Epoch 16623  \tTraining Loss: 0.6916495788411353\tValidation Loss: 0.6916434377010922\n",
            "Epoch 16624  \tTraining Loss: 0.6916495183553983\tValidation Loss: 0.6916433767032147\n",
            "Epoch 16625  \tTraining Loss: 0.6916494578689134\tValidation Loss: 0.6916433157045891\n",
            "Epoch 16626  \tTraining Loss: 0.6916493973816805\tValidation Loss: 0.6916432547052154\n",
            "Epoch 16627  \tTraining Loss: 0.6916493368936995\tValidation Loss: 0.6916431937050933\n",
            "Epoch 16628  \tTraining Loss: 0.6916492764049704\tValidation Loss: 0.6916431327042231\n",
            "Epoch 16629  \tTraining Loss: 0.6916492159154931\tValidation Loss: 0.6916430717026043\n",
            "Epoch 16630  \tTraining Loss: 0.6916491554252678\tValidation Loss: 0.6916430107002374\n",
            "Epoch 16631  \tTraining Loss: 0.6916490949342942\tValidation Loss: 0.691642949697122\n",
            "Epoch 16632  \tTraining Loss: 0.6916490344425724\tValidation Loss: 0.6916428886932582\n",
            "Epoch 16633  \tTraining Loss: 0.6916489739501024\tValidation Loss: 0.691642827688646\n",
            "Epoch 16634  \tTraining Loss: 0.691648913456884\tValidation Loss: 0.6916427666832853\n",
            "Epoch 16635  \tTraining Loss: 0.6916488529629173\tValidation Loss: 0.6916427056771762\n",
            "Epoch 16636  \tTraining Loss: 0.6916487924682022\tValidation Loss: 0.6916426446703184\n",
            "Epoch 16637  \tTraining Loss: 0.6916487319727388\tValidation Loss: 0.6916425836627123\n",
            "Epoch 16638  \tTraining Loss: 0.691648671476527\tValidation Loss: 0.6916425226543573\n",
            "Epoch 16639  \tTraining Loss: 0.6916486109795666\tValidation Loss: 0.6916424616452538\n",
            "Epoch 16640  \tTraining Loss: 0.6916485504818579\tValidation Loss: 0.6916424006354015\n",
            "Epoch 16641  \tTraining Loss: 0.6916484899834006\tValidation Loss: 0.6916423396248007\n",
            "Epoch 16642  \tTraining Loss: 0.6916484294841947\tValidation Loss: 0.6916422786134511\n",
            "Epoch 16643  \tTraining Loss: 0.6916483689842402\tValidation Loss: 0.6916422176013526\n",
            "Epoch 16644  \tTraining Loss: 0.6916483084835372\tValidation Loss: 0.6916421565885056\n",
            "Epoch 16645  \tTraining Loss: 0.6916482479820856\tValidation Loss: 0.6916420955749095\n",
            "Epoch 16646  \tTraining Loss: 0.6916481874798851\tValidation Loss: 0.6916420345605646\n",
            "Epoch 16647  \tTraining Loss: 0.691648126976936\tValidation Loss: 0.691641973545471\n",
            "Epoch 16648  \tTraining Loss: 0.6916480664732383\tValidation Loss: 0.6916419125296284\n",
            "Epoch 16649  \tTraining Loss: 0.6916480059687915\tValidation Loss: 0.6916418515130366\n",
            "Epoch 16650  \tTraining Loss: 0.691647945463596\tValidation Loss: 0.691641790495696\n",
            "Epoch 16651  \tTraining Loss: 0.6916478849576518\tValidation Loss: 0.6916417294776064\n",
            "Epoch 16652  \tTraining Loss: 0.6916478244509586\tValidation Loss: 0.6916416684587676\n",
            "Epoch 16653  \tTraining Loss: 0.6916477639435166\tValidation Loss: 0.6916416074391799\n",
            "Epoch 16654  \tTraining Loss: 0.6916477034353256\tValidation Loss: 0.691641546418843\n",
            "Epoch 16655  \tTraining Loss: 0.6916476429263855\tValidation Loss: 0.691641485397757\n",
            "Epoch 16656  \tTraining Loss: 0.6916475824166965\tValidation Loss: 0.6916414243759217\n",
            "Epoch 16657  \tTraining Loss: 0.6916475219062583\tValidation Loss: 0.6916413633533371\n",
            "Epoch 16658  \tTraining Loss: 0.6916474613950713\tValidation Loss: 0.6916413023300035\n",
            "Epoch 16659  \tTraining Loss: 0.691647400883135\tValidation Loss: 0.6916412413059205\n",
            "Epoch 16660  \tTraining Loss: 0.6916473403704497\tValidation Loss: 0.6916411802810883\n",
            "Epoch 16661  \tTraining Loss: 0.691647279857015\tValidation Loss: 0.6916411192555066\n",
            "Epoch 16662  \tTraining Loss: 0.6916472193428314\tValidation Loss: 0.6916410582291755\n",
            "Epoch 16663  \tTraining Loss: 0.6916471588278983\tValidation Loss: 0.6916409972020952\n",
            "Epoch 16664  \tTraining Loss: 0.6916470983122162\tValidation Loss: 0.6916409361742654\n",
            "Epoch 16665  \tTraining Loss: 0.6916470377957845\tValidation Loss: 0.6916408751456861\n",
            "Epoch 16666  \tTraining Loss: 0.6916469772786036\tValidation Loss: 0.6916408141163572\n",
            "Epoch 16667  \tTraining Loss: 0.6916469167606732\tValidation Loss: 0.6916407530862787\n",
            "Epoch 16668  \tTraining Loss: 0.6916468562419937\tValidation Loss: 0.6916406920554509\n",
            "Epoch 16669  \tTraining Loss: 0.6916467957225646\tValidation Loss: 0.6916406310238733\n",
            "Epoch 16670  \tTraining Loss: 0.691646735202386\tValidation Loss: 0.691640569991546\n",
            "Epoch 16671  \tTraining Loss: 0.6916466746814578\tValidation Loss: 0.6916405089584692\n",
            "Epoch 16672  \tTraining Loss: 0.6916466141597801\tValidation Loss: 0.6916404479246426\n",
            "Epoch 16673  \tTraining Loss: 0.6916465536373531\tValidation Loss: 0.6916403868900662\n",
            "Epoch 16674  \tTraining Loss: 0.6916464931141761\tValidation Loss: 0.6916403258547401\n",
            "Epoch 16675  \tTraining Loss: 0.6916464325902497\tValidation Loss: 0.6916402648186644\n",
            "Epoch 16676  \tTraining Loss: 0.6916463720655737\tValidation Loss: 0.6916402037818387\n",
            "Epoch 16677  \tTraining Loss: 0.691646311540148\tValidation Loss: 0.6916401427442631\n",
            "Epoch 16678  \tTraining Loss: 0.6916462510139724\tValidation Loss: 0.6916400817059376\n",
            "Epoch 16679  \tTraining Loss: 0.6916461904870471\tValidation Loss: 0.6916400206668621\n",
            "Epoch 16680  \tTraining Loss: 0.691646129959372\tValidation Loss: 0.6916399596270367\n",
            "Epoch 16681  \tTraining Loss: 0.6916460694309471\tValidation Loss: 0.6916398985864615\n",
            "Epoch 16682  \tTraining Loss: 0.6916460089017723\tValidation Loss: 0.6916398375451361\n",
            "Epoch 16683  \tTraining Loss: 0.6916459483718477\tValidation Loss: 0.6916397765030606\n",
            "Epoch 16684  \tTraining Loss: 0.6916458878411731\tValidation Loss: 0.6916397154602351\n",
            "Epoch 16685  \tTraining Loss: 0.6916458273097484\tValidation Loss: 0.6916396544166595\n",
            "Epoch 16686  \tTraining Loss: 0.6916457667775741\tValidation Loss: 0.6916395933723337\n",
            "Epoch 16687  \tTraining Loss: 0.6916457062446495\tValidation Loss: 0.6916395323272576\n",
            "Epoch 16688  \tTraining Loss: 0.6916456457109749\tValidation Loss: 0.6916394712814314\n",
            "Epoch 16689  \tTraining Loss: 0.6916455851765502\tValidation Loss: 0.6916394102348551\n",
            "Epoch 16690  \tTraining Loss: 0.6916455246413753\tValidation Loss: 0.6916393491875283\n",
            "Epoch 16691  \tTraining Loss: 0.6916454641054504\tValidation Loss: 0.6916392881394512\n",
            "Epoch 16692  \tTraining Loss: 0.6916454035687752\tValidation Loss: 0.6916392270906238\n",
            "Epoch 16693  \tTraining Loss: 0.6916453430313498\tValidation Loss: 0.6916391660410459\n",
            "Epoch 16694  \tTraining Loss: 0.691645282493174\tValidation Loss: 0.6916391049907178\n",
            "Epoch 16695  \tTraining Loss: 0.6916452219542482\tValidation Loss: 0.6916390439396392\n",
            "Epoch 16696  \tTraining Loss: 0.6916451614145719\tValidation Loss: 0.69163898288781\n",
            "Epoch 16697  \tTraining Loss: 0.6916451008741453\tValidation Loss: 0.6916389218352302\n",
            "Epoch 16698  \tTraining Loss: 0.6916450403329684\tValidation Loss: 0.6916388607819002\n",
            "Epoch 16699  \tTraining Loss: 0.6916449797910409\tValidation Loss: 0.6916387997278194\n",
            "Epoch 16700  \tTraining Loss: 0.6916449192483631\tValidation Loss: 0.691638738672988\n",
            "Epoch 16701  \tTraining Loss: 0.6916448587049348\tValidation Loss: 0.6916386776174059\n",
            "Epoch 16702  \tTraining Loss: 0.6916447981607559\tValidation Loss: 0.6916386165610733\n",
            "Epoch 16703  \tTraining Loss: 0.6916447376158265\tValidation Loss: 0.6916385555039898\n",
            "Epoch 16704  \tTraining Loss: 0.6916446770701464\tValidation Loss: 0.6916384944461557\n",
            "Epoch 16705  \tTraining Loss: 0.6916446165237159\tValidation Loss: 0.6916384333875708\n",
            "Epoch 16706  \tTraining Loss: 0.6916445559765346\tValidation Loss: 0.6916383723282351\n",
            "Epoch 16707  \tTraining Loss: 0.6916444954286026\tValidation Loss: 0.6916383112681485\n",
            "Epoch 16708  \tTraining Loss: 0.6916444348799201\tValidation Loss: 0.6916382502073111\n",
            "Epoch 16709  \tTraining Loss: 0.6916443743304866\tValidation Loss: 0.6916381891457227\n",
            "Epoch 16710  \tTraining Loss: 0.6916443137803024\tValidation Loss: 0.6916381280833835\n",
            "Epoch 16711  \tTraining Loss: 0.6916442532293674\tValidation Loss: 0.6916380670202932\n",
            "Epoch 16712  \tTraining Loss: 0.6916441926776816\tValidation Loss: 0.691638005956452\n",
            "Epoch 16713  \tTraining Loss: 0.6916441321252451\tValidation Loss: 0.6916379448918597\n",
            "Epoch 16714  \tTraining Loss: 0.6916440715720573\tValidation Loss: 0.6916378838265166\n",
            "Epoch 16715  \tTraining Loss: 0.6916440110181189\tValidation Loss: 0.6916378227604221\n",
            "Epoch 16716  \tTraining Loss: 0.6916439504634294\tValidation Loss: 0.6916377616935765\n",
            "Epoch 16717  \tTraining Loss: 0.6916438899079888\tValidation Loss: 0.6916377006259798\n",
            "Epoch 16718  \tTraining Loss: 0.6916438293517972\tValidation Loss: 0.6916376395576319\n",
            "Epoch 16719  \tTraining Loss: 0.6916437687948546\tValidation Loss: 0.6916375784885328\n",
            "Epoch 16720  \tTraining Loss: 0.6916437082371608\tValidation Loss: 0.6916375174186824\n",
            "Epoch 16721  \tTraining Loss: 0.691643647678716\tValidation Loss: 0.6916374563480807\n",
            "Epoch 16722  \tTraining Loss: 0.6916435871195199\tValidation Loss: 0.6916373952767277\n",
            "Epoch 16723  \tTraining Loss: 0.6916435265595726\tValidation Loss: 0.6916373342046235\n",
            "Epoch 16724  \tTraining Loss: 0.6916434659988742\tValidation Loss: 0.6916372731317677\n",
            "Epoch 16725  \tTraining Loss: 0.6916434054374245\tValidation Loss: 0.6916372120581604\n",
            "Epoch 16726  \tTraining Loss: 0.6916433448752234\tValidation Loss: 0.6916371509838019\n",
            "Epoch 16727  \tTraining Loss: 0.691643284312271\tValidation Loss: 0.6916370899086918\n",
            "Epoch 16728  \tTraining Loss: 0.6916432237485671\tValidation Loss: 0.6916370288328302\n",
            "Epoch 16729  \tTraining Loss: 0.691643163184112\tValidation Loss: 0.6916369677562171\n",
            "Epoch 16730  \tTraining Loss: 0.6916431026189054\tValidation Loss: 0.6916369066788524\n",
            "Epoch 16731  \tTraining Loss: 0.6916430420529474\tValidation Loss: 0.6916368456007359\n",
            "Epoch 16732  \tTraining Loss: 0.6916429814862379\tValidation Loss: 0.6916367845218678\n",
            "Epoch 16733  \tTraining Loss: 0.6916429209187767\tValidation Loss: 0.6916367234422481\n",
            "Epoch 16734  \tTraining Loss: 0.691642860350564\tValidation Loss: 0.6916366623618768\n",
            "Epoch 16735  \tTraining Loss: 0.6916427997815997\tValidation Loss: 0.6916366012807538\n",
            "Epoch 16736  \tTraining Loss: 0.6916427392118838\tValidation Loss: 0.6916365401988788\n",
            "Epoch 16737  \tTraining Loss: 0.6916426786414164\tValidation Loss: 0.691636479116252\n",
            "Epoch 16738  \tTraining Loss: 0.6916426180701971\tValidation Loss: 0.6916364180328736\n",
            "Epoch 16739  \tTraining Loss: 0.6916425574982261\tValidation Loss: 0.691636356948743\n",
            "Epoch 16740  \tTraining Loss: 0.6916424969255035\tValidation Loss: 0.6916362958638607\n",
            "Epoch 16741  \tTraining Loss: 0.6916424363520288\tValidation Loss: 0.6916362347782264\n",
            "Epoch 16742  \tTraining Loss: 0.6916423757778026\tValidation Loss: 0.6916361736918402\n",
            "Epoch 16743  \tTraining Loss: 0.6916423152028243\tValidation Loss: 0.6916361126047018\n",
            "Epoch 16744  \tTraining Loss: 0.6916422546270942\tValidation Loss: 0.6916360515168116\n",
            "Epoch 16745  \tTraining Loss: 0.6916421940506121\tValidation Loss: 0.6916359904281693\n",
            "Epoch 16746  \tTraining Loss: 0.6916421334733782\tValidation Loss: 0.6916359293387747\n",
            "Epoch 16747  \tTraining Loss: 0.6916420728953923\tValidation Loss: 0.6916358682486281\n",
            "Epoch 16748  \tTraining Loss: 0.6916420123166542\tValidation Loss: 0.6916358071577294\n",
            "Epoch 16749  \tTraining Loss: 0.6916419517371641\tValidation Loss: 0.6916357460660784\n",
            "Epoch 16750  \tTraining Loss: 0.691641891156922\tValidation Loss: 0.6916356849736752\n",
            "Epoch 16751  \tTraining Loss: 0.6916418305759277\tValidation Loss: 0.6916356238805198\n",
            "Epoch 16752  \tTraining Loss: 0.6916417699941813\tValidation Loss: 0.691635562786612\n",
            "Epoch 16753  \tTraining Loss: 0.6916417094116828\tValidation Loss: 0.6916355016919519\n",
            "Epoch 16754  \tTraining Loss: 0.6916416488284318\tValidation Loss: 0.6916354405965395\n",
            "Epoch 16755  \tTraining Loss: 0.6916415882444288\tValidation Loss: 0.6916353795003746\n",
            "Epoch 16756  \tTraining Loss: 0.6916415276596735\tValidation Loss: 0.6916353184034573\n",
            "Epoch 16757  \tTraining Loss: 0.6916414670741657\tValidation Loss: 0.6916352573057876\n",
            "Epoch 16758  \tTraining Loss: 0.6916414064879058\tValidation Loss: 0.6916351962073654\n",
            "Epoch 16759  \tTraining Loss: 0.6916413459008933\tValidation Loss: 0.6916351351081906\n",
            "Epoch 16760  \tTraining Loss: 0.6916412853131285\tValidation Loss: 0.6916350740082634\n",
            "Epoch 16761  \tTraining Loss: 0.6916412247246112\tValidation Loss: 0.6916350129075833\n",
            "Epoch 16762  \tTraining Loss: 0.6916411641353415\tValidation Loss: 0.691634951806151\n",
            "Epoch 16763  \tTraining Loss: 0.6916411035453193\tValidation Loss: 0.6916348907039658\n",
            "Epoch 16764  \tTraining Loss: 0.6916410429545443\tValidation Loss: 0.691634829601028\n",
            "Epoch 16765  \tTraining Loss: 0.6916409823630171\tValidation Loss: 0.6916347684973374\n",
            "Epoch 16766  \tTraining Loss: 0.691640921770737\tValidation Loss: 0.691634707392894\n",
            "Epoch 16767  \tTraining Loss: 0.6916408611777043\tValidation Loss: 0.691634646287698\n",
            "Epoch 16768  \tTraining Loss: 0.691640800583919\tValidation Loss: 0.691634585181749\n",
            "Epoch 16769  \tTraining Loss: 0.6916407399893809\tValidation Loss: 0.6916345240750473\n",
            "Epoch 16770  \tTraining Loss: 0.6916406793940901\tValidation Loss: 0.6916344629675927\n",
            "Epoch 16771  \tTraining Loss: 0.6916406187980466\tValidation Loss: 0.6916344018593851\n",
            "Epoch 16772  \tTraining Loss: 0.6916405582012503\tValidation Loss: 0.6916343407504246\n",
            "Epoch 16773  \tTraining Loss: 0.6916404976037009\tValidation Loss: 0.691634279640711\n",
            "Epoch 16774  \tTraining Loss: 0.6916404370053988\tValidation Loss: 0.6916342185302446\n",
            "Epoch 16775  \tTraining Loss: 0.6916403764063438\tValidation Loss: 0.691634157419025\n",
            "Epoch 16776  \tTraining Loss: 0.6916403158065358\tValidation Loss: 0.6916340963070525\n",
            "Epoch 16777  \tTraining Loss: 0.691640255205975\tValidation Loss: 0.6916340351943268\n",
            "Epoch 16778  \tTraining Loss: 0.691640194604661\tValidation Loss: 0.6916339740808479\n",
            "Epoch 16779  \tTraining Loss: 0.691640134002594\tValidation Loss: 0.6916339129666158\n",
            "Epoch 16780  \tTraining Loss: 0.691640073399774\tValidation Loss: 0.6916338518516306\n",
            "Epoch 16781  \tTraining Loss: 0.6916400127962008\tValidation Loss: 0.6916337907358923\n",
            "Epoch 16782  \tTraining Loss: 0.6916399521918746\tValidation Loss: 0.6916337296194004\n",
            "Epoch 16783  \tTraining Loss: 0.6916398915867951\tValidation Loss: 0.6916336685021554\n",
            "Epoch 16784  \tTraining Loss: 0.6916398309809626\tValidation Loss: 0.691633607384157\n",
            "Epoch 16785  \tTraining Loss: 0.6916397703743765\tValidation Loss: 0.6916335462654052\n",
            "Epoch 16786  \tTraining Loss: 0.6916397097670374\tValidation Loss: 0.6916334851459002\n",
            "Epoch 16787  \tTraining Loss: 0.6916396491589449\tValidation Loss: 0.6916334240256415\n",
            "Epoch 16788  \tTraining Loss: 0.6916395885500992\tValidation Loss: 0.6916333629046295\n",
            "Epoch 16789  \tTraining Loss: 0.6916395279405\tValidation Loss: 0.691633301782864\n",
            "Epoch 16790  \tTraining Loss: 0.6916394673301474\tValidation Loss: 0.6916332406603448\n",
            "Epoch 16791  \tTraining Loss: 0.6916394067190414\tValidation Loss: 0.6916331795370722\n",
            "Epoch 16792  \tTraining Loss: 0.691639346107182\tValidation Loss: 0.691633118413046\n",
            "Epoch 16793  \tTraining Loss: 0.6916392854945689\tValidation Loss: 0.6916330572882662\n",
            "Epoch 16794  \tTraining Loss: 0.6916392248812024\tValidation Loss: 0.6916329961627325\n",
            "Epoch 16795  \tTraining Loss: 0.6916391642670824\tValidation Loss: 0.6916329350364454\n",
            "Epoch 16796  \tTraining Loss: 0.6916391036522088\tValidation Loss: 0.6916328739094045\n",
            "Epoch 16797  \tTraining Loss: 0.6916390430365816\tValidation Loss: 0.6916328127816098\n",
            "Epoch 16798  \tTraining Loss: 0.6916389824202006\tValidation Loss: 0.6916327516530613\n",
            "Epoch 16799  \tTraining Loss: 0.691638921803066\tValidation Loss: 0.691632690523759\n",
            "Epoch 16800  \tTraining Loss: 0.6916388611851776\tValidation Loss: 0.6916326293937028\n",
            "Epoch 16801  \tTraining Loss: 0.6916388005665355\tValidation Loss: 0.6916325682628929\n",
            "Epoch 16802  \tTraining Loss: 0.6916387399471395\tValidation Loss: 0.6916325071313288\n",
            "Epoch 16803  \tTraining Loss: 0.6916386793269899\tValidation Loss: 0.6916324459990109\n",
            "Epoch 16804  \tTraining Loss: 0.6916386187060862\tValidation Loss: 0.691632384865939\n",
            "Epoch 16805  \tTraining Loss: 0.6916385580844286\tValidation Loss: 0.691632323732113\n",
            "Epoch 16806  \tTraining Loss: 0.6916384974620172\tValidation Loss: 0.6916322625975331\n",
            "Epoch 16807  \tTraining Loss: 0.6916384368388518\tValidation Loss: 0.691632201462199\n",
            "Epoch 16808  \tTraining Loss: 0.6916383762149325\tValidation Loss: 0.6916321403261108\n",
            "Epoch 16809  \tTraining Loss: 0.6916383155902591\tValidation Loss: 0.6916320791892685\n",
            "Epoch 16810  \tTraining Loss: 0.6916382549648317\tValidation Loss: 0.6916320180516721\n",
            "Epoch 16811  \tTraining Loss: 0.69163819433865\tValidation Loss: 0.6916319569133214\n",
            "Epoch 16812  \tTraining Loss: 0.6916381337117143\tValidation Loss: 0.6916318957742165\n",
            "Epoch 16813  \tTraining Loss: 0.6916380730840247\tValidation Loss: 0.6916318346343572\n",
            "Epoch 16814  \tTraining Loss: 0.6916380124555807\tValidation Loss: 0.6916317734937436\n",
            "Epoch 16815  \tTraining Loss: 0.6916379518263824\tValidation Loss: 0.6916317123523756\n",
            "Epoch 16816  \tTraining Loss: 0.6916378911964299\tValidation Loss: 0.6916316512102535\n",
            "Epoch 16817  \tTraining Loss: 0.6916378305657231\tValidation Loss: 0.6916315900673767\n",
            "Epoch 16818  \tTraining Loss: 0.6916377699342621\tValidation Loss: 0.6916315289237456\n",
            "Epoch 16819  \tTraining Loss: 0.6916377093020466\tValidation Loss: 0.69163146777936\n",
            "Epoch 16820  \tTraining Loss: 0.6916376486690768\tValidation Loss: 0.6916314066342201\n",
            "Epoch 16821  \tTraining Loss: 0.6916375880353526\tValidation Loss: 0.6916313454883254\n",
            "Epoch 16822  \tTraining Loss: 0.6916375274008738\tValidation Loss: 0.6916312843416762\n",
            "Epoch 16823  \tTraining Loss: 0.6916374667656406\tValidation Loss: 0.6916312231942725\n",
            "Epoch 16824  \tTraining Loss: 0.691637406129653\tValidation Loss: 0.691631162046114\n",
            "Epoch 16825  \tTraining Loss: 0.6916373454929108\tValidation Loss: 0.6916311008972008\n",
            "Epoch 16826  \tTraining Loss: 0.691637284855414\tValidation Loss: 0.691631039747533\n",
            "Epoch 16827  \tTraining Loss: 0.6916372242171626\tValidation Loss: 0.6916309785971105\n",
            "Epoch 16828  \tTraining Loss: 0.6916371635781565\tValidation Loss: 0.6916309174459333\n",
            "Epoch 16829  \tTraining Loss: 0.6916371029383958\tValidation Loss: 0.6916308562940011\n",
            "Epoch 16830  \tTraining Loss: 0.6916370422978803\tValidation Loss: 0.6916307951413142\n",
            "Epoch 16831  \tTraining Loss: 0.6916369816566101\tValidation Loss: 0.6916307339878724\n",
            "Epoch 16832  \tTraining Loss: 0.6916369210145851\tValidation Loss: 0.6916306728336757\n",
            "Epoch 16833  \tTraining Loss: 0.6916368603718053\tValidation Loss: 0.6916306116787241\n",
            "Epoch 16834  \tTraining Loss: 0.6916367997282706\tValidation Loss: 0.6916305505230176\n",
            "Epoch 16835  \tTraining Loss: 0.6916367390839812\tValidation Loss: 0.6916304893665559\n",
            "Epoch 16836  \tTraining Loss: 0.6916366784389367\tValidation Loss: 0.6916304282093395\n",
            "Epoch 16837  \tTraining Loss: 0.6916366177931373\tValidation Loss: 0.6916303670513677\n",
            "Epoch 16838  \tTraining Loss: 0.6916365571465829\tValidation Loss: 0.691630305892641\n",
            "Epoch 16839  \tTraining Loss: 0.6916364964992737\tValidation Loss: 0.6916302447331591\n",
            "Epoch 16840  \tTraining Loss: 0.6916364358512093\tValidation Loss: 0.6916301835729222\n",
            "Epoch 16841  \tTraining Loss: 0.6916363752023897\tValidation Loss: 0.69163012241193\n",
            "Epoch 16842  \tTraining Loss: 0.6916363145528152\tValidation Loss: 0.6916300612501826\n",
            "Epoch 16843  \tTraining Loss: 0.6916362539024855\tValidation Loss: 0.6916300000876798\n",
            "Epoch 16844  \tTraining Loss: 0.6916361932514006\tValidation Loss: 0.6916299389244218\n",
            "Epoch 16845  \tTraining Loss: 0.6916361325995605\tValidation Loss: 0.6916298777604086\n",
            "Epoch 16846  \tTraining Loss: 0.6916360719469652\tValidation Loss: 0.6916298165956399\n",
            "Epoch 16847  \tTraining Loss: 0.6916360112936146\tValidation Loss: 0.691629755430116\n",
            "Epoch 16848  \tTraining Loss: 0.6916359506395087\tValidation Loss: 0.6916296942638366\n",
            "Epoch 16849  \tTraining Loss: 0.6916358899846474\tValidation Loss: 0.6916296330968016\n",
            "Epoch 16850  \tTraining Loss: 0.6916358293290308\tValidation Loss: 0.6916295719290112\n",
            "Epoch 16851  \tTraining Loss: 0.691635768672659\tValidation Loss: 0.6916295107604653\n",
            "Epoch 16852  \tTraining Loss: 0.6916357080155313\tValidation Loss: 0.691629449591164\n",
            "Epoch 16853  \tTraining Loss: 0.6916356473576485\tValidation Loss: 0.691629388421107\n",
            "Epoch 16854  \tTraining Loss: 0.69163558669901\tValidation Loss: 0.6916293272502945\n",
            "Epoch 16855  \tTraining Loss: 0.6916355260396161\tValidation Loss: 0.6916292660787261\n",
            "Epoch 16856  \tTraining Loss: 0.6916354653794666\tValidation Loss: 0.6916292049064021\n",
            "Epoch 16857  \tTraining Loss: 0.6916354047185616\tValidation Loss: 0.6916291437333226\n",
            "Epoch 16858  \tTraining Loss: 0.6916353440569007\tValidation Loss: 0.6916290825594872\n",
            "Epoch 16859  \tTraining Loss: 0.6916352833944843\tValidation Loss: 0.691629021384896\n",
            "Epoch 16860  \tTraining Loss: 0.6916352227313123\tValidation Loss: 0.691628960209549\n",
            "Epoch 16861  \tTraining Loss: 0.6916351620673845\tValidation Loss: 0.6916288990334463\n",
            "Epoch 16862  \tTraining Loss: 0.6916351014027009\tValidation Loss: 0.6916288378565876\n",
            "Epoch 16863  \tTraining Loss: 0.6916350407372615\tValidation Loss: 0.6916287766789732\n",
            "Epoch 16864  \tTraining Loss: 0.6916349800710662\tValidation Loss: 0.6916287155006026\n",
            "Epoch 16865  \tTraining Loss: 0.6916349194041153\tValidation Loss: 0.6916286543214761\n",
            "Epoch 16866  \tTraining Loss: 0.6916348587364082\tValidation Loss: 0.6916285931415936\n",
            "Epoch 16867  \tTraining Loss: 0.6916347980679451\tValidation Loss: 0.6916285319609551\n",
            "Epoch 16868  \tTraining Loss: 0.6916347373987264\tValidation Loss: 0.6916284707795605\n",
            "Epoch 16869  \tTraining Loss: 0.6916346767287515\tValidation Loss: 0.6916284095974098\n",
            "Epoch 16870  \tTraining Loss: 0.6916346160580207\tValidation Loss: 0.691628348414503\n",
            "Epoch 16871  \tTraining Loss: 0.6916345553865337\tValidation Loss: 0.69162828723084\n",
            "Epoch 16872  \tTraining Loss: 0.6916344947142906\tValidation Loss: 0.6916282260464209\n",
            "Epoch 16873  \tTraining Loss: 0.6916344340412915\tValidation Loss: 0.6916281648612456\n",
            "Epoch 16874  \tTraining Loss: 0.6916343733675362\tValidation Loss: 0.6916281036753138\n",
            "Epoch 16875  \tTraining Loss: 0.6916343126930247\tValidation Loss: 0.6916280424886259\n",
            "Epoch 16876  \tTraining Loss: 0.691634252017757\tValidation Loss: 0.6916279813011815\n",
            "Epoch 16877  \tTraining Loss: 0.6916341913417331\tValidation Loss: 0.6916279201129809\n",
            "Epoch 16878  \tTraining Loss: 0.6916341306649527\tValidation Loss: 0.6916278589240239\n",
            "Epoch 16879  \tTraining Loss: 0.6916340699874162\tValidation Loss: 0.6916277977343105\n",
            "Epoch 16880  \tTraining Loss: 0.6916340093091232\tValidation Loss: 0.6916277365438407\n",
            "Epoch 16881  \tTraining Loss: 0.6916339486300739\tValidation Loss: 0.6916276753526142\n",
            "Epoch 16882  \tTraining Loss: 0.6916338879502681\tValidation Loss: 0.6916276141606312\n",
            "Epoch 16883  \tTraining Loss: 0.691633827269706\tValidation Loss: 0.6916275529678918\n",
            "Epoch 16884  \tTraining Loss: 0.6916337665883873\tValidation Loss: 0.6916274917743956\n",
            "Epoch 16885  \tTraining Loss: 0.691633705906312\tValidation Loss: 0.6916274305801431\n",
            "Epoch 16886  \tTraining Loss: 0.6916336452234803\tValidation Loss: 0.6916273693851337\n",
            "Epoch 16887  \tTraining Loss: 0.691633584539892\tValidation Loss: 0.6916273081893677\n",
            "Epoch 16888  \tTraining Loss: 0.6916335238555469\tValidation Loss: 0.691627246992845\n",
            "Epoch 16889  \tTraining Loss: 0.6916334631704454\tValidation Loss: 0.6916271857955654\n",
            "Epoch 16890  \tTraining Loss: 0.691633402484587\tValidation Loss: 0.6916271245975292\n",
            "Epoch 16891  \tTraining Loss: 0.6916333417979721\tValidation Loss: 0.691627063398736\n",
            "Epoch 16892  \tTraining Loss: 0.6916332811106003\tValidation Loss: 0.6916270021991862\n",
            "Epoch 16893  \tTraining Loss: 0.6916332204224718\tValidation Loss: 0.6916269409988793\n",
            "Epoch 16894  \tTraining Loss: 0.6916331597335864\tValidation Loss: 0.6916268797978157\n",
            "Epoch 16895  \tTraining Loss: 0.6916330990439443\tValidation Loss: 0.6916268185959951\n",
            "Epoch 16896  \tTraining Loss: 0.6916330383535452\tValidation Loss: 0.6916267573934173\n",
            "Epoch 16897  \tTraining Loss: 0.6916329776623892\tValidation Loss: 0.6916266961900827\n",
            "Epoch 16898  \tTraining Loss: 0.6916329169704762\tValidation Loss: 0.6916266349859909\n",
            "Epoch 16899  \tTraining Loss: 0.6916328562778062\tValidation Loss: 0.6916265737811421\n",
            "Epoch 16900  \tTraining Loss: 0.6916327955843794\tValidation Loss: 0.6916265125755363\n",
            "Epoch 16901  \tTraining Loss: 0.6916327348901954\tValidation Loss: 0.6916264513691732\n",
            "Epoch 16902  \tTraining Loss: 0.6916326741952544\tValidation Loss: 0.6916263901620531\n",
            "Epoch 16903  \tTraining Loss: 0.6916326134995562\tValidation Loss: 0.6916263289541756\n",
            "Epoch 16904  \tTraining Loss: 0.6916325528031009\tValidation Loss: 0.6916262677455409\n",
            "Epoch 16905  \tTraining Loss: 0.6916324921058884\tValidation Loss: 0.691626206536149\n",
            "Epoch 16906  \tTraining Loss: 0.6916324314079187\tValidation Loss: 0.6916261453259998\n",
            "Epoch 16907  \tTraining Loss: 0.6916323707091918\tValidation Loss: 0.6916260841150933\n",
            "Epoch 16908  \tTraining Loss: 0.6916323100097076\tValidation Loss: 0.6916260229034293\n",
            "Epoch 16909  \tTraining Loss: 0.6916322493094661\tValidation Loss: 0.6916259616910081\n",
            "Epoch 16910  \tTraining Loss: 0.6916321886084673\tValidation Loss: 0.6916259004778292\n",
            "Epoch 16911  \tTraining Loss: 0.691632127906711\tValidation Loss: 0.6916258392638931\n",
            "Epoch 16912  \tTraining Loss: 0.6916320672041973\tValidation Loss: 0.6916257780491993\n",
            "Epoch 16913  \tTraining Loss: 0.6916320065009264\tValidation Loss: 0.691625716833748\n",
            "Epoch 16914  \tTraining Loss: 0.6916319457968978\tValidation Loss: 0.6916256556175393\n",
            "Epoch 16915  \tTraining Loss: 0.6916318850921118\tValidation Loss: 0.6916255944005728\n",
            "Epoch 16916  \tTraining Loss: 0.6916318243865681\tValidation Loss: 0.6916255331828487\n",
            "Epoch 16917  \tTraining Loss: 0.6916317636802669\tValidation Loss: 0.6916254719643671\n",
            "Epoch 16918  \tTraining Loss: 0.6916317029732082\tValidation Loss: 0.6916254107451276\n",
            "Epoch 16919  \tTraining Loss: 0.6916316422653919\tValidation Loss: 0.6916253495251304\n",
            "Epoch 16920  \tTraining Loss: 0.6916315815568178\tValidation Loss: 0.6916252883043755\n",
            "Epoch 16921  \tTraining Loss: 0.691631520847486\tValidation Loss: 0.6916252270828629\n",
            "Epoch 16922  \tTraining Loss: 0.6916314601373965\tValidation Loss: 0.6916251658605924\n",
            "Epoch 16923  \tTraining Loss: 0.6916313994265493\tValidation Loss: 0.691625104637564\n",
            "Epoch 16924  \tTraining Loss: 0.6916313387149442\tValidation Loss: 0.6916250434137778\n",
            "Epoch 16925  \tTraining Loss: 0.6916312780025814\tValidation Loss: 0.6916249821892336\n",
            "Epoch 16926  \tTraining Loss: 0.6916312172894606\tValidation Loss: 0.6916249209639315\n",
            "Epoch 16927  \tTraining Loss: 0.6916311565755819\tValidation Loss: 0.6916248597378714\n",
            "Epoch 16928  \tTraining Loss: 0.6916310958609453\tValidation Loss: 0.6916247985110532\n",
            "Epoch 16929  \tTraining Loss: 0.6916310351455508\tValidation Loss: 0.6916247372834771\n",
            "Epoch 16930  \tTraining Loss: 0.6916309744293984\tValidation Loss: 0.6916246760551428\n",
            "Epoch 16931  \tTraining Loss: 0.6916309137124877\tValidation Loss: 0.6916246148260503\n",
            "Epoch 16932  \tTraining Loss: 0.6916308529948191\tValidation Loss: 0.6916245535961998\n",
            "Epoch 16933  \tTraining Loss: 0.6916307922763922\tValidation Loss: 0.6916244923655911\n",
            "Epoch 16934  \tTraining Loss: 0.6916307315572074\tValidation Loss: 0.6916244311342241\n",
            "Epoch 16935  \tTraining Loss: 0.6916306708372644\tValidation Loss: 0.691624369902099\n",
            "Epoch 16936  \tTraining Loss: 0.6916306101165632\tValidation Loss: 0.6916243086692153\n",
            "Epoch 16937  \tTraining Loss: 0.6916305493951037\tValidation Loss: 0.6916242474355736\n",
            "Epoch 16938  \tTraining Loss: 0.691630488672886\tValidation Loss: 0.6916241862011734\n",
            "Epoch 16939  \tTraining Loss: 0.6916304279499099\tValidation Loss: 0.691624124966015\n",
            "Epoch 16940  \tTraining Loss: 0.6916303672261758\tValidation Loss: 0.691624063730098\n",
            "Epoch 16941  \tTraining Loss: 0.691630306501683\tValidation Loss: 0.6916240024934227\n",
            "Epoch 16942  \tTraining Loss: 0.6916302457764318\tValidation Loss: 0.6916239412559888\n",
            "Epoch 16943  \tTraining Loss: 0.6916301850504224\tValidation Loss: 0.6916238800177964\n",
            "Epoch 16944  \tTraining Loss: 0.6916301243236544\tValidation Loss: 0.6916238187788455\n",
            "Epoch 16945  \tTraining Loss: 0.691630063596128\tValidation Loss: 0.6916237575391362\n",
            "Epoch 16946  \tTraining Loss: 0.691630002867843\tValidation Loss: 0.691623696298668\n",
            "Epoch 16947  \tTraining Loss: 0.6916299421387995\tValidation Loss: 0.6916236350574412\n",
            "Epoch 16948  \tTraining Loss: 0.6916298814089974\tValidation Loss: 0.6916235738154559\n",
            "Epoch 16949  \tTraining Loss: 0.6916298206784366\tValidation Loss: 0.6916235125727117\n",
            "Epoch 16950  \tTraining Loss: 0.6916297599471173\tValidation Loss: 0.6916234513292089\n",
            "Epoch 16951  \tTraining Loss: 0.6916296992150391\tValidation Loss: 0.6916233900849473\n",
            "Epoch 16952  \tTraining Loss: 0.6916296384822023\tValidation Loss: 0.6916233288399269\n",
            "Epoch 16953  \tTraining Loss: 0.6916295777486068\tValidation Loss: 0.6916232675941476\n",
            "Epoch 16954  \tTraining Loss: 0.6916295170142523\tValidation Loss: 0.6916232063476095\n",
            "Epoch 16955  \tTraining Loss: 0.6916294562791392\tValidation Loss: 0.6916231451003126\n",
            "Epoch 16956  \tTraining Loss: 0.6916293955432672\tValidation Loss: 0.6916230838522566\n",
            "Epoch 16957  \tTraining Loss: 0.6916293348066362\tValidation Loss: 0.6916230226034417\n",
            "Epoch 16958  \tTraining Loss: 0.6916292740692465\tValidation Loss: 0.6916229613538678\n",
            "Epoch 16959  \tTraining Loss: 0.6916292133310976\tValidation Loss: 0.6916229001035349\n",
            "Epoch 16960  \tTraining Loss: 0.6916291525921899\tValidation Loss: 0.6916228388524428\n",
            "Epoch 16961  \tTraining Loss: 0.6916290918525232\tValidation Loss: 0.6916227776005918\n",
            "Epoch 16962  \tTraining Loss: 0.6916290311120973\tValidation Loss: 0.6916227163479814\n",
            "Epoch 16963  \tTraining Loss: 0.6916289703709124\tValidation Loss: 0.6916226550946121\n",
            "Epoch 16964  \tTraining Loss: 0.6916289096289683\tValidation Loss: 0.6916225938404835\n",
            "Epoch 16965  \tTraining Loss: 0.6916288488862651\tValidation Loss: 0.6916225325855957\n",
            "Epoch 16966  \tTraining Loss: 0.6916287881428028\tValidation Loss: 0.6916224713299486\n",
            "Epoch 16967  \tTraining Loss: 0.6916287273985812\tValidation Loss: 0.6916224100735423\n",
            "Epoch 16968  \tTraining Loss: 0.6916286666536003\tValidation Loss: 0.6916223488163765\n",
            "Epoch 16969  \tTraining Loss: 0.6916286059078602\tValidation Loss: 0.6916222875584516\n",
            "Epoch 16970  \tTraining Loss: 0.6916285451613609\tValidation Loss: 0.691622226299767\n",
            "Epoch 16971  \tTraining Loss: 0.6916284844141022\tValidation Loss: 0.6916221650403231\n",
            "Epoch 16972  \tTraining Loss: 0.691628423666084\tValidation Loss: 0.6916221037801199\n",
            "Epoch 16973  \tTraining Loss: 0.6916283629173063\tValidation Loss: 0.6916220425191572\n",
            "Epoch 16974  \tTraining Loss: 0.6916283021677695\tValidation Loss: 0.6916219812574348\n",
            "Epoch 16975  \tTraining Loss: 0.691628241417473\tValidation Loss: 0.6916219199949528\n",
            "Epoch 16976  \tTraining Loss: 0.691628180666417\tValidation Loss: 0.6916218587317114\n",
            "Epoch 16977  \tTraining Loss: 0.6916281199146015\tValidation Loss: 0.6916217974677105\n",
            "Epoch 16978  \tTraining Loss: 0.6916280591620264\tValidation Loss: 0.6916217362029495\n",
            "Epoch 16979  \tTraining Loss: 0.6916279984086917\tValidation Loss: 0.6916216749374292\n",
            "Epoch 16980  \tTraining Loss: 0.6916279376545973\tValidation Loss: 0.691621613671149\n",
            "Epoch 16981  \tTraining Loss: 0.6916278768997434\tValidation Loss: 0.6916215524041093\n",
            "Epoch 16982  \tTraining Loss: 0.6916278161441295\tValidation Loss: 0.6916214911363096\n",
            "Epoch 16983  \tTraining Loss: 0.6916277553877561\tValidation Loss: 0.6916214298677501\n",
            "Epoch 16984  \tTraining Loss: 0.6916276946306229\tValidation Loss: 0.6916213685984307\n",
            "Epoch 16985  \tTraining Loss: 0.6916276338727297\tValidation Loss: 0.6916213073283516\n",
            "Epoch 16986  \tTraining Loss: 0.6916275731140769\tValidation Loss: 0.6916212460575124\n",
            "Epoch 16987  \tTraining Loss: 0.6916275123546641\tValidation Loss: 0.6916211847859135\n",
            "Epoch 16988  \tTraining Loss: 0.6916274515944913\tValidation Loss: 0.6916211235135544\n",
            "Epoch 16989  \tTraining Loss: 0.6916273908335588\tValidation Loss: 0.6916210622404354\n",
            "Epoch 16990  \tTraining Loss: 0.6916273300718662\tValidation Loss: 0.6916210009665562\n",
            "Epoch 16991  \tTraining Loss: 0.6916272693094137\tValidation Loss: 0.6916209396919171\n",
            "Epoch 16992  \tTraining Loss: 0.6916272085462011\tValidation Loss: 0.6916208784165179\n",
            "Epoch 16993  \tTraining Loss: 0.6916271477822283\tValidation Loss: 0.6916208171403585\n",
            "Epoch 16994  \tTraining Loss: 0.6916270870174954\tValidation Loss: 0.6916207558634389\n",
            "Epoch 16995  \tTraining Loss: 0.6916270262520027\tValidation Loss: 0.691620694585759\n",
            "Epoch 16996  \tTraining Loss: 0.6916269654857495\tValidation Loss: 0.6916206333073189\n",
            "Epoch 16997  \tTraining Loss: 0.6916269047187362\tValidation Loss: 0.6916205720281187\n",
            "Epoch 16998  \tTraining Loss: 0.6916268439509627\tValidation Loss: 0.6916205107481582\n",
            "Epoch 16999  \tTraining Loss: 0.6916267831824289\tValidation Loss: 0.6916204494674372\n",
            "Epoch 17000  \tTraining Loss: 0.6916267224131347\tValidation Loss: 0.6916203881859558\n",
            "Epoch 17001  \tTraining Loss: 0.6916266616430803\tValidation Loss: 0.6916203269037142\n",
            "Epoch 17002  \tTraining Loss: 0.6916266008722656\tValidation Loss: 0.6916202656207121\n",
            "Epoch 17003  \tTraining Loss: 0.6916265401006902\tValidation Loss: 0.6916202043369495\n",
            "Epoch 17004  \tTraining Loss: 0.6916264793283546\tValidation Loss: 0.6916201430524264\n",
            "Epoch 17005  \tTraining Loss: 0.6916264185552586\tValidation Loss: 0.6916200817671427\n",
            "Epoch 17006  \tTraining Loss: 0.6916263577814019\tValidation Loss: 0.6916200204810985\n",
            "Epoch 17007  \tTraining Loss: 0.6916262970067849\tValidation Loss: 0.6916199591942936\n",
            "Epoch 17008  \tTraining Loss: 0.6916262362314072\tValidation Loss: 0.6916198979067283\n",
            "Epoch 17009  \tTraining Loss: 0.6916261754552688\tValidation Loss: 0.6916198366184022\n",
            "Epoch 17010  \tTraining Loss: 0.6916261146783699\tValidation Loss: 0.6916197753293154\n",
            "Epoch 17011  \tTraining Loss: 0.6916260539007104\tValidation Loss: 0.6916197140394679\n",
            "Epoch 17012  \tTraining Loss: 0.69162599312229\tValidation Loss: 0.6916196527488596\n",
            "Epoch 17013  \tTraining Loss: 0.6916259323431091\tValidation Loss: 0.6916195914574905\n",
            "Epoch 17014  \tTraining Loss: 0.6916258715631673\tValidation Loss: 0.6916195301653607\n",
            "Epoch 17015  \tTraining Loss: 0.6916258107824648\tValidation Loss: 0.69161946887247\n",
            "Epoch 17016  \tTraining Loss: 0.6916257500010013\tValidation Loss: 0.6916194075788183\n",
            "Epoch 17017  \tTraining Loss: 0.6916256892187772\tValidation Loss: 0.6916193462844057\n",
            "Epoch 17018  \tTraining Loss: 0.691625628435792\tValidation Loss: 0.6916192849892322\n",
            "Epoch 17019  \tTraining Loss: 0.691625567652046\tValidation Loss: 0.6916192236932978\n",
            "Epoch 17020  \tTraining Loss: 0.6916255068675389\tValidation Loss: 0.6916191623966021\n",
            "Epoch 17021  \tTraining Loss: 0.6916254460822708\tValidation Loss: 0.6916191010991456\n",
            "Epoch 17022  \tTraining Loss: 0.6916253852962418\tValidation Loss: 0.6916190398009279\n",
            "Epoch 17023  \tTraining Loss: 0.6916253245094518\tValidation Loss: 0.6916189785019492\n",
            "Epoch 17024  \tTraining Loss: 0.6916252637219005\tValidation Loss: 0.6916189172022091\n",
            "Epoch 17025  \tTraining Loss: 0.6916252029335883\tValidation Loss: 0.691618855901708\n",
            "Epoch 17026  \tTraining Loss: 0.6916251421445148\tValidation Loss: 0.6916187946004456\n",
            "Epoch 17027  \tTraining Loss: 0.6916250813546801\tValidation Loss: 0.691618733298422\n",
            "Epoch 17028  \tTraining Loss: 0.6916250205640841\tValidation Loss: 0.6916186719956372\n",
            "Epoch 17029  \tTraining Loss: 0.6916249597727271\tValidation Loss: 0.691618610692091\n",
            "Epoch 17030  \tTraining Loss: 0.6916248989806086\tValidation Loss: 0.6916185493877834\n",
            "Epoch 17031  \tTraining Loss: 0.6916248381877289\tValidation Loss: 0.6916184880827145\n",
            "Epoch 17032  \tTraining Loss: 0.6916247773940879\tValidation Loss: 0.6916184267768842\n",
            "Epoch 17033  \tTraining Loss: 0.6916247165996854\tValidation Loss: 0.6916183654702924\n",
            "Epoch 17034  \tTraining Loss: 0.6916246558045214\tValidation Loss: 0.6916183041629391\n",
            "Epoch 17035  \tTraining Loss: 0.691624595008596\tValidation Loss: 0.6916182428548244\n",
            "Epoch 17036  \tTraining Loss: 0.6916245342119093\tValidation Loss: 0.6916181815459481\n",
            "Epoch 17037  \tTraining Loss: 0.6916244734144609\tValidation Loss: 0.6916181202363101\n",
            "Epoch 17038  \tTraining Loss: 0.691624412616251\tValidation Loss: 0.6916180589259108\n",
            "Epoch 17039  \tTraining Loss: 0.6916243518172795\tValidation Loss: 0.6916179976147495\n",
            "Epoch 17040  \tTraining Loss: 0.6916242910175464\tValidation Loss: 0.6916179363028268\n",
            "Epoch 17041  \tTraining Loss: 0.6916242302170515\tValidation Loss: 0.6916178749901422\n",
            "Epoch 17042  \tTraining Loss: 0.6916241694157952\tValidation Loss: 0.6916178136766958\n",
            "Epoch 17043  \tTraining Loss: 0.691624108613777\tValidation Loss: 0.6916177523624879\n",
            "Epoch 17044  \tTraining Loss: 0.6916240478109971\tValidation Loss: 0.6916176910475179\n",
            "Epoch 17045  \tTraining Loss: 0.6916239870074553\tValidation Loss: 0.6916176297317863\n",
            "Epoch 17046  \tTraining Loss: 0.6916239262031518\tValidation Loss: 0.6916175684152928\n",
            "Epoch 17047  \tTraining Loss: 0.6916238653980865\tValidation Loss: 0.6916175070980373\n",
            "Epoch 17048  \tTraining Loss: 0.6916238045922593\tValidation Loss: 0.6916174457800198\n",
            "Epoch 17049  \tTraining Loss: 0.69162374378567\tValidation Loss: 0.6916173844612405\n",
            "Epoch 17050  \tTraining Loss: 0.691623682978319\tValidation Loss: 0.6916173231416991\n",
            "Epoch 17051  \tTraining Loss: 0.6916236221702059\tValidation Loss: 0.6916172618213957\n",
            "Epoch 17052  \tTraining Loss: 0.6916235613613309\tValidation Loss: 0.6916172005003303\n",
            "Epoch 17053  \tTraining Loss: 0.6916235005516937\tValidation Loss: 0.6916171391785026\n",
            "Epoch 17054  \tTraining Loss: 0.6916234397412945\tValidation Loss: 0.691617077855913\n",
            "Epoch 17055  \tTraining Loss: 0.6916233789301331\tValidation Loss: 0.6916170165325611\n",
            "Epoch 17056  \tTraining Loss: 0.6916233181182097\tValidation Loss: 0.691616955208447\n",
            "Epoch 17057  \tTraining Loss: 0.6916232573055241\tValidation Loss: 0.6916168938835707\n",
            "Epoch 17058  \tTraining Loss: 0.6916231964920762\tValidation Loss: 0.6916168325579323\n",
            "Epoch 17059  \tTraining Loss: 0.691623135677866\tValidation Loss: 0.6916167712315314\n",
            "Epoch 17060  \tTraining Loss: 0.6916230748628936\tValidation Loss: 0.6916167099043682\n",
            "Epoch 17061  \tTraining Loss: 0.6916230140471591\tValidation Loss: 0.6916166485764427\n",
            "Epoch 17062  \tTraining Loss: 0.6916229532306619\tValidation Loss: 0.6916165872477548\n",
            "Epoch 17063  \tTraining Loss: 0.6916228924134025\tValidation Loss: 0.6916165259183042\n",
            "Epoch 17064  \tTraining Loss: 0.6916228315953807\tValidation Loss: 0.6916164645880915\n",
            "Epoch 17065  \tTraining Loss: 0.6916227707765965\tValidation Loss: 0.6916164032571162\n",
            "Epoch 17066  \tTraining Loss: 0.6916227099570498\tValidation Loss: 0.6916163419253784\n",
            "Epoch 17067  \tTraining Loss: 0.6916226491367404\tValidation Loss: 0.691616280592878\n",
            "Epoch 17068  \tTraining Loss: 0.6916225883156687\tValidation Loss: 0.6916162192596149\n",
            "Epoch 17069  \tTraining Loss: 0.6916225274938342\tValidation Loss: 0.6916161579255893\n",
            "Epoch 17070  \tTraining Loss: 0.6916224666712373\tValidation Loss: 0.6916160965908011\n",
            "Epoch 17071  \tTraining Loss: 0.6916224058478777\tValidation Loss: 0.6916160352552502\n",
            "Epoch 17072  \tTraining Loss: 0.6916223450237555\tValidation Loss: 0.6916159739189365\n",
            "Epoch 17073  \tTraining Loss: 0.6916222841988705\tValidation Loss: 0.69161591258186\n",
            "Epoch 17074  \tTraining Loss: 0.6916222233732228\tValidation Loss: 0.6916158512440208\n",
            "Epoch 17075  \tTraining Loss: 0.6916221625468124\tValidation Loss: 0.6916157899054187\n",
            "Epoch 17076  \tTraining Loss: 0.691622101719639\tValidation Loss: 0.6916157285660539\n",
            "Epoch 17077  \tTraining Loss: 0.6916220408917029\tValidation Loss: 0.6916156672259262\n",
            "Epoch 17078  \tTraining Loss: 0.6916219800630039\tValidation Loss: 0.6916156058850355\n",
            "Epoch 17079  \tTraining Loss: 0.691621919233542\tValidation Loss: 0.6916155445433818\n",
            "Epoch 17080  \tTraining Loss: 0.6916218584033171\tValidation Loss: 0.6916154832009651\n",
            "Epoch 17081  \tTraining Loss: 0.6916217975723293\tValidation Loss: 0.6916154218577855\n",
            "Epoch 17082  \tTraining Loss: 0.6916217367405786\tValidation Loss: 0.6916153605138429\n",
            "Epoch 17083  \tTraining Loss: 0.6916216759080647\tValidation Loss: 0.691615299169137\n",
            "Epoch 17084  \tTraining Loss: 0.6916216150747878\tValidation Loss: 0.6916152378236683\n",
            "Epoch 17085  \tTraining Loss: 0.6916215542407479\tValidation Loss: 0.6916151764774362\n",
            "Epoch 17086  \tTraining Loss: 0.6916214934059447\tValidation Loss: 0.691615115130441\n",
            "Epoch 17087  \tTraining Loss: 0.6916214325703784\tValidation Loss: 0.6916150537826826\n",
            "Epoch 17088  \tTraining Loss: 0.6916213717340489\tValidation Loss: 0.691614992434161\n",
            "Epoch 17089  \tTraining Loss: 0.6916213108969562\tValidation Loss: 0.691614931084876\n",
            "Epoch 17090  \tTraining Loss: 0.6916212500591001\tValidation Loss: 0.6916148697348278\n",
            "Epoch 17091  \tTraining Loss: 0.6916211892204809\tValidation Loss: 0.6916148083840162\n",
            "Epoch 17092  \tTraining Loss: 0.6916211283810982\tValidation Loss: 0.6916147470324413\n",
            "Epoch 17093  \tTraining Loss: 0.6916210675409523\tValidation Loss: 0.6916146856801029\n",
            "Epoch 17094  \tTraining Loss: 0.6916210067000429\tValidation Loss: 0.6916146243270012\n",
            "Epoch 17095  \tTraining Loss: 0.6916209458583701\tValidation Loss: 0.6916145629731358\n",
            "Epoch 17096  \tTraining Loss: 0.6916208850159338\tValidation Loss: 0.6916145016185071\n",
            "Epoch 17097  \tTraining Loss: 0.691620824172734\tValidation Loss: 0.6916144402631147\n",
            "Epoch 17098  \tTraining Loss: 0.6916207633287706\tValidation Loss: 0.6916143789069589\n",
            "Epoch 17099  \tTraining Loss: 0.6916207024840438\tValidation Loss: 0.6916143175500393\n",
            "Epoch 17100  \tTraining Loss: 0.6916206416385533\tValidation Loss: 0.6916142561923563\n",
            "Epoch 17101  \tTraining Loss: 0.6916205807922992\tValidation Loss: 0.6916141948339094\n",
            "Epoch 17102  \tTraining Loss: 0.6916205199452814\tValidation Loss: 0.6916141334746989\n",
            "Epoch 17103  \tTraining Loss: 0.6916204590975\tValidation Loss: 0.6916140721147247\n",
            "Epoch 17104  \tTraining Loss: 0.6916203982489548\tValidation Loss: 0.6916140107539868\n",
            "Epoch 17105  \tTraining Loss: 0.6916203373996459\tValidation Loss: 0.6916139493924848\n",
            "Epoch 17106  \tTraining Loss: 0.6916202765495731\tValidation Loss: 0.6916138880302193\n",
            "Epoch 17107  \tTraining Loss: 0.6916202156987367\tValidation Loss: 0.6916138266671897\n",
            "Epoch 17108  \tTraining Loss: 0.6916201548471361\tValidation Loss: 0.6916137653033962\n",
            "Epoch 17109  \tTraining Loss: 0.6916200939947719\tValidation Loss: 0.691613703938839\n",
            "Epoch 17110  \tTraining Loss: 0.6916200331416437\tValidation Loss: 0.6916136425735177\n",
            "Epoch 17111  \tTraining Loss: 0.6916199722877514\tValidation Loss: 0.6916135812074323\n",
            "Epoch 17112  \tTraining Loss: 0.6916199114330953\tValidation Loss: 0.6916135198405831\n",
            "Epoch 17113  \tTraining Loss: 0.691619850577675\tValidation Loss: 0.6916134584729696\n",
            "Epoch 17114  \tTraining Loss: 0.6916197897214909\tValidation Loss: 0.6916133971045922\n",
            "Epoch 17115  \tTraining Loss: 0.6916197288645424\tValidation Loss: 0.6916133357354506\n",
            "Epoch 17116  \tTraining Loss: 0.69161966800683\tValidation Loss: 0.6916132743655448\n",
            "Epoch 17117  \tTraining Loss: 0.6916196071483534\tValidation Loss: 0.6916132129948749\n",
            "Epoch 17118  \tTraining Loss: 0.6916195462891125\tValidation Loss: 0.6916131516234406\n",
            "Epoch 17119  \tTraining Loss: 0.6916194854291075\tValidation Loss: 0.6916130902512422\n",
            "Epoch 17120  \tTraining Loss: 0.6916194245683382\tValidation Loss: 0.6916130288782795\n",
            "Epoch 17121  \tTraining Loss: 0.6916193637068047\tValidation Loss: 0.6916129675045524\n",
            "Epoch 17122  \tTraining Loss: 0.6916193028445067\tValidation Loss: 0.6916129061300611\n",
            "Epoch 17123  \tTraining Loss: 0.6916192419814444\tValidation Loss: 0.6916128447548052\n",
            "Epoch 17124  \tTraining Loss: 0.6916191811176179\tValidation Loss: 0.6916127833787851\n",
            "Epoch 17125  \tTraining Loss: 0.6916191202530267\tValidation Loss: 0.6916127220020003\n",
            "Epoch 17126  \tTraining Loss: 0.6916190593876713\tValidation Loss: 0.6916126606244513\n",
            "Epoch 17127  \tTraining Loss: 0.6916189985215513\tValidation Loss: 0.6916125992461375\n",
            "Epoch 17128  \tTraining Loss: 0.6916189376546668\tValidation Loss: 0.6916125378670592\n",
            "Epoch 17129  \tTraining Loss: 0.6916188767870176\tValidation Loss: 0.6916124764872165\n",
            "Epoch 17130  \tTraining Loss: 0.691618815918604\tValidation Loss: 0.6916124151066091\n",
            "Epoch 17131  \tTraining Loss: 0.6916187550494257\tValidation Loss: 0.6916123537252369\n",
            "Epoch 17132  \tTraining Loss: 0.6916186941794827\tValidation Loss: 0.6916122923431002\n",
            "Epoch 17133  \tTraining Loss: 0.6916186333087753\tValidation Loss: 0.6916122309601987\n",
            "Epoch 17134  \tTraining Loss: 0.6916185724373028\tValidation Loss: 0.6916121695765325\n",
            "Epoch 17135  \tTraining Loss: 0.6916185115650658\tValidation Loss: 0.6916121081921014\n",
            "Epoch 17136  \tTraining Loss: 0.691618450692064\tValidation Loss: 0.6916120468069056\n",
            "Epoch 17137  \tTraining Loss: 0.6916183898182974\tValidation Loss: 0.6916119854209448\n",
            "Epoch 17138  \tTraining Loss: 0.6916183289437658\tValidation Loss: 0.6916119240342193\n",
            "Epoch 17139  \tTraining Loss: 0.6916182680684694\tValidation Loss: 0.6916118626467289\n",
            "Epoch 17140  \tTraining Loss: 0.6916182071924081\tValidation Loss: 0.6916118012584735\n",
            "Epoch 17141  \tTraining Loss: 0.6916181463155817\tValidation Loss: 0.691611739869453\n",
            "Epoch 17142  \tTraining Loss: 0.6916180854379906\tValidation Loss: 0.6916116784796676\n",
            "Epoch 17143  \tTraining Loss: 0.6916180245596344\tValidation Loss: 0.6916116170891172\n",
            "Epoch 17144  \tTraining Loss: 0.691617963680513\tValidation Loss: 0.6916115556978016\n",
            "Epoch 17145  \tTraining Loss: 0.6916179028006266\tValidation Loss: 0.691611494305721\n",
            "Epoch 17146  \tTraining Loss: 0.6916178419199751\tValidation Loss: 0.6916114329128752\n",
            "Epoch 17147  \tTraining Loss: 0.6916177810385585\tValidation Loss: 0.6916113715192643\n",
            "Epoch 17148  \tTraining Loss: 0.6916177201563767\tValidation Loss: 0.6916113101248881\n",
            "Epoch 17149  \tTraining Loss: 0.6916176592734297\tValidation Loss: 0.6916112487297467\n",
            "Epoch 17150  \tTraining Loss: 0.6916175983897173\tValidation Loss: 0.6916111873338401\n",
            "Epoch 17151  \tTraining Loss: 0.6916175375052399\tValidation Loss: 0.6916111259371681\n",
            "Epoch 17152  \tTraining Loss: 0.691617476619997\tValidation Loss: 0.6916110645397308\n",
            "Epoch 17153  \tTraining Loss: 0.6916174157339887\tValidation Loss: 0.691611003141528\n",
            "Epoch 17154  \tTraining Loss: 0.6916173548472152\tValidation Loss: 0.69161094174256\n",
            "Epoch 17155  \tTraining Loss: 0.6916172939596762\tValidation Loss: 0.6916108803428264\n",
            "Epoch 17156  \tTraining Loss: 0.6916172330713718\tValidation Loss: 0.6916108189423275\n",
            "Epoch 17157  \tTraining Loss: 0.6916171721823019\tValidation Loss: 0.6916107575410629\n",
            "Epoch 17158  \tTraining Loss: 0.6916171112924664\tValidation Loss: 0.6916106961390329\n",
            "Epoch 17159  \tTraining Loss: 0.6916170504018654\tValidation Loss: 0.6916106347362372\n",
            "Epoch 17160  \tTraining Loss: 0.6916169895104989\tValidation Loss: 0.6916105733326761\n",
            "Epoch 17161  \tTraining Loss: 0.6916169286183669\tValidation Loss: 0.6916105119283492\n",
            "Epoch 17162  \tTraining Loss: 0.691616867725469\tValidation Loss: 0.6916104505232568\n",
            "Epoch 17163  \tTraining Loss: 0.6916168068318055\tValidation Loss: 0.6916103891173986\n",
            "Epoch 17164  \tTraining Loss: 0.6916167459373764\tValidation Loss: 0.6916103277107746\n",
            "Epoch 17165  \tTraining Loss: 0.6916166850421815\tValidation Loss: 0.6916102663033848\n",
            "Epoch 17166  \tTraining Loss: 0.6916166241462208\tValidation Loss: 0.6916102048952294\n",
            "Epoch 17167  \tTraining Loss: 0.6916165632494944\tValidation Loss: 0.691610143486308\n",
            "Epoch 17168  \tTraining Loss: 0.691616502352002\tValidation Loss: 0.6916100820766209\n",
            "Epoch 17169  \tTraining Loss: 0.6916164414537439\tValidation Loss: 0.6916100206661677\n",
            "Epoch 17170  \tTraining Loss: 0.6916163805547197\tValidation Loss: 0.6916099592549487\n",
            "Epoch 17171  \tTraining Loss: 0.6916163196549298\tValidation Loss: 0.6916098978429638\n",
            "Epoch 17172  \tTraining Loss: 0.6916162587543736\tValidation Loss: 0.6916098364302129\n",
            "Epoch 17173  \tTraining Loss: 0.6916161978530517\tValidation Loss: 0.6916097750166957\n",
            "Epoch 17174  \tTraining Loss: 0.6916161369509636\tValidation Loss: 0.6916097136024127\n",
            "Epoch 17175  \tTraining Loss: 0.6916160760481095\tValidation Loss: 0.6916096521873637\n",
            "Epoch 17176  \tTraining Loss: 0.6916160151444893\tValidation Loss: 0.6916095907715484\n",
            "Epoch 17177  \tTraining Loss: 0.6916159542401029\tValidation Loss: 0.6916095293549669\n",
            "Epoch 17178  \tTraining Loss: 0.6916158933349505\tValidation Loss: 0.6916094679376192\n",
            "Epoch 17179  \tTraining Loss: 0.6916158324290318\tValidation Loss: 0.6916094065195054\n",
            "Epoch 17180  \tTraining Loss: 0.6916157715223468\tValidation Loss: 0.6916093451006253\n",
            "Epoch 17181  \tTraining Loss: 0.6916157106148955\tValidation Loss: 0.691609283680979\n",
            "Epoch 17182  \tTraining Loss: 0.691615649706678\tValidation Loss: 0.6916092222605663\n",
            "Epoch 17183  \tTraining Loss: 0.6916155887976942\tValidation Loss: 0.6916091608393873\n",
            "Epoch 17184  \tTraining Loss: 0.691615527887944\tValidation Loss: 0.6916090994174418\n",
            "Epoch 17185  \tTraining Loss: 0.6916154669774273\tValidation Loss: 0.69160903799473\n",
            "Epoch 17186  \tTraining Loss: 0.6916154060661442\tValidation Loss: 0.6916089765712516\n",
            "Epoch 17187  \tTraining Loss: 0.6916153451540947\tValidation Loss: 0.6916089151470068\n",
            "Epoch 17188  \tTraining Loss: 0.6916152842412787\tValidation Loss: 0.6916088537219954\n",
            "Epoch 17189  \tTraining Loss: 0.6916152233276962\tValidation Loss: 0.6916087922962175\n",
            "Epoch 17190  \tTraining Loss: 0.6916151624133471\tValidation Loss: 0.6916087308696731\n",
            "Epoch 17191  \tTraining Loss: 0.6916151014982314\tValidation Loss: 0.691608669442362\n",
            "Epoch 17192  \tTraining Loss: 0.691615040582349\tValidation Loss: 0.6916086080142844\n",
            "Epoch 17193  \tTraining Loss: 0.6916149796657\tValidation Loss: 0.6916085465854399\n",
            "Epoch 17194  \tTraining Loss: 0.6916149187482843\tValidation Loss: 0.6916084851558287\n",
            "Epoch 17195  \tTraining Loss: 0.6916148578301018\tValidation Loss: 0.6916084237254508\n",
            "Epoch 17196  \tTraining Loss: 0.6916147969111526\tValidation Loss: 0.6916083622943063\n",
            "Epoch 17197  \tTraining Loss: 0.6916147359914365\tValidation Loss: 0.6916083008623948\n",
            "Epoch 17198  \tTraining Loss: 0.6916146750709538\tValidation Loss: 0.6916082394297165\n",
            "Epoch 17199  \tTraining Loss: 0.691614614149704\tValidation Loss: 0.6916081779962713\n",
            "Epoch 17200  \tTraining Loss: 0.6916145532276875\tValidation Loss: 0.6916081165620592\n",
            "Epoch 17201  \tTraining Loss: 0.6916144923049039\tValidation Loss: 0.6916080551270802\n",
            "Epoch 17202  \tTraining Loss: 0.6916144313813535\tValidation Loss: 0.6916079936913342\n",
            "Epoch 17203  \tTraining Loss: 0.691614370457036\tValidation Loss: 0.6916079322548211\n",
            "Epoch 17204  \tTraining Loss: 0.6916143095319515\tValidation Loss: 0.6916078708175412\n",
            "Epoch 17205  \tTraining Loss: 0.6916142486060999\tValidation Loss: 0.691607809379494\n",
            "Epoch 17206  \tTraining Loss: 0.6916141876794812\tValidation Loss: 0.6916077479406797\n",
            "Epoch 17207  \tTraining Loss: 0.6916141267520955\tValidation Loss: 0.6916076865010983\n",
            "Epoch 17208  \tTraining Loss: 0.6916140658239426\tValidation Loss: 0.6916076250607498\n",
            "Epoch 17209  \tTraining Loss: 0.6916140048950224\tValidation Loss: 0.6916075636196339\n",
            "Epoch 17210  \tTraining Loss: 0.691613943965335\tValidation Loss: 0.6916075021777509\n",
            "Epoch 17211  \tTraining Loss: 0.6916138830348805\tValidation Loss: 0.6916074407351006\n",
            "Epoch 17212  \tTraining Loss: 0.6916138221036585\tValidation Loss: 0.6916073792916831\n",
            "Epoch 17213  \tTraining Loss: 0.6916137611716694\tValidation Loss: 0.6916073178474982\n",
            "Epoch 17214  \tTraining Loss: 0.6916137002389128\tValidation Loss: 0.6916072564025458\n",
            "Epoch 17215  \tTraining Loss: 0.6916136393053887\tValidation Loss: 0.6916071949568262\n",
            "Epoch 17216  \tTraining Loss: 0.6916135783710973\tValidation Loss: 0.6916071335103391\n",
            "Epoch 17217  \tTraining Loss: 0.6916135174360385\tValidation Loss: 0.6916070720630845\n",
            "Epoch 17218  \tTraining Loss: 0.6916134565002121\tValidation Loss: 0.6916070106150624\n",
            "Epoch 17219  \tTraining Loss: 0.6916133955636182\tValidation Loss: 0.6916069491662726\n",
            "Epoch 17220  \tTraining Loss: 0.6916133346262567\tValidation Loss: 0.6916068877167155\n",
            "Epoch 17221  \tTraining Loss: 0.6916132736881277\tValidation Loss: 0.6916068262663907\n",
            "Epoch 17222  \tTraining Loss: 0.691613212749231\tValidation Loss: 0.6916067648152983\n",
            "Epoch 17223  \tTraining Loss: 0.6916131518095667\tValidation Loss: 0.6916067033634382\n",
            "Epoch 17224  \tTraining Loss: 0.6916130908691348\tValidation Loss: 0.6916066419108103\n",
            "Epoch 17225  \tTraining Loss: 0.691613029927935\tValidation Loss: 0.6916065804574147\n",
            "Epoch 17226  \tTraining Loss: 0.6916129689859676\tValidation Loss: 0.6916065190032515\n",
            "Epoch 17227  \tTraining Loss: 0.6916129080432324\tValidation Loss: 0.6916064575483204\n",
            "Epoch 17228  \tTraining Loss: 0.6916128470997294\tValidation Loss: 0.6916063960926214\n",
            "Epoch 17229  \tTraining Loss: 0.6916127861554583\tValidation Loss: 0.6916063346361547\n",
            "Epoch 17230  \tTraining Loss: 0.6916127252104195\tValidation Loss: 0.6916062731789201\n",
            "Epoch 17231  \tTraining Loss: 0.6916126642646128\tValidation Loss: 0.6916062117209175\n",
            "Epoch 17232  \tTraining Loss: 0.6916126033180381\tValidation Loss: 0.6916061502621469\n",
            "Epoch 17233  \tTraining Loss: 0.6916125423706954\tValidation Loss: 0.6916060888026083\n",
            "Epoch 17234  \tTraining Loss: 0.6916124814225847\tValidation Loss: 0.6916060273423017\n",
            "Epoch 17235  \tTraining Loss: 0.6916124204737061\tValidation Loss: 0.6916059658812271\n",
            "Epoch 17236  \tTraining Loss: 0.6916123595240592\tValidation Loss: 0.6916059044193844\n",
            "Epoch 17237  \tTraining Loss: 0.6916122985736443\tValidation Loss: 0.6916058429567735\n",
            "Epoch 17238  \tTraining Loss: 0.691612237622461\tValidation Loss: 0.6916057814933945\n",
            "Epoch 17239  \tTraining Loss: 0.6916121766705099\tValidation Loss: 0.6916057200292473\n",
            "Epoch 17240  \tTraining Loss: 0.6916121157177904\tValidation Loss: 0.6916056585643318\n",
            "Epoch 17241  \tTraining Loss: 0.6916120547643025\tValidation Loss: 0.6916055970986481\n",
            "Epoch 17242  \tTraining Loss: 0.6916119938100466\tValidation Loss: 0.6916055356321962\n",
            "Epoch 17243  \tTraining Loss: 0.6916119328550222\tValidation Loss: 0.6916054741649759\n",
            "Epoch 17244  \tTraining Loss: 0.6916118718992296\tValidation Loss: 0.6916054126969872\n",
            "Epoch 17245  \tTraining Loss: 0.6916118109426684\tValidation Loss: 0.6916053512282302\n",
            "Epoch 17246  \tTraining Loss: 0.6916117499853389\tValidation Loss: 0.6916052897587046\n",
            "Epoch 17247  \tTraining Loss: 0.6916116890272409\tValidation Loss: 0.6916052282884108\n",
            "Epoch 17248  \tTraining Loss: 0.6916116280683746\tValidation Loss: 0.6916051668173483\n",
            "Epoch 17249  \tTraining Loss: 0.6916115671087396\tValidation Loss: 0.6916051053455174\n",
            "Epoch 17250  \tTraining Loss: 0.6916115061483361\tValidation Loss: 0.691605043872918\n",
            "Epoch 17251  \tTraining Loss: 0.6916114451871641\tValidation Loss: 0.6916049823995498\n",
            "Epoch 17252  \tTraining Loss: 0.6916113842252234\tValidation Loss: 0.6916049209254131\n",
            "Epoch 17253  \tTraining Loss: 0.6916113232625141\tValidation Loss: 0.6916048594505079\n",
            "Epoch 17254  \tTraining Loss: 0.691611262299036\tValidation Loss: 0.6916047979748339\n",
            "Epoch 17255  \tTraining Loss: 0.6916112013347893\tValidation Loss: 0.6916047364983912\n",
            "Epoch 17256  \tTraining Loss: 0.6916111403697739\tValidation Loss: 0.6916046750211796\n",
            "Epoch 17257  \tTraining Loss: 0.6916110794039896\tValidation Loss: 0.6916046135431995\n",
            "Epoch 17258  \tTraining Loss: 0.6916110184374366\tValidation Loss: 0.6916045520644504\n",
            "Epoch 17259  \tTraining Loss: 0.6916109574701146\tValidation Loss: 0.6916044905849326\n",
            "Epoch 17260  \tTraining Loss: 0.6916108965020239\tValidation Loss: 0.6916044291046457\n",
            "Epoch 17261  \tTraining Loss: 0.6916108355331642\tValidation Loss: 0.6916043676235899\n",
            "Epoch 17262  \tTraining Loss: 0.6916107745635355\tValidation Loss: 0.6916043061417654\n",
            "Epoch 17263  \tTraining Loss: 0.6916107135931377\tValidation Loss: 0.6916042446591717\n",
            "Epoch 17264  \tTraining Loss: 0.6916106526219713\tValidation Loss: 0.691604183175809\n",
            "Epoch 17265  \tTraining Loss: 0.6916105916500356\tValidation Loss: 0.6916041216916774\n",
            "Epoch 17266  \tTraining Loss: 0.6916105306773308\tValidation Loss: 0.6916040602067767\n",
            "Epoch 17267  \tTraining Loss: 0.6916104697038569\tValidation Loss: 0.6916039987211067\n",
            "Epoch 17268  \tTraining Loss: 0.691610408729614\tValidation Loss: 0.6916039372346677\n",
            "Epoch 17269  \tTraining Loss: 0.6916103477546018\tValidation Loss: 0.6916038757474595\n",
            "Epoch 17270  \tTraining Loss: 0.6916102867788203\tValidation Loss: 0.6916038142594821\n",
            "Epoch 17271  \tTraining Loss: 0.6916102258022697\tValidation Loss: 0.6916037527707356\n",
            "Epoch 17272  \tTraining Loss: 0.6916101648249499\tValidation Loss: 0.6916036912812196\n",
            "Epoch 17273  \tTraining Loss: 0.6916101038468606\tValidation Loss: 0.6916036297909344\n",
            "Epoch 17274  \tTraining Loss: 0.6916100428680021\tValidation Loss: 0.6916035682998798\n",
            "Epoch 17275  \tTraining Loss: 0.6916099818883741\tValidation Loss: 0.6916035068080559\n",
            "Epoch 17276  \tTraining Loss: 0.6916099209079768\tValidation Loss: 0.6916034453154626\n",
            "Epoch 17277  \tTraining Loss: 0.69160985992681\tValidation Loss: 0.6916033838220998\n",
            "Epoch 17278  \tTraining Loss: 0.6916097989448738\tValidation Loss: 0.6916033223279675\n",
            "Epoch 17279  \tTraining Loss: 0.6916097379621681\tValidation Loss: 0.6916032608330659\n",
            "Epoch 17280  \tTraining Loss: 0.6916096769786927\tValidation Loss: 0.6916031993373947\n",
            "Epoch 17281  \tTraining Loss: 0.6916096159944479\tValidation Loss: 0.6916031378409536\n",
            "Epoch 17282  \tTraining Loss: 0.6916095550094333\tValidation Loss: 0.6916030763437433\n",
            "Epoch 17283  \tTraining Loss: 0.6916094940236492\tValidation Loss: 0.6916030148457631\n",
            "Epoch 17284  \tTraining Loss: 0.6916094330370955\tValidation Loss: 0.6916029533470135\n",
            "Epoch 17285  \tTraining Loss: 0.691609372049772\tValidation Loss: 0.691602891847494\n",
            "Epoch 17286  \tTraining Loss: 0.6916093110616788\tValidation Loss: 0.6916028303472048\n",
            "Epoch 17287  \tTraining Loss: 0.6916092500728157\tValidation Loss: 0.6916027688461458\n",
            "Epoch 17288  \tTraining Loss: 0.6916091890831828\tValidation Loss: 0.6916027073443172\n",
            "Epoch 17289  \tTraining Loss: 0.6916091280927803\tValidation Loss: 0.6916026458417185\n",
            "Epoch 17290  \tTraining Loss: 0.6916090671016077\tValidation Loss: 0.69160258433835\n",
            "Epoch 17291  \tTraining Loss: 0.6916090061096654\tValidation Loss: 0.6916025228342118\n",
            "Epoch 17292  \tTraining Loss: 0.6916089451169529\tValidation Loss: 0.6916024613293034\n",
            "Epoch 17293  \tTraining Loss: 0.6916088841234705\tValidation Loss: 0.6916023998236253\n",
            "Epoch 17294  \tTraining Loss: 0.6916088231292183\tValidation Loss: 0.6916023383171769\n",
            "Epoch 17295  \tTraining Loss: 0.691608762134196\tValidation Loss: 0.6916022768099587\n",
            "Epoch 17296  \tTraining Loss: 0.6916087011384034\tValidation Loss: 0.6916022153019704\n",
            "Epoch 17297  \tTraining Loss: 0.691608640141841\tValidation Loss: 0.691602153793212\n",
            "Epoch 17298  \tTraining Loss: 0.6916085791445084\tValidation Loss: 0.6916020922836833\n",
            "Epoch 17299  \tTraining Loss: 0.6916085181464056\tValidation Loss: 0.6916020307733847\n",
            "Epoch 17300  \tTraining Loss: 0.6916084571475324\tValidation Loss: 0.6916019692623158\n",
            "Epoch 17301  \tTraining Loss: 0.6916083961478892\tValidation Loss: 0.6916019077504767\n",
            "Epoch 17302  \tTraining Loss: 0.6916083351474756\tValidation Loss: 0.6916018462378672\n",
            "Epoch 17303  \tTraining Loss: 0.6916082741462918\tValidation Loss: 0.6916017847244876\n",
            "Epoch 17304  \tTraining Loss: 0.6916082131443376\tValidation Loss: 0.6916017232103376\n",
            "Epoch 17305  \tTraining Loss: 0.691608152141613\tValidation Loss: 0.6916016616954174\n",
            "Epoch 17306  \tTraining Loss: 0.691608091138118\tValidation Loss: 0.6916016001797265\n",
            "Epoch 17307  \tTraining Loss: 0.6916080301338525\tValidation Loss: 0.6916015386632653\n",
            "Epoch 17308  \tTraining Loss: 0.6916079691288168\tValidation Loss: 0.6916014771460337\n",
            "Epoch 17309  \tTraining Loss: 0.6916079081230105\tValidation Loss: 0.6916014156280316\n",
            "Epoch 17310  \tTraining Loss: 0.6916078471164335\tValidation Loss: 0.6916013541092589\n",
            "Epoch 17311  \tTraining Loss: 0.6916077861090861\tValidation Loss: 0.6916012925897158\n",
            "Epoch 17312  \tTraining Loss: 0.691607725100968\tValidation Loss: 0.691601231069402\n",
            "Epoch 17313  \tTraining Loss: 0.6916076640920794\tValidation Loss: 0.6916011695483176\n",
            "Epoch 17314  \tTraining Loss: 0.6916076030824201\tValidation Loss: 0.6916011080264625\n",
            "Epoch 17315  \tTraining Loss: 0.6916075420719899\tValidation Loss: 0.6916010465038367\n",
            "Epoch 17316  \tTraining Loss: 0.6916074810607893\tValidation Loss: 0.6916009849804403\n",
            "Epoch 17317  \tTraining Loss: 0.6916074200488177\tValidation Loss: 0.691600923456273\n",
            "Epoch 17318  \tTraining Loss: 0.6916073590360754\tValidation Loss: 0.6916008619313351\n",
            "Epoch 17319  \tTraining Loss: 0.6916072980225622\tValidation Loss: 0.6916008004056263\n",
            "Epoch 17320  \tTraining Loss: 0.6916072370082782\tValidation Loss: 0.6916007388791465\n",
            "Epoch 17321  \tTraining Loss: 0.6916071759932232\tValidation Loss: 0.6916006773518961\n",
            "Epoch 17322  \tTraining Loss: 0.6916071149773975\tValidation Loss: 0.6916006158238746\n",
            "Epoch 17323  \tTraining Loss: 0.6916070539608006\tValidation Loss: 0.6916005542950823\n",
            "Epoch 17324  \tTraining Loss: 0.6916069929434328\tValidation Loss: 0.6916004927655188\n",
            "Epoch 17325  \tTraining Loss: 0.6916069319252939\tValidation Loss: 0.6916004312351844\n",
            "Epoch 17326  \tTraining Loss: 0.691606870906384\tValidation Loss: 0.691600369704079\n",
            "Epoch 17327  \tTraining Loss: 0.6916068098867031\tValidation Loss: 0.6916003081722024\n",
            "Epoch 17328  \tTraining Loss: 0.691606748866251\tValidation Loss: 0.6916002466395548\n",
            "Epoch 17329  \tTraining Loss: 0.6916066878450275\tValidation Loss: 0.691600185106136\n",
            "Epoch 17330  \tTraining Loss: 0.6916066268230331\tValidation Loss: 0.691600123571946\n",
            "Epoch 17331  \tTraining Loss: 0.6916065658002674\tValidation Loss: 0.6916000620369848\n",
            "Epoch 17332  \tTraining Loss: 0.6916065047767305\tValidation Loss: 0.6916000005012524\n",
            "Epoch 17333  \tTraining Loss: 0.6916064437524221\tValidation Loss: 0.6915999389647486\n",
            "Epoch 17334  \tTraining Loss: 0.6916063827273425\tValidation Loss: 0.6915998774274735\n",
            "Epoch 17335  \tTraining Loss: 0.6916063217014916\tValidation Loss: 0.6915998158894272\n",
            "Epoch 17336  \tTraining Loss: 0.6916062606748693\tValidation Loss: 0.6915997543506095\n",
            "Epoch 17337  \tTraining Loss: 0.6916061996474755\tValidation Loss: 0.6915996928110203\n",
            "Epoch 17338  \tTraining Loss: 0.6916061386193103\tValidation Loss: 0.6915996312706598\n",
            "Epoch 17339  \tTraining Loss: 0.6916060775903735\tValidation Loss: 0.6915995697295276\n",
            "Epoch 17340  \tTraining Loss: 0.6916060165606652\tValidation Loss: 0.6915995081876242\n",
            "Epoch 17341  \tTraining Loss: 0.6916059555301854\tValidation Loss: 0.691599446644949\n",
            "Epoch 17342  \tTraining Loss: 0.6916058944989342\tValidation Loss: 0.6915993851015023\n",
            "Epoch 17343  \tTraining Loss: 0.6916058334669111\tValidation Loss: 0.691599323557284\n",
            "Epoch 17344  \tTraining Loss: 0.6916057724341164\tValidation Loss: 0.691599262012294\n",
            "Epoch 17345  \tTraining Loss: 0.69160571140055\tValidation Loss: 0.6915992004665326\n",
            "Epoch 17346  \tTraining Loss: 0.6916056503662119\tValidation Loss: 0.6915991389199991\n",
            "Epoch 17347  \tTraining Loss: 0.691605589331102\tValidation Loss: 0.6915990773726941\n",
            "Epoch 17348  \tTraining Loss: 0.6916055282952204\tValidation Loss: 0.6915990158246172\n",
            "Epoch 17349  \tTraining Loss: 0.691605467258567\tValidation Loss: 0.6915989542757686\n",
            "Epoch 17350  \tTraining Loss: 0.6916054062211416\tValidation Loss: 0.6915988927261482\n",
            "Epoch 17351  \tTraining Loss: 0.6916053451829445\tValidation Loss: 0.6915988311757558\n",
            "Epoch 17352  \tTraining Loss: 0.6916052841439753\tValidation Loss: 0.6915987696245917\n",
            "Epoch 17353  \tTraining Loss: 0.6916052231042341\tValidation Loss: 0.6915987080726554\n",
            "Epoch 17354  \tTraining Loss: 0.691605162063721\tValidation Loss: 0.6915986465199472\n",
            "Epoch 17355  \tTraining Loss: 0.6916051010224359\tValidation Loss: 0.6915985849664672\n",
            "Epoch 17356  \tTraining Loss: 0.6916050399803788\tValidation Loss: 0.691598523412215\n",
            "Epoch 17357  \tTraining Loss: 0.6916049789375495\tValidation Loss: 0.6915984618571908\n",
            "Epoch 17358  \tTraining Loss: 0.6916049178939482\tValidation Loss: 0.6915984003013944\n",
            "Epoch 17359  \tTraining Loss: 0.6916048568495746\tValidation Loss: 0.6915983387448259\n",
            "Epoch 17360  \tTraining Loss: 0.6916047958044289\tValidation Loss: 0.6915982771874852\n",
            "Epoch 17361  \tTraining Loss: 0.691604734758511\tValidation Loss: 0.6915982156293724\n",
            "Epoch 17362  \tTraining Loss: 0.6916046737118208\tValidation Loss: 0.6915981540704874\n",
            "Epoch 17363  \tTraining Loss: 0.6916046126643582\tValidation Loss: 0.69159809251083\n",
            "Epoch 17364  \tTraining Loss: 0.6916045516161236\tValidation Loss: 0.6915980309504004\n",
            "Epoch 17365  \tTraining Loss: 0.6916044905671163\tValidation Loss: 0.6915979693891984\n",
            "Epoch 17366  \tTraining Loss: 0.6916044295173368\tValidation Loss: 0.6915979078272241\n",
            "Epoch 17367  \tTraining Loss: 0.6916043684667849\tValidation Loss: 0.6915978462644774\n",
            "Epoch 17368  \tTraining Loss: 0.6916043074154604\tValidation Loss: 0.6915977847009582\n",
            "Epoch 17369  \tTraining Loss: 0.6916042463633635\tValidation Loss: 0.6915977231366666\n",
            "Epoch 17370  \tTraining Loss: 0.6916041853104942\tValidation Loss: 0.6915976615716025\n",
            "Epoch 17371  \tTraining Loss: 0.6916041242568521\tValidation Loss: 0.6915976000057659\n",
            "Epoch 17372  \tTraining Loss: 0.6916040632024375\tValidation Loss: 0.6915975384391566\n",
            "Epoch 17373  \tTraining Loss: 0.6916040021472505\tValidation Loss: 0.6915974768717748\n",
            "Epoch 17374  \tTraining Loss: 0.6916039410912905\tValidation Loss: 0.6915974153036204\n",
            "Epoch 17375  \tTraining Loss: 0.6916038800345581\tValidation Loss: 0.6915973537346933\n",
            "Epoch 17376  \tTraining Loss: 0.6916038189770528\tValidation Loss: 0.6915972921649937\n",
            "Epoch 17377  \tTraining Loss: 0.6916037579187748\tValidation Loss: 0.6915972305945212\n",
            "Epoch 17378  \tTraining Loss: 0.6916036968597241\tValidation Loss: 0.6915971690232758\n",
            "Epoch 17379  \tTraining Loss: 0.6916036357999005\tValidation Loss: 0.6915971074512578\n",
            "Epoch 17380  \tTraining Loss: 0.691603574739304\tValidation Loss: 0.691597045878467\n",
            "Epoch 17381  \tTraining Loss: 0.6916035136779347\tValidation Loss: 0.6915969843049032\n",
            "Epoch 17382  \tTraining Loss: 0.6916034526157925\tValidation Loss: 0.6915969227305666\n",
            "Epoch 17383  \tTraining Loss: 0.6916033915528772\tValidation Loss: 0.691596861155457\n",
            "Epoch 17384  \tTraining Loss: 0.6916033304891891\tValidation Loss: 0.6915967995795747\n",
            "Epoch 17385  \tTraining Loss: 0.6916032694247279\tValidation Loss: 0.6915967380029191\n",
            "Epoch 17386  \tTraining Loss: 0.6916032083594936\tValidation Loss: 0.6915966764254906\n",
            "Epoch 17387  \tTraining Loss: 0.6916031472934863\tValidation Loss: 0.6915966148472891\n",
            "Epoch 17388  \tTraining Loss: 0.691603086226706\tValidation Loss: 0.6915965532683143\n",
            "Epoch 17389  \tTraining Loss: 0.6916030251591523\tValidation Loss: 0.6915964916885666\n",
            "Epoch 17390  \tTraining Loss: 0.6916029640908256\tValidation Loss: 0.6915964301080458\n",
            "Epoch 17391  \tTraining Loss: 0.6916029030217256\tValidation Loss: 0.6915963685267517\n",
            "Epoch 17392  \tTraining Loss: 0.6916028419518524\tValidation Loss: 0.6915963069446843\n",
            "Epoch 17393  \tTraining Loss: 0.6916027808812059\tValidation Loss: 0.6915962453618437\n",
            "Epoch 17394  \tTraining Loss: 0.6916027198097859\tValidation Loss: 0.6915961837782298\n",
            "Epoch 17395  \tTraining Loss: 0.6916026587375927\tValidation Loss: 0.6915961221938427\n",
            "Epoch 17396  \tTraining Loss: 0.6916025976646263\tValidation Loss: 0.6915960606086823\n",
            "Epoch 17397  \tTraining Loss: 0.6916025365908863\tValidation Loss: 0.6915959990227483\n",
            "Epoch 17398  \tTraining Loss: 0.6916024755163728\tValidation Loss: 0.691595937436041\n",
            "Epoch 17399  \tTraining Loss: 0.6916024144410859\tValidation Loss: 0.6915958758485602\n",
            "Epoch 17400  \tTraining Loss: 0.6916023533650255\tValidation Loss: 0.6915958142603059\n",
            "Epoch 17401  \tTraining Loss: 0.6916022922881917\tValidation Loss: 0.6915957526712782\n",
            "Epoch 17402  \tTraining Loss: 0.6916022312105841\tValidation Loss: 0.6915956910814769\n",
            "Epoch 17403  \tTraining Loss: 0.6916021701322029\tValidation Loss: 0.691595629490902\n",
            "Epoch 17404  \tTraining Loss: 0.6916021090530481\tValidation Loss: 0.6915955678995535\n",
            "Epoch 17405  \tTraining Loss: 0.6916020479731196\tValidation Loss: 0.6915955063074313\n",
            "Epoch 17406  \tTraining Loss: 0.6916019868924175\tValidation Loss: 0.6915954447145354\n",
            "Epoch 17407  \tTraining Loss: 0.6916019258109416\tValidation Loss: 0.6915953831208659\n",
            "Epoch 17408  \tTraining Loss: 0.6916018647286918\tValidation Loss: 0.6915953215264227\n",
            "Epoch 17409  \tTraining Loss: 0.6916018036456683\tValidation Loss: 0.6915952599312056\n",
            "Epoch 17410  \tTraining Loss: 0.6916017425618709\tValidation Loss: 0.6915951983352147\n",
            "Epoch 17411  \tTraining Loss: 0.6916016814772996\tValidation Loss: 0.69159513673845\n",
            "Epoch 17412  \tTraining Loss: 0.6916016203919545\tValidation Loss: 0.6915950751409112\n",
            "Epoch 17413  \tTraining Loss: 0.6916015593058353\tValidation Loss: 0.6915950135425988\n",
            "Epoch 17414  \tTraining Loss: 0.6916014982189423\tValidation Loss: 0.6915949519435123\n",
            "Epoch 17415  \tTraining Loss: 0.6916014371312752\tValidation Loss: 0.6915948903436518\n",
            "Epoch 17416  \tTraining Loss: 0.691601376042834\tValidation Loss: 0.6915948287430175\n",
            "Epoch 17417  \tTraining Loss: 0.6916013149536189\tValidation Loss: 0.691594767141609\n",
            "Epoch 17418  \tTraining Loss: 0.6916012538636296\tValidation Loss: 0.6915947055394264\n",
            "Epoch 17419  \tTraining Loss: 0.6916011927728661\tValidation Loss: 0.6915946439364697\n",
            "Epoch 17420  \tTraining Loss: 0.6916011316813283\tValidation Loss: 0.691594582332739\n",
            "Epoch 17421  \tTraining Loss: 0.6916010705890167\tValidation Loss: 0.691594520728234\n",
            "Epoch 17422  \tTraining Loss: 0.6916010094959305\tValidation Loss: 0.6915944591229547\n",
            "Epoch 17423  \tTraining Loss: 0.6916009484020702\tValidation Loss: 0.6915943975169012\n",
            "Epoch 17424  \tTraining Loss: 0.6916008873074354\tValidation Loss: 0.6915943359100736\n",
            "Epoch 17425  \tTraining Loss: 0.6916008262120265\tValidation Loss: 0.6915942743024717\n",
            "Epoch 17426  \tTraining Loss: 0.691600765115843\tValidation Loss: 0.6915942126940953\n",
            "Epoch 17427  \tTraining Loss: 0.6916007040188852\tValidation Loss: 0.6915941510849446\n",
            "Epoch 17428  \tTraining Loss: 0.6916006429211529\tValidation Loss: 0.6915940894750194\n",
            "Epoch 17429  \tTraining Loss: 0.6916005818226463\tValidation Loss: 0.6915940278643199\n",
            "Epoch 17430  \tTraining Loss: 0.691600520723365\tValidation Loss: 0.6915939662528457\n",
            "Epoch 17431  \tTraining Loss: 0.6916004596233092\tValidation Loss: 0.6915939046405972\n",
            "Epoch 17432  \tTraining Loss: 0.6916003985224788\tValidation Loss: 0.6915938430275743\n",
            "Epoch 17433  \tTraining Loss: 0.6916003374208739\tValidation Loss: 0.6915937814137765\n",
            "Epoch 17434  \tTraining Loss: 0.6916002763184943\tValidation Loss: 0.6915937197992043\n",
            "Epoch 17435  \tTraining Loss: 0.69160021521534\tValidation Loss: 0.6915936581838574\n",
            "Epoch 17436  \tTraining Loss: 0.691600154111411\tValidation Loss: 0.6915935965677359\n",
            "Epoch 17437  \tTraining Loss: 0.6916000930067073\tValidation Loss: 0.6915935349508396\n",
            "Epoch 17438  \tTraining Loss: 0.6916000319012289\tValidation Loss: 0.6915934733331687\n",
            "Epoch 17439  \tTraining Loss: 0.6915999707949755\tValidation Loss: 0.6915934117147229\n",
            "Epoch 17440  \tTraining Loss: 0.6915999096879473\tValidation Loss: 0.6915933500955024\n",
            "Epoch 17441  \tTraining Loss: 0.6915998485801443\tValidation Loss: 0.691593288475507\n",
            "Epoch 17442  \tTraining Loss: 0.6915997874715664\tValidation Loss: 0.6915932268547368\n",
            "Epoch 17443  \tTraining Loss: 0.6915997263622136\tValidation Loss: 0.6915931652331917\n",
            "Epoch 17444  \tTraining Loss: 0.6915996652520857\tValidation Loss: 0.6915931036108716\n",
            "Epoch 17445  \tTraining Loss: 0.6915996041411829\tValidation Loss: 0.6915930419877766\n",
            "Epoch 17446  \tTraining Loss: 0.6915995430295051\tValidation Loss: 0.6915929803639065\n",
            "Epoch 17447  \tTraining Loss: 0.691599481917052\tValidation Loss: 0.6915929187392614\n",
            "Epoch 17448  \tTraining Loss: 0.6915994208038241\tValidation Loss: 0.6915928571138412\n",
            "Epoch 17449  \tTraining Loss: 0.6915993596898208\tValidation Loss: 0.6915927954876461\n",
            "Epoch 17450  \tTraining Loss: 0.6915992985750424\tValidation Loss: 0.6915927338606758\n",
            "Epoch 17451  \tTraining Loss: 0.6915992374594888\tValidation Loss: 0.6915926722329302\n",
            "Epoch 17452  \tTraining Loss: 0.69159917634316\tValidation Loss: 0.6915926106044095\n",
            "Epoch 17453  \tTraining Loss: 0.691599115226056\tValidation Loss: 0.6915925489751136\n",
            "Epoch 17454  \tTraining Loss: 0.6915990541081766\tValidation Loss: 0.6915924873450424\n",
            "Epoch 17455  \tTraining Loss: 0.6915989929895219\tValidation Loss: 0.6915924257141959\n",
            "Epoch 17456  \tTraining Loss: 0.6915989318700918\tValidation Loss: 0.6915923640825741\n",
            "Epoch 17457  \tTraining Loss: 0.6915988707498862\tValidation Loss: 0.6915923024501769\n",
            "Epoch 17458  \tTraining Loss: 0.6915988096289053\tValidation Loss: 0.6915922408170043\n",
            "Epoch 17459  \tTraining Loss: 0.6915987485071489\tValidation Loss: 0.6915921791830563\n",
            "Epoch 17460  \tTraining Loss: 0.6915986873846169\tValidation Loss: 0.6915921175483328\n",
            "Epoch 17461  \tTraining Loss: 0.6915986262613096\tValidation Loss: 0.6915920559128339\n",
            "Epoch 17462  \tTraining Loss: 0.6915985651372265\tValidation Loss: 0.6915919942765594\n",
            "Epoch 17463  \tTraining Loss: 0.6915985040123679\tValidation Loss: 0.6915919326395094\n",
            "Epoch 17464  \tTraining Loss: 0.6915984428867337\tValidation Loss: 0.6915918710016837\n",
            "Epoch 17465  \tTraining Loss: 0.6915983817603237\tValidation Loss: 0.6915918093630825\n",
            "Epoch 17466  \tTraining Loss: 0.6915983206331381\tValidation Loss: 0.6915917477237057\n",
            "Epoch 17467  \tTraining Loss: 0.6915982595051767\tValidation Loss: 0.691591686083553\n",
            "Epoch 17468  \tTraining Loss: 0.6915981983764395\tValidation Loss: 0.6915916244426247\n",
            "Epoch 17469  \tTraining Loss: 0.6915981372469266\tValidation Loss: 0.6915915628009207\n",
            "Epoch 17470  \tTraining Loss: 0.6915980761166378\tValidation Loss: 0.6915915011584408\n",
            "Epoch 17471  \tTraining Loss: 0.6915980149855733\tValidation Loss: 0.6915914395151851\n",
            "Epoch 17472  \tTraining Loss: 0.6915979538537327\tValidation Loss: 0.6915913778711537\n",
            "Epoch 17473  \tTraining Loss: 0.6915978927211164\tValidation Loss: 0.6915913162263462\n",
            "Epoch 17474  \tTraining Loss: 0.6915978315877238\tValidation Loss: 0.6915912545807628\n",
            "Epoch 17475  \tTraining Loss: 0.6915977704535554\tValidation Loss: 0.6915911929344035\n",
            "Epoch 17476  \tTraining Loss: 0.6915977093186109\tValidation Loss: 0.6915911312872682\n",
            "Epoch 17477  \tTraining Loss: 0.6915976481828904\tValidation Loss: 0.6915910696393569\n",
            "Epoch 17478  \tTraining Loss: 0.6915975870463937\tValidation Loss: 0.6915910079906696\n",
            "Epoch 17479  \tTraining Loss: 0.6915975259091208\tValidation Loss: 0.6915909463412061\n",
            "Epoch 17480  \tTraining Loss: 0.691597464771072\tValidation Loss: 0.6915908846909666\n",
            "Epoch 17481  \tTraining Loss: 0.6915974036322468\tValidation Loss: 0.6915908230399508\n",
            "Epoch 17482  \tTraining Loss: 0.6915973424926454\tValidation Loss: 0.6915907613881589\n",
            "Epoch 17483  \tTraining Loss: 0.6915972813522677\tValidation Loss: 0.6915906997355908\n",
            "Epoch 17484  \tTraining Loss: 0.6915972202111138\tValidation Loss: 0.6915906380822464\n",
            "Epoch 17485  \tTraining Loss: 0.6915971590691835\tValidation Loss: 0.6915905764281258\n",
            "Epoch 17486  \tTraining Loss: 0.6915970979264768\tValidation Loss: 0.691590514773229\n",
            "Epoch 17487  \tTraining Loss: 0.6915970367829938\tValidation Loss: 0.6915904531175556\n",
            "Epoch 17488  \tTraining Loss: 0.6915969756387342\tValidation Loss: 0.6915903914611059\n",
            "Epoch 17489  \tTraining Loss: 0.6915969144936983\tValidation Loss: 0.6915903298038797\n",
            "Epoch 17490  \tTraining Loss: 0.691596853347886\tValidation Loss: 0.6915902681458772\n",
            "Epoch 17491  \tTraining Loss: 0.691596792201297\tValidation Loss: 0.6915902064870982\n",
            "Epoch 17492  \tTraining Loss: 0.6915967310539314\tValidation Loss: 0.6915901448275427\n",
            "Epoch 17493  \tTraining Loss: 0.6915966699057893\tValidation Loss: 0.6915900831672106\n",
            "Epoch 17494  \tTraining Loss: 0.6915966087568705\tValidation Loss: 0.6915900215061018\n",
            "Epoch 17495  \tTraining Loss: 0.691596547607175\tValidation Loss: 0.6915899598442166\n",
            "Epoch 17496  \tTraining Loss: 0.6915964864567029\tValidation Loss: 0.6915898981815547\n",
            "Epoch 17497  \tTraining Loss: 0.6915964253054541\tValidation Loss: 0.691589836518116\n",
            "Epoch 17498  \tTraining Loss: 0.6915963641534284\tValidation Loss: 0.6915897748539007\n",
            "Epoch 17499  \tTraining Loss: 0.691596303000626\tValidation Loss: 0.6915897131889086\n",
            "Epoch 17500  \tTraining Loss: 0.6915962418470467\tValidation Loss: 0.6915896515231398\n",
            "Epoch 17501  \tTraining Loss: 0.6915961806926905\tValidation Loss: 0.6915895898565941\n",
            "Epoch 17502  \tTraining Loss: 0.6915961195375575\tValidation Loss: 0.6915895281892717\n",
            "Epoch 17503  \tTraining Loss: 0.6915960583816476\tValidation Loss: 0.6915894665211723\n",
            "Epoch 17504  \tTraining Loss: 0.6915959972249607\tValidation Loss: 0.6915894048522959\n",
            "Epoch 17505  \tTraining Loss: 0.6915959360674967\tValidation Loss: 0.6915893431826428\n",
            "Epoch 17506  \tTraining Loss: 0.6915958749092558\tValidation Loss: 0.6915892815122126\n",
            "Epoch 17507  \tTraining Loss: 0.6915958137502377\tValidation Loss: 0.6915892198410053\n",
            "Epoch 17508  \tTraining Loss: 0.6915957525904427\tValidation Loss: 0.691589158169021\n",
            "Epoch 17509  \tTraining Loss: 0.6915956914298703\tValidation Loss: 0.6915890964962597\n",
            "Epoch 17510  \tTraining Loss: 0.6915956302685209\tValidation Loss: 0.6915890348227213\n",
            "Epoch 17511  \tTraining Loss: 0.6915955691063943\tValidation Loss: 0.6915889731484057\n",
            "Epoch 17512  \tTraining Loss: 0.6915955079434906\tValidation Loss: 0.6915889114733129\n",
            "Epoch 17513  \tTraining Loss: 0.6915954467798093\tValidation Loss: 0.691588849797443\n",
            "Epoch 17514  \tTraining Loss: 0.691595385615351\tValidation Loss: 0.6915887881207957\n",
            "Epoch 17515  \tTraining Loss: 0.6915953244501153\tValidation Loss: 0.6915887264433713\n",
            "Epoch 17516  \tTraining Loss: 0.6915952632841021\tValidation Loss: 0.6915886647651696\n",
            "Epoch 17517  \tTraining Loss: 0.6915952021173116\tValidation Loss: 0.6915886030861904\n",
            "Epoch 17518  \tTraining Loss: 0.6915951409497437\tValidation Loss: 0.6915885414064339\n",
            "Epoch 17519  \tTraining Loss: 0.6915950797813984\tValidation Loss: 0.6915884797258999\n",
            "Epoch 17520  \tTraining Loss: 0.6915950186122756\tValidation Loss: 0.6915884180445886\n",
            "Epoch 17521  \tTraining Loss: 0.6915949574423751\tValidation Loss: 0.6915883563624998\n",
            "Epoch 17522  \tTraining Loss: 0.6915948962716972\tValidation Loss: 0.6915882946796335\n",
            "Epoch 17523  \tTraining Loss: 0.6915948351002416\tValidation Loss: 0.6915882329959896\n",
            "Epoch 17524  \tTraining Loss: 0.6915947739280084\tValidation Loss: 0.6915881713115682\n",
            "Epoch 17525  \tTraining Loss: 0.6915947127549975\tValidation Loss: 0.6915881096263692\n",
            "Epoch 17526  \tTraining Loss: 0.6915946515812091\tValidation Loss: 0.6915880479403926\n",
            "Epoch 17527  \tTraining Loss: 0.6915945904066427\tValidation Loss: 0.6915879862536383\n",
            "Epoch 17528  \tTraining Loss: 0.6915945292312987\tValidation Loss: 0.6915879245661063\n",
            "Epoch 17529  \tTraining Loss: 0.6915944680551769\tValidation Loss: 0.6915878628777964\n",
            "Epoch 17530  \tTraining Loss: 0.6915944068782772\tValidation Loss: 0.6915878011887089\n",
            "Epoch 17531  \tTraining Loss: 0.6915943457005997\tValidation Loss: 0.6915877394988436\n",
            "Epoch 17532  \tTraining Loss: 0.6915942845221443\tValidation Loss: 0.6915876778082003\n",
            "Epoch 17533  \tTraining Loss: 0.6915942233429109\tValidation Loss: 0.6915876161167793\n",
            "Epoch 17534  \tTraining Loss: 0.6915941621628997\tValidation Loss: 0.6915875544245804\n",
            "Epoch 17535  \tTraining Loss: 0.6915941009821104\tValidation Loss: 0.6915874927316035\n",
            "Epoch 17536  \tTraining Loss: 0.6915940398005431\tValidation Loss: 0.6915874310378487\n",
            "Epoch 17537  \tTraining Loss: 0.6915939786181977\tValidation Loss: 0.6915873693433159\n",
            "Epoch 17538  \tTraining Loss: 0.6915939174350743\tValidation Loss: 0.691587307648005\n",
            "Epoch 17539  \tTraining Loss: 0.6915938562511726\tValidation Loss: 0.691587245951916\n",
            "Epoch 17540  \tTraining Loss: 0.6915937950664929\tValidation Loss: 0.6915871842550491\n",
            "Epoch 17541  \tTraining Loss: 0.691593733881035\tValidation Loss: 0.6915871225574037\n",
            "Epoch 17542  \tTraining Loss: 0.6915936726947988\tValidation Loss: 0.6915870608589806\n",
            "Epoch 17543  \tTraining Loss: 0.6915936115077843\tValidation Loss: 0.6915869991597791\n",
            "Epoch 17544  \tTraining Loss: 0.6915935503199915\tValidation Loss: 0.6915869374597993\n",
            "Epoch 17545  \tTraining Loss: 0.6915934891314205\tValidation Loss: 0.6915868757590412\n",
            "Epoch 17546  \tTraining Loss: 0.691593427942071\tValidation Loss: 0.691586814057505\n",
            "Epoch 17547  \tTraining Loss: 0.6915933667519432\tValidation Loss: 0.6915867523551902\n",
            "Epoch 17548  \tTraining Loss: 0.691593305561037\tValidation Loss: 0.6915866906520973\n",
            "Epoch 17549  \tTraining Loss: 0.6915932443693523\tValidation Loss: 0.6915866289482259\n",
            "Epoch 17550  \tTraining Loss: 0.6915931831768891\tValidation Loss: 0.6915865672435761\n",
            "Epoch 17551  \tTraining Loss: 0.6915931219836474\tValidation Loss: 0.6915865055381477\n",
            "Epoch 17552  \tTraining Loss: 0.6915930607896271\tValidation Loss: 0.691586443831941\n",
            "Epoch 17553  \tTraining Loss: 0.6915929995948283\tValidation Loss: 0.6915863821249557\n",
            "Epoch 17554  \tTraining Loss: 0.6915929383992507\tValidation Loss: 0.6915863204171918\n",
            "Epoch 17555  \tTraining Loss: 0.6915928772028946\tValidation Loss: 0.6915862587086494\n",
            "Epoch 17556  \tTraining Loss: 0.6915928160057598\tValidation Loss: 0.6915861969993282\n",
            "Epoch 17557  \tTraining Loss: 0.6915927548078462\tValidation Loss: 0.6915861352892285\n",
            "Epoch 17558  \tTraining Loss: 0.6915926936091539\tValidation Loss: 0.6915860735783501\n",
            "Epoch 17559  \tTraining Loss: 0.6915926324096827\tValidation Loss: 0.6915860118666929\n",
            "Epoch 17560  \tTraining Loss: 0.6915925712094328\tValidation Loss: 0.691585950154257\n",
            "Epoch 17561  \tTraining Loss: 0.6915925100084039\tValidation Loss: 0.6915858884410423\n",
            "Epoch 17562  \tTraining Loss: 0.6915924488065963\tValidation Loss: 0.6915858267270487\n",
            "Epoch 17563  \tTraining Loss: 0.6915923876040098\tValidation Loss: 0.6915857650122763\n",
            "Epoch 17564  \tTraining Loss: 0.691592326400644\tValidation Loss: 0.691585703296725\n",
            "Epoch 17565  \tTraining Loss: 0.6915922651964995\tValidation Loss: 0.6915856415803947\n",
            "Epoch 17566  \tTraining Loss: 0.691592203991576\tValidation Loss: 0.6915855798632856\n",
            "Epoch 17567  \tTraining Loss: 0.6915921427858733\tValidation Loss: 0.6915855181453975\n",
            "Epoch 17568  \tTraining Loss: 0.6915920815793917\tValidation Loss: 0.6915854564267302\n",
            "Epoch 17569  \tTraining Loss: 0.6915920203721307\tValidation Loss: 0.691585394707284\n",
            "Epoch 17570  \tTraining Loss: 0.6915919591640908\tValidation Loss: 0.6915853329870587\n",
            "Epoch 17571  \tTraining Loss: 0.6915918979552715\tValidation Loss: 0.6915852712660543\n",
            "Epoch 17572  \tTraining Loss: 0.691591836745673\tValidation Loss: 0.6915852095442706\n",
            "Epoch 17573  \tTraining Loss: 0.6915917755352955\tValidation Loss: 0.6915851478217078\n",
            "Epoch 17574  \tTraining Loss: 0.6915917143241385\tValidation Loss: 0.6915850860983658\n",
            "Epoch 17575  \tTraining Loss: 0.6915916531122022\tValidation Loss: 0.6915850243742445\n",
            "Epoch 17576  \tTraining Loss: 0.6915915918994864\tValidation Loss: 0.691584962649344\n",
            "Epoch 17577  \tTraining Loss: 0.6915915306859914\tValidation Loss: 0.691584900923664\n",
            "Epoch 17578  \tTraining Loss: 0.691591469471717\tValidation Loss: 0.6915848391972047\n",
            "Epoch 17579  \tTraining Loss: 0.691591408256663\tValidation Loss: 0.6915847774699663\n",
            "Epoch 17580  \tTraining Loss: 0.6915913470408296\tValidation Loss: 0.6915847157419481\n",
            "Epoch 17581  \tTraining Loss: 0.6915912858242167\tValidation Loss: 0.6915846540131506\n",
            "Epoch 17582  \tTraining Loss: 0.6915912246068241\tValidation Loss: 0.6915845922835736\n",
            "Epoch 17583  \tTraining Loss: 0.691591163388652\tValidation Loss: 0.6915845305532171\n",
            "Epoch 17584  \tTraining Loss: 0.6915911021697003\tValidation Loss: 0.6915844688220811\n",
            "Epoch 17585  \tTraining Loss: 0.691591040949969\tValidation Loss: 0.6915844070901656\n",
            "Epoch 17586  \tTraining Loss: 0.6915909797294579\tValidation Loss: 0.6915843453574703\n",
            "Epoch 17587  \tTraining Loss: 0.6915909185081671\tValidation Loss: 0.6915842836239953\n",
            "Epoch 17588  \tTraining Loss: 0.6915908572860966\tValidation Loss: 0.6915842218897407\n",
            "Epoch 17589  \tTraining Loss: 0.6915907960632462\tValidation Loss: 0.6915841601547065\n",
            "Epoch 17590  \tTraining Loss: 0.6915907348396161\tValidation Loss: 0.6915840984188923\n",
            "Epoch 17591  \tTraining Loss: 0.6915906736152061\tValidation Loss: 0.6915840366822985\n",
            "Epoch 17592  \tTraining Loss: 0.6915906123900162\tValidation Loss: 0.6915839749449249\n",
            "Epoch 17593  \tTraining Loss: 0.6915905511640464\tValidation Loss: 0.6915839132067714\n",
            "Epoch 17594  \tTraining Loss: 0.6915904899372965\tValidation Loss: 0.691583851467838\n",
            "Epoch 17595  \tTraining Loss: 0.6915904287097668\tValidation Loss: 0.6915837897281246\n",
            "Epoch 17596  \tTraining Loss: 0.691590367481457\tValidation Loss: 0.6915837279876313\n",
            "Epoch 17597  \tTraining Loss: 0.6915903062523673\tValidation Loss: 0.6915836662463581\n",
            "Epoch 17598  \tTraining Loss: 0.6915902450224973\tValidation Loss: 0.691583604504305\n",
            "Epoch 17599  \tTraining Loss: 0.6915901837918472\tValidation Loss: 0.6915835427614715\n",
            "Epoch 17600  \tTraining Loss: 0.6915901225604171\tValidation Loss: 0.6915834810178583\n",
            "Epoch 17601  \tTraining Loss: 0.6915900613282067\tValidation Loss: 0.6915834192734647\n",
            "Epoch 17602  \tTraining Loss: 0.6915900000952161\tValidation Loss: 0.6915833575282911\n",
            "Epoch 17603  \tTraining Loss: 0.6915899388614453\tValidation Loss: 0.6915832957823373\n",
            "Epoch 17604  \tTraining Loss: 0.6915898776268941\tValidation Loss: 0.6915832340356033\n",
            "Epoch 17605  \tTraining Loss: 0.6915898163915626\tValidation Loss: 0.6915831722880891\n",
            "Epoch 17606  \tTraining Loss: 0.6915897551554507\tValidation Loss: 0.6915831105397944\n",
            "Epoch 17607  \tTraining Loss: 0.6915896939185585\tValidation Loss: 0.6915830487907196\n",
            "Epoch 17608  \tTraining Loss: 0.6915896326808859\tValidation Loss: 0.6915829870408644\n",
            "Epoch 17609  \tTraining Loss: 0.6915895714424327\tValidation Loss: 0.6915829252902287\n",
            "Epoch 17610  \tTraining Loss: 0.6915895102031994\tValidation Loss: 0.6915828635388127\n",
            "Epoch 17611  \tTraining Loss: 0.6915894489631852\tValidation Loss: 0.6915828017866164\n",
            "Epoch 17612  \tTraining Loss: 0.6915893877223905\tValidation Loss: 0.6915827400336395\n",
            "Epoch 17613  \tTraining Loss: 0.6915893264808152\tValidation Loss: 0.6915826782798818\n",
            "Epoch 17614  \tTraining Loss: 0.6915892652384593\tValidation Loss: 0.6915826165253439\n",
            "Epoch 17615  \tTraining Loss: 0.6915892039953229\tValidation Loss: 0.6915825547700254\n",
            "Epoch 17616  \tTraining Loss: 0.6915891427514056\tValidation Loss: 0.6915824930139263\n",
            "Epoch 17617  \tTraining Loss: 0.6915890815067077\tValidation Loss: 0.6915824312570464\n",
            "Epoch 17618  \tTraining Loss: 0.691589020261229\tValidation Loss: 0.691582369499386\n",
            "Epoch 17619  \tTraining Loss: 0.6915889590149695\tValidation Loss: 0.6915823077409448\n",
            "Epoch 17620  \tTraining Loss: 0.6915888977679292\tValidation Loss: 0.6915822459817229\n",
            "Epoch 17621  \tTraining Loss: 0.691588836520108\tValidation Loss: 0.6915821842217201\n",
            "Epoch 17622  \tTraining Loss: 0.6915887752715061\tValidation Loss: 0.6915821224609365\n",
            "Epoch 17623  \tTraining Loss: 0.6915887140221231\tValidation Loss: 0.6915820606993722\n",
            "Epoch 17624  \tTraining Loss: 0.6915886527719591\tValidation Loss: 0.691581998937027\n",
            "Epoch 17625  \tTraining Loss: 0.6915885915210144\tValidation Loss: 0.6915819371739008\n",
            "Epoch 17626  \tTraining Loss: 0.6915885302692883\tValidation Loss: 0.6915818754099938\n",
            "Epoch 17627  \tTraining Loss: 0.6915884690167815\tValidation Loss: 0.6915818136453057\n",
            "Epoch 17628  \tTraining Loss: 0.6915884077634934\tValidation Loss: 0.6915817518798367\n",
            "Epoch 17629  \tTraining Loss: 0.6915883465094241\tValidation Loss: 0.6915816901135866\n",
            "Epoch 17630  \tTraining Loss: 0.6915882852545739\tValidation Loss: 0.6915816283465553\n",
            "Epoch 17631  \tTraining Loss: 0.6915882239989425\tValidation Loss: 0.6915815665787431\n",
            "Epoch 17632  \tTraining Loss: 0.6915881627425297\tValidation Loss: 0.6915815048101498\n",
            "Epoch 17633  \tTraining Loss: 0.6915881014853357\tValidation Loss: 0.6915814430407752\n",
            "Epoch 17634  \tTraining Loss: 0.6915880402273605\tValidation Loss: 0.6915813812706193\n",
            "Epoch 17635  \tTraining Loss: 0.6915879789686039\tValidation Loss: 0.6915813194996822\n",
            "Epoch 17636  \tTraining Loss: 0.691587917709066\tValidation Loss: 0.691581257727964\n",
            "Epoch 17637  \tTraining Loss: 0.6915878564487465\tValidation Loss: 0.6915811959554644\n",
            "Epoch 17638  \tTraining Loss: 0.6915877951876459\tValidation Loss: 0.6915811341821834\n",
            "Epoch 17639  \tTraining Loss: 0.6915877339257637\tValidation Loss: 0.6915810724081212\n",
            "Epoch 17640  \tTraining Loss: 0.6915876726631\tValidation Loss: 0.6915810106332773\n",
            "Epoch 17641  \tTraining Loss: 0.6915876113996549\tValidation Loss: 0.6915809488576522\n",
            "Epoch 17642  \tTraining Loss: 0.6915875501354279\tValidation Loss: 0.6915808870812455\n",
            "Epoch 17643  \tTraining Loss: 0.6915874888704197\tValidation Loss: 0.6915808253040574\n",
            "Epoch 17644  \tTraining Loss: 0.6915874276046298\tValidation Loss: 0.6915807635260878\n",
            "Epoch 17645  \tTraining Loss: 0.6915873663380581\tValidation Loss: 0.6915807017473365\n",
            "Epoch 17646  \tTraining Loss: 0.6915873050707048\tValidation Loss: 0.6915806399678036\n",
            "Epoch 17647  \tTraining Loss: 0.6915872438025698\tValidation Loss: 0.6915805781874892\n",
            "Epoch 17648  \tTraining Loss: 0.691587182533653\tValidation Loss: 0.6915805164063931\n",
            "Epoch 17649  \tTraining Loss: 0.6915871212639544\tValidation Loss: 0.6915804546245152\n",
            "Epoch 17650  \tTraining Loss: 0.6915870599934741\tValidation Loss: 0.6915803928418556\n",
            "Epoch 17651  \tTraining Loss: 0.6915869987222119\tValidation Loss: 0.6915803310584143\n",
            "Epoch 17652  \tTraining Loss: 0.6915869374501676\tValidation Loss: 0.6915802692741911\n",
            "Epoch 17653  \tTraining Loss: 0.6915868761773416\tValidation Loss: 0.6915802074891861\n",
            "Epoch 17654  \tTraining Loss: 0.6915868149037335\tValidation Loss: 0.6915801457033993\n",
            "Epoch 17655  \tTraining Loss: 0.6915867536293437\tValidation Loss: 0.6915800839168306\n",
            "Epoch 17656  \tTraining Loss: 0.6915866923541718\tValidation Loss: 0.6915800221294799\n",
            "Epoch 17657  \tTraining Loss: 0.6915866310782176\tValidation Loss: 0.6915799603413473\n",
            "Epoch 17658  \tTraining Loss: 0.6915865698014816\tValidation Loss: 0.6915798985524326\n",
            "Epoch 17659  \tTraining Loss: 0.6915865085239633\tValidation Loss: 0.6915798367627358\n",
            "Epoch 17660  \tTraining Loss: 0.6915864472456629\tValidation Loss: 0.6915797749722572\n",
            "Epoch 17661  \tTraining Loss: 0.6915863859665803\tValidation Loss: 0.6915797131809964\n",
            "Epoch 17662  \tTraining Loss: 0.6915863246867154\tValidation Loss: 0.6915796513889534\n",
            "Epoch 17663  \tTraining Loss: 0.6915862634060684\tValidation Loss: 0.6915795895961283\n",
            "Epoch 17664  \tTraining Loss: 0.6915862021246391\tValidation Loss: 0.6915795278025209\n",
            "Epoch 17665  \tTraining Loss: 0.6915861408424274\tValidation Loss: 0.6915794660081312\n",
            "Epoch 17666  \tTraining Loss: 0.6915860795594334\tValidation Loss: 0.6915794042129594\n",
            "Epoch 17667  \tTraining Loss: 0.691586018275657\tValidation Loss: 0.6915793424170054\n",
            "Epoch 17668  \tTraining Loss: 0.6915859569910983\tValidation Loss: 0.6915792806202689\n",
            "Epoch 17669  \tTraining Loss: 0.6915858957057569\tValidation Loss: 0.6915792188227501\n",
            "Epoch 17670  \tTraining Loss: 0.6915858344196333\tValidation Loss: 0.6915791570244488\n",
            "Epoch 17671  \tTraining Loss: 0.6915857731327268\tValidation Loss: 0.6915790952253652\n",
            "Epoch 17672  \tTraining Loss: 0.691585711845038\tValidation Loss: 0.6915790334254991\n",
            "Epoch 17673  \tTraining Loss: 0.6915856505565666\tValidation Loss: 0.6915789716248504\n",
            "Epoch 17674  \tTraining Loss: 0.6915855892673126\tValidation Loss: 0.6915789098234192\n",
            "Epoch 17675  \tTraining Loss: 0.6915855279772759\tValidation Loss: 0.6915788480212056\n",
            "Epoch 17676  \tTraining Loss: 0.6915854666864567\tValidation Loss: 0.6915787862182093\n",
            "Epoch 17677  \tTraining Loss: 0.6915854053948545\tValidation Loss: 0.6915787244144304\n",
            "Epoch 17678  \tTraining Loss: 0.6915853441024696\tValidation Loss: 0.6915786626098687\n",
            "Epoch 17679  \tTraining Loss: 0.6915852828093021\tValidation Loss: 0.6915786008045245\n",
            "Epoch 17680  \tTraining Loss: 0.6915852215153516\tValidation Loss: 0.6915785389983975\n",
            "Epoch 17681  \tTraining Loss: 0.6915851602206183\tValidation Loss: 0.6915784771914877\n",
            "Epoch 17682  \tTraining Loss: 0.6915850989251022\tValidation Loss: 0.6915784153837952\n",
            "Epoch 17683  \tTraining Loss: 0.6915850376288031\tValidation Loss: 0.6915783535753197\n",
            "Epoch 17684  \tTraining Loss: 0.6915849763317211\tValidation Loss: 0.6915782917660614\n",
            "Epoch 17685  \tTraining Loss: 0.691584915033856\tValidation Loss: 0.6915782299560204\n",
            "Epoch 17686  \tTraining Loss: 0.6915848537352081\tValidation Loss: 0.6915781681451963\n",
            "Epoch 17687  \tTraining Loss: 0.6915847924357769\tValidation Loss: 0.6915781063335892\n",
            "Epoch 17688  \tTraining Loss: 0.6915847311355627\tValidation Loss: 0.6915780445211992\n",
            "Epoch 17689  \tTraining Loss: 0.6915846698345653\tValidation Loss: 0.6915779827080262\n",
            "Epoch 17690  \tTraining Loss: 0.691584608532785\tValidation Loss: 0.69157792089407\n",
            "Epoch 17691  \tTraining Loss: 0.6915845472302213\tValidation Loss: 0.6915778590793309\n",
            "Epoch 17692  \tTraining Loss: 0.6915844859268745\tValidation Loss: 0.6915777972638086\n",
            "Epoch 17693  \tTraining Loss: 0.6915844246227444\tValidation Loss: 0.691577735447503\n",
            "Epoch 17694  \tTraining Loss: 0.691584363317831\tValidation Loss: 0.6915776736304143\n",
            "Epoch 17695  \tTraining Loss: 0.6915843020121343\tValidation Loss: 0.6915776118125424\n",
            "Epoch 17696  \tTraining Loss: 0.6915842407056543\tValidation Loss: 0.6915775499938872\n",
            "Epoch 17697  \tTraining Loss: 0.6915841793983909\tValidation Loss: 0.6915774881744489\n",
            "Epoch 17698  \tTraining Loss: 0.6915841180903439\tValidation Loss: 0.6915774263542271\n",
            "Epoch 17699  \tTraining Loss: 0.6915840567815136\tValidation Loss: 0.6915773645332219\n",
            "Epoch 17700  \tTraining Loss: 0.6915839954718997\tValidation Loss: 0.6915773027114334\n",
            "Epoch 17701  \tTraining Loss: 0.6915839341615025\tValidation Loss: 0.6915772408888615\n",
            "Epoch 17702  \tTraining Loss: 0.6915838728503216\tValidation Loss: 0.691577179065506\n",
            "Epoch 17703  \tTraining Loss: 0.6915838115383572\tValidation Loss: 0.6915771172413672\n",
            "Epoch 17704  \tTraining Loss: 0.6915837502256091\tValidation Loss: 0.6915770554164448\n",
            "Epoch 17705  \tTraining Loss: 0.6915836889120773\tValidation Loss: 0.6915769935907388\n",
            "Epoch 17706  \tTraining Loss: 0.691583627597762\tValidation Loss: 0.6915769317642492\n",
            "Epoch 17707  \tTraining Loss: 0.6915835662826629\tValidation Loss: 0.691576869936976\n",
            "Epoch 17708  \tTraining Loss: 0.69158350496678\tValidation Loss: 0.6915768081089192\n",
            "Epoch 17709  \tTraining Loss: 0.6915834436501134\tValidation Loss: 0.6915767462800787\n",
            "Epoch 17710  \tTraining Loss: 0.6915833823326629\tValidation Loss: 0.6915766844504545\n",
            "Epoch 17711  \tTraining Loss: 0.6915833210144285\tValidation Loss: 0.6915766226200464\n",
            "Epoch 17712  \tTraining Loss: 0.6915832596954103\tValidation Loss: 0.6915765607888547\n",
            "Epoch 17713  \tTraining Loss: 0.6915831983756082\tValidation Loss: 0.691576498956879\n",
            "Epoch 17714  \tTraining Loss: 0.6915831370550223\tValidation Loss: 0.6915764371241196\n",
            "Epoch 17715  \tTraining Loss: 0.6915830757336521\tValidation Loss: 0.6915763752905763\n",
            "Epoch 17716  \tTraining Loss: 0.691583014411498\tValidation Loss: 0.6915763134562489\n",
            "Epoch 17717  \tTraining Loss: 0.6915829530885599\tValidation Loss: 0.6915762516211377\n",
            "Epoch 17718  \tTraining Loss: 0.6915828917648377\tValidation Loss: 0.6915761897852425\n",
            "Epoch 17719  \tTraining Loss: 0.6915828304403313\tValidation Loss: 0.6915761279485633\n",
            "Epoch 17720  \tTraining Loss: 0.6915827691150409\tValidation Loss: 0.6915760661111\n",
            "Epoch 17721  \tTraining Loss: 0.6915827077889662\tValidation Loss: 0.6915760042728526\n",
            "Epoch 17722  \tTraining Loss: 0.6915826464621073\tValidation Loss: 0.6915759424338211\n",
            "Epoch 17723  \tTraining Loss: 0.6915825851344642\tValidation Loss: 0.6915758805940054\n",
            "Epoch 17724  \tTraining Loss: 0.6915825238060368\tValidation Loss: 0.6915758187534057\n",
            "Epoch 17725  \tTraining Loss: 0.691582462476825\tValidation Loss: 0.6915757569120217\n",
            "Epoch 17726  \tTraining Loss: 0.6915824011468289\tValidation Loss: 0.6915756950698533\n",
            "Epoch 17727  \tTraining Loss: 0.6915823398160484\tValidation Loss: 0.6915756332269007\n",
            "Epoch 17728  \tTraining Loss: 0.6915822784844836\tValidation Loss: 0.6915755713831638\n",
            "Epoch 17729  \tTraining Loss: 0.6915822171521343\tValidation Loss: 0.6915755095386426\n",
            "Epoch 17730  \tTraining Loss: 0.6915821558190004\tValidation Loss: 0.6915754476933369\n",
            "Epoch 17731  \tTraining Loss: 0.691582094485082\tValidation Loss: 0.6915753858472469\n",
            "Epoch 17732  \tTraining Loss: 0.691582033150379\tValidation Loss: 0.6915753240003725\n",
            "Epoch 17733  \tTraining Loss: 0.6915819718148917\tValidation Loss: 0.6915752621527134\n",
            "Epoch 17734  \tTraining Loss: 0.6915819104786195\tValidation Loss: 0.6915752003042699\n",
            "Epoch 17735  \tTraining Loss: 0.6915818491415628\tValidation Loss: 0.6915751384550419\n",
            "Epoch 17736  \tTraining Loss: 0.6915817878037214\tValidation Loss: 0.6915750766050291\n",
            "Epoch 17737  \tTraining Loss: 0.6915817264650952\tValidation Loss: 0.6915750147542319\n",
            "Epoch 17738  \tTraining Loss: 0.6915816651256843\tValidation Loss: 0.69157495290265\n",
            "Epoch 17739  \tTraining Loss: 0.6915816037854886\tValidation Loss: 0.6915748910502834\n",
            "Epoch 17740  \tTraining Loss: 0.6915815424445082\tValidation Loss: 0.6915748291971321\n",
            "Epoch 17741  \tTraining Loss: 0.6915814811027428\tValidation Loss: 0.6915747673431961\n",
            "Epoch 17742  \tTraining Loss: 0.6915814197601925\tValidation Loss: 0.6915747054884752\n",
            "Epoch 17743  \tTraining Loss: 0.6915813584168574\tValidation Loss: 0.6915746436329696\n",
            "Epoch 17744  \tTraining Loss: 0.6915812970727372\tValidation Loss: 0.6915745817766792\n",
            "Epoch 17745  \tTraining Loss: 0.6915812357278321\tValidation Loss: 0.6915745199196037\n",
            "Epoch 17746  \tTraining Loss: 0.691581174382142\tValidation Loss: 0.6915744580617434\n",
            "Epoch 17747  \tTraining Loss: 0.6915811130356669\tValidation Loss: 0.6915743962030982\n",
            "Epoch 17748  \tTraining Loss: 0.6915810516884067\tValidation Loss: 0.691574334343668\n",
            "Epoch 17749  \tTraining Loss: 0.6915809903403612\tValidation Loss: 0.6915742724834527\n",
            "Epoch 17750  \tTraining Loss: 0.6915809289915307\tValidation Loss: 0.6915742106224524\n",
            "Epoch 17751  \tTraining Loss: 0.6915808676419151\tValidation Loss: 0.691574148760667\n",
            "Epoch 17752  \tTraining Loss: 0.6915808062915141\tValidation Loss: 0.6915740868980966\n",
            "Epoch 17753  \tTraining Loss: 0.691580744940328\tValidation Loss: 0.691574025034741\n",
            "Epoch 17754  \tTraining Loss: 0.6915806835883564\tValidation Loss: 0.6915739631706\n",
            "Epoch 17755  \tTraining Loss: 0.6915806222355997\tValidation Loss: 0.6915739013056742\n",
            "Epoch 17756  \tTraining Loss: 0.6915805608820575\tValidation Loss: 0.6915738394399628\n",
            "Epoch 17757  \tTraining Loss: 0.69158049952773\tValidation Loss: 0.6915737775734663\n",
            "Epoch 17758  \tTraining Loss: 0.6915804381726169\tValidation Loss: 0.6915737157061845\n",
            "Epoch 17759  \tTraining Loss: 0.6915803768167188\tValidation Loss: 0.6915736538381172\n",
            "Epoch 17760  \tTraining Loss: 0.6915803154600348\tValidation Loss: 0.6915735919692646\n",
            "Epoch 17761  \tTraining Loss: 0.6915802541025654\tValidation Loss: 0.6915735300996266\n",
            "Epoch 17762  \tTraining Loss: 0.6915801927443105\tValidation Loss: 0.6915734682292032\n",
            "Epoch 17763  \tTraining Loss: 0.6915801313852701\tValidation Loss: 0.6915734063579942\n",
            "Epoch 17764  \tTraining Loss: 0.6915800700254439\tValidation Loss: 0.6915733444859998\n",
            "Epoch 17765  \tTraining Loss: 0.6915800086648319\tValidation Loss: 0.6915732826132198\n",
            "Epoch 17766  \tTraining Loss: 0.6915799473034345\tValidation Loss: 0.6915732207396542\n",
            "Epoch 17767  \tTraining Loss: 0.6915798859412513\tValidation Loss: 0.6915731588653031\n",
            "Epoch 17768  \tTraining Loss: 0.6915798245782825\tValidation Loss: 0.6915730969901662\n",
            "Epoch 17769  \tTraining Loss: 0.6915797632145276\tValidation Loss: 0.6915730351142438\n",
            "Epoch 17770  \tTraining Loss: 0.6915797018499871\tValidation Loss: 0.6915729732375356\n",
            "Epoch 17771  \tTraining Loss: 0.6915796404846607\tValidation Loss: 0.6915729113600416\n",
            "Epoch 17772  \tTraining Loss: 0.6915795791185484\tValidation Loss: 0.6915728494817619\n",
            "Epoch 17773  \tTraining Loss: 0.6915795177516502\tValidation Loss: 0.6915727876026964\n",
            "Epoch 17774  \tTraining Loss: 0.691579456383966\tValidation Loss: 0.691572725722845\n",
            "Epoch 17775  \tTraining Loss: 0.6915793950154959\tValidation Loss: 0.6915726638422076\n",
            "Epoch 17776  \tTraining Loss: 0.6915793336462397\tValidation Loss: 0.6915726019607846\n",
            "Epoch 17777  \tTraining Loss: 0.6915792722761974\tValidation Loss: 0.6915725400785755\n",
            "Epoch 17778  \tTraining Loss: 0.6915792109053692\tValidation Loss: 0.6915724781955803\n",
            "Epoch 17779  \tTraining Loss: 0.6915791495337548\tValidation Loss: 0.6915724163117993\n",
            "Epoch 17780  \tTraining Loss: 0.6915790881613543\tValidation Loss: 0.6915723544272322\n",
            "Epoch 17781  \tTraining Loss: 0.6915790267881674\tValidation Loss: 0.6915722925418789\n",
            "Epoch 17782  \tTraining Loss: 0.6915789654141945\tValidation Loss: 0.6915722306557397\n",
            "Epoch 17783  \tTraining Loss: 0.6915789040394352\tValidation Loss: 0.6915721687688141\n",
            "Epoch 17784  \tTraining Loss: 0.6915788426638897\tValidation Loss: 0.6915721068811025\n",
            "Epoch 17785  \tTraining Loss: 0.6915787812875578\tValidation Loss: 0.6915720449926047\n",
            "Epoch 17786  \tTraining Loss: 0.6915787199104397\tValidation Loss: 0.6915719831033206\n",
            "Epoch 17787  \tTraining Loss: 0.6915786585325351\tValidation Loss: 0.6915719212132503\n",
            "Epoch 17788  \tTraining Loss: 0.6915785971538441\tValidation Loss: 0.6915718593223935\n",
            "Epoch 17789  \tTraining Loss: 0.6915785357743666\tValidation Loss: 0.6915717974307505\n",
            "Epoch 17790  \tTraining Loss: 0.6915784743941027\tValidation Loss: 0.6915717355383211\n",
            "Epoch 17791  \tTraining Loss: 0.6915784130130521\tValidation Loss: 0.6915716736451053\n",
            "Epoch 17792  \tTraining Loss: 0.6915783516312152\tValidation Loss: 0.6915716117511032\n",
            "Epoch 17793  \tTraining Loss: 0.6915782902485916\tValidation Loss: 0.6915715498563143\n",
            "Epoch 17794  \tTraining Loss: 0.6915782288651814\tValidation Loss: 0.6915714879607391\n",
            "Epoch 17795  \tTraining Loss: 0.6915781674809844\tValidation Loss: 0.6915714260643774\n",
            "Epoch 17796  \tTraining Loss: 0.6915781060960009\tValidation Loss: 0.691571364167229\n",
            "Epoch 17797  \tTraining Loss: 0.6915780447102307\tValidation Loss: 0.6915713022692941\n",
            "Epoch 17798  \tTraining Loss: 0.6915779833236736\tValidation Loss: 0.6915712403705725\n",
            "Epoch 17799  \tTraining Loss: 0.6915779219363298\tValidation Loss: 0.6915711784710642\n",
            "Epoch 17800  \tTraining Loss: 0.6915778605481991\tValidation Loss: 0.6915711165707692\n",
            "Epoch 17801  \tTraining Loss: 0.6915777991592816\tValidation Loss: 0.6915710546696876\n",
            "Epoch 17802  \tTraining Loss: 0.6915777377695773\tValidation Loss: 0.6915709927678191\n",
            "Epoch 17803  \tTraining Loss: 0.6915776763790861\tValidation Loss: 0.6915709308651639\n",
            "Epoch 17804  \tTraining Loss: 0.6915776149878078\tValidation Loss: 0.6915708689617216\n",
            "Epoch 17805  \tTraining Loss: 0.6915775535957426\tValidation Loss: 0.6915708070574927\n",
            "Epoch 17806  \tTraining Loss: 0.6915774922028903\tValidation Loss: 0.6915707451524767\n",
            "Epoch 17807  \tTraining Loss: 0.6915774308092509\tValidation Loss: 0.6915706832466739\n",
            "Epoch 17808  \tTraining Loss: 0.6915773694148245\tValidation Loss: 0.6915706213400842\n",
            "Epoch 17809  \tTraining Loss: 0.6915773080196111\tValidation Loss: 0.6915705594327073\n",
            "Epoch 17810  \tTraining Loss: 0.6915772466236104\tValidation Loss: 0.6915704975245435\n",
            "Epoch 17811  \tTraining Loss: 0.6915771852268225\tValidation Loss: 0.6915704356155925\n",
            "Epoch 17812  \tTraining Loss: 0.6915771238292475\tValidation Loss: 0.6915703737058545\n",
            "Epoch 17813  \tTraining Loss: 0.6915770624308851\tValidation Loss: 0.6915703117953294\n",
            "Epoch 17814  \tTraining Loss: 0.6915770010317355\tValidation Loss: 0.6915702498840169\n",
            "Epoch 17815  \tTraining Loss: 0.6915769396317986\tValidation Loss: 0.6915701879719174\n",
            "Epoch 17816  \tTraining Loss: 0.6915768782310743\tValidation Loss: 0.6915701260590308\n",
            "Epoch 17817  \tTraining Loss: 0.6915768168295625\tValidation Loss: 0.6915700641453566\n",
            "Epoch 17818  \tTraining Loss: 0.6915767554272635\tValidation Loss: 0.6915700022308953\n",
            "Epoch 17819  \tTraining Loss: 0.6915766940241769\tValidation Loss: 0.6915699403156466\n",
            "Epoch 17820  \tTraining Loss: 0.6915766326203029\tValidation Loss: 0.6915698783996105\n",
            "Epoch 17821  \tTraining Loss: 0.6915765712156413\tValidation Loss: 0.691569816482787\n",
            "Epoch 17822  \tTraining Loss: 0.6915765098101921\tValidation Loss: 0.6915697545651761\n",
            "Epoch 17823  \tTraining Loss: 0.6915764484039555\tValidation Loss: 0.6915696926467777\n",
            "Epoch 17824  \tTraining Loss: 0.6915763869969311\tValidation Loss: 0.6915696307275918\n",
            "Epoch 17825  \tTraining Loss: 0.6915763255891192\tValidation Loss: 0.6915695688076184\n",
            "Epoch 17826  \tTraining Loss: 0.6915762641805194\tValidation Loss: 0.6915695068868575\n",
            "Epoch 17827  \tTraining Loss: 0.6915762027711321\tValidation Loss: 0.6915694449653089\n",
            "Epoch 17828  \tTraining Loss: 0.6915761413609569\tValidation Loss: 0.6915693830429727\n",
            "Epoch 17829  \tTraining Loss: 0.691576079949994\tValidation Loss: 0.6915693211198487\n",
            "Epoch 17830  \tTraining Loss: 0.6915760185382434\tValidation Loss: 0.6915692591959371\n",
            "Epoch 17831  \tTraining Loss: 0.6915759571257046\tValidation Loss: 0.6915691972712378\n",
            "Epoch 17832  \tTraining Loss: 0.6915758957123781\tValidation Loss: 0.6915691353457507\n",
            "Epoch 17833  \tTraining Loss: 0.6915758342982636\tValidation Loss: 0.6915690734194758\n",
            "Epoch 17834  \tTraining Loss: 0.6915757728833613\tValidation Loss: 0.691569011492413\n",
            "Epoch 17835  \tTraining Loss: 0.6915757114676709\tValidation Loss: 0.6915689495645624\n",
            "Epoch 17836  \tTraining Loss: 0.6915756500511925\tValidation Loss: 0.691568887635924\n",
            "Epoch 17837  \tTraining Loss: 0.6915755886339261\tValidation Loss: 0.6915688257064975\n",
            "Epoch 17838  \tTraining Loss: 0.6915755272158716\tValidation Loss: 0.6915687637762831\n",
            "Epoch 17839  \tTraining Loss: 0.691575465797029\tValidation Loss: 0.6915687018452806\n",
            "Epoch 17840  \tTraining Loss: 0.6915754043773981\tValidation Loss: 0.69156863991349\n",
            "Epoch 17841  \tTraining Loss: 0.6915753429569791\tValidation Loss: 0.6915685779809115\n",
            "Epoch 17842  \tTraining Loss: 0.6915752815357719\tValidation Loss: 0.691568516047545\n",
            "Epoch 17843  \tTraining Loss: 0.6915752201137764\tValidation Loss: 0.6915684541133902\n",
            "Epoch 17844  \tTraining Loss: 0.6915751586909926\tValidation Loss: 0.6915683921784473\n",
            "Epoch 17845  \tTraining Loss: 0.6915750972674205\tValidation Loss: 0.6915683302427161\n",
            "Epoch 17846  \tTraining Loss: 0.6915750358430601\tValidation Loss: 0.6915682683061968\n",
            "Epoch 17847  \tTraining Loss: 0.6915749744179113\tValidation Loss: 0.6915682063688892\n",
            "Epoch 17848  \tTraining Loss: 0.6915749129919739\tValidation Loss: 0.6915681444307932\n",
            "Epoch 17849  \tTraining Loss: 0.6915748515652482\tValidation Loss: 0.6915680824919089\n",
            "Epoch 17850  \tTraining Loss: 0.6915747901377339\tValidation Loss: 0.6915680205522363\n",
            "Epoch 17851  \tTraining Loss: 0.6915747287094313\tValidation Loss: 0.6915679586117753\n",
            "Epoch 17852  \tTraining Loss: 0.69157466728034\tValidation Loss: 0.6915678966705259\n",
            "Epoch 17853  \tTraining Loss: 0.6915746058504602\tValidation Loss: 0.6915678347284879\n",
            "Epoch 17854  \tTraining Loss: 0.6915745444197916\tValidation Loss: 0.6915677727856615\n",
            "Epoch 17855  \tTraining Loss: 0.6915744829883345\tValidation Loss: 0.6915677108420466\n",
            "Epoch 17856  \tTraining Loss: 0.6915744215560886\tValidation Loss: 0.6915676488976429\n",
            "Epoch 17857  \tTraining Loss: 0.6915743601230541\tValidation Loss: 0.6915675869524509\n",
            "Epoch 17858  \tTraining Loss: 0.6915742986892307\tValidation Loss: 0.69156752500647\n",
            "Epoch 17859  \tTraining Loss: 0.6915742372546186\tValidation Loss: 0.6915674630597007\n",
            "Epoch 17860  \tTraining Loss: 0.6915741758192177\tValidation Loss: 0.6915674011121425\n",
            "Epoch 17861  \tTraining Loss: 0.6915741143830278\tValidation Loss: 0.6915673391637956\n",
            "Epoch 17862  \tTraining Loss: 0.6915740529460491\tValidation Loss: 0.69156727721466\n",
            "Epoch 17863  \tTraining Loss: 0.6915739915082815\tValidation Loss: 0.6915672152647356\n",
            "Epoch 17864  \tTraining Loss: 0.691573930069725\tValidation Loss: 0.6915671533140224\n",
            "Epoch 17865  \tTraining Loss: 0.6915738686303794\tValidation Loss: 0.6915670913625203\n",
            "Epoch 17866  \tTraining Loss: 0.6915738071902449\tValidation Loss: 0.6915670294102292\n",
            "Epoch 17867  \tTraining Loss: 0.6915737457493212\tValidation Loss: 0.6915669674571493\n",
            "Epoch 17868  \tTraining Loss: 0.6915736843076085\tValidation Loss: 0.6915669055032803\n",
            "Epoch 17869  \tTraining Loss: 0.6915736228651066\tValidation Loss: 0.6915668435486223\n",
            "Epoch 17870  \tTraining Loss: 0.6915735614218156\tValidation Loss: 0.6915667815931754\n",
            "Epoch 17871  \tTraining Loss: 0.6915734999777353\tValidation Loss: 0.6915667196369394\n",
            "Epoch 17872  \tTraining Loss: 0.6915734385328658\tValidation Loss: 0.6915666576799143\n",
            "Epoch 17873  \tTraining Loss: 0.6915733770872071\tValidation Loss: 0.6915665957221\n",
            "Epoch 17874  \tTraining Loss: 0.6915733156407592\tValidation Loss: 0.6915665337634966\n",
            "Epoch 17875  \tTraining Loss: 0.6915732541935219\tValidation Loss: 0.6915664718041039\n",
            "Epoch 17876  \tTraining Loss: 0.6915731927454952\tValidation Loss: 0.6915664098439221\n",
            "Epoch 17877  \tTraining Loss: 0.691573131296679\tValidation Loss: 0.6915663478829511\n",
            "Epoch 17878  \tTraining Loss: 0.6915730698470737\tValidation Loss: 0.6915662859211906\n",
            "Epoch 17879  \tTraining Loss: 0.6915730083966787\tValidation Loss: 0.6915662239586409\n",
            "Epoch 17880  \tTraining Loss: 0.6915729469454942\tValidation Loss: 0.6915661619953019\n",
            "Epoch 17881  \tTraining Loss: 0.6915728854935203\tValidation Loss: 0.6915661000311732\n",
            "Epoch 17882  \tTraining Loss: 0.6915728240407567\tValidation Loss: 0.6915660380662554\n",
            "Epoch 17883  \tTraining Loss: 0.6915727625872036\tValidation Loss: 0.6915659761005479\n",
            "Epoch 17884  \tTraining Loss: 0.6915727011328608\tValidation Loss: 0.691565914134051\n",
            "Epoch 17885  \tTraining Loss: 0.6915726396777284\tValidation Loss: 0.6915658521667646\n",
            "Epoch 17886  \tTraining Loss: 0.6915725782218063\tValidation Loss: 0.6915657901986887\n",
            "Epoch 17887  \tTraining Loss: 0.6915725167650945\tValidation Loss: 0.691565728229823\n",
            "Epoch 17888  \tTraining Loss: 0.6915724553075929\tValidation Loss: 0.6915656662601678\n",
            "Epoch 17889  \tTraining Loss: 0.6915723938493016\tValidation Loss: 0.6915656042897229\n",
            "Epoch 17890  \tTraining Loss: 0.6915723323902202\tValidation Loss: 0.6915655423184883\n",
            "Epoch 17891  \tTraining Loss: 0.6915722709303492\tValidation Loss: 0.691565480346464\n",
            "Epoch 17892  \tTraining Loss: 0.6915722094696883\tValidation Loss: 0.6915654183736499\n",
            "Epoch 17893  \tTraining Loss: 0.6915721480082374\tValidation Loss: 0.691565356400046\n",
            "Epoch 17894  \tTraining Loss: 0.6915720865459967\tValidation Loss: 0.6915652944256525\n",
            "Epoch 17895  \tTraining Loss: 0.6915720250829657\tValidation Loss: 0.6915652324504689\n",
            "Epoch 17896  \tTraining Loss: 0.6915719636191449\tValidation Loss: 0.6915651704744954\n",
            "Epoch 17897  \tTraining Loss: 0.6915719021545339\tValidation Loss: 0.691565108497732\n",
            "Epoch 17898  \tTraining Loss: 0.6915718406891328\tValidation Loss: 0.6915650465201787\n",
            "Epoch 17899  \tTraining Loss: 0.6915717792229417\tValidation Loss: 0.6915649845418353\n",
            "Epoch 17900  \tTraining Loss: 0.6915717177559604\tValidation Loss: 0.691564922562702\n",
            "Epoch 17901  \tTraining Loss: 0.6915716562881888\tValidation Loss: 0.6915648605827786\n",
            "Epoch 17902  \tTraining Loss: 0.691571594819627\tValidation Loss: 0.691564798602065\n",
            "Epoch 17903  \tTraining Loss: 0.6915715333502752\tValidation Loss: 0.6915647366205613\n",
            "Epoch 17904  \tTraining Loss: 0.6915714718801328\tValidation Loss: 0.6915646746382674\n",
            "Epoch 17905  \tTraining Loss: 0.6915714104092002\tValidation Loss: 0.6915646126551834\n",
            "Epoch 17906  \tTraining Loss: 0.6915713489374773\tValidation Loss: 0.6915645506713092\n",
            "Epoch 17907  \tTraining Loss: 0.6915712874649639\tValidation Loss: 0.6915644886866447\n",
            "Epoch 17908  \tTraining Loss: 0.69157122599166\tValidation Loss: 0.6915644267011898\n",
            "Epoch 17909  \tTraining Loss: 0.6915711645175658\tValidation Loss: 0.6915643647149448\n",
            "Epoch 17910  \tTraining Loss: 0.691571103042681\tValidation Loss: 0.6915643027279091\n",
            "Epoch 17911  \tTraining Loss: 0.6915710415670057\tValidation Loss: 0.6915642407400833\n",
            "Epoch 17912  \tTraining Loss: 0.6915709800905397\tValidation Loss: 0.6915641787514669\n",
            "Epoch 17913  \tTraining Loss: 0.6915709186132833\tValidation Loss: 0.6915641167620601\n",
            "Epoch 17914  \tTraining Loss: 0.6915708571352362\tValidation Loss: 0.6915640547718629\n",
            "Epoch 17915  \tTraining Loss: 0.6915707956563986\tValidation Loss: 0.691563992780875\n",
            "Epoch 17916  \tTraining Loss: 0.6915707341767701\tValidation Loss: 0.6915639307890966\n",
            "Epoch 17917  \tTraining Loss: 0.6915706726963509\tValidation Loss: 0.6915638687965278\n",
            "Epoch 17918  \tTraining Loss: 0.6915706112151411\tValidation Loss: 0.6915638068031681\n",
            "Epoch 17919  \tTraining Loss: 0.6915705497331402\tValidation Loss: 0.6915637448090178\n",
            "Epoch 17920  \tTraining Loss: 0.6915704882503487\tValidation Loss: 0.6915636828140769\n",
            "Epoch 17921  \tTraining Loss: 0.6915704267667663\tValidation Loss: 0.6915636208183452\n",
            "Epoch 17922  \tTraining Loss: 0.6915703652823929\tValidation Loss: 0.6915635588218227\n",
            "Epoch 17923  \tTraining Loss: 0.6915703037972286\tValidation Loss: 0.6915634968245095\n",
            "Epoch 17924  \tTraining Loss: 0.6915702423112735\tValidation Loss: 0.6915634348264054\n",
            "Epoch 17925  \tTraining Loss: 0.6915701808245273\tValidation Loss: 0.6915633728275106\n",
            "Epoch 17926  \tTraining Loss: 0.6915701193369899\tValidation Loss: 0.6915633108278247\n",
            "Epoch 17927  \tTraining Loss: 0.6915700578486617\tValidation Loss: 0.691563248827348\n",
            "Epoch 17928  \tTraining Loss: 0.6915699963595423\tValidation Loss: 0.6915631868260802\n",
            "Epoch 17929  \tTraining Loss: 0.6915699348696316\tValidation Loss: 0.6915631248240215\n",
            "Epoch 17930  \tTraining Loss: 0.69156987337893\tValidation Loss: 0.6915630628211716\n",
            "Epoch 17931  \tTraining Loss: 0.691569811887437\tValidation Loss: 0.6915630008175309\n",
            "Epoch 17932  \tTraining Loss: 0.6915697503951529\tValidation Loss: 0.6915629388130992\n",
            "Epoch 17933  \tTraining Loss: 0.6915696889020776\tValidation Loss: 0.6915628768078761\n",
            "Epoch 17934  \tTraining Loss: 0.6915696274082109\tValidation Loss: 0.6915628148018618\n",
            "Epoch 17935  \tTraining Loss: 0.6915695659135528\tValidation Loss: 0.6915627527950565\n",
            "Epoch 17936  \tTraining Loss: 0.6915695044181034\tValidation Loss: 0.6915626907874599\n",
            "Epoch 17937  \tTraining Loss: 0.6915694429218626\tValidation Loss: 0.691562628779072\n",
            "Epoch 17938  \tTraining Loss: 0.6915693814248304\tValidation Loss: 0.6915625667698928\n",
            "Epoch 17939  \tTraining Loss: 0.6915693199270067\tValidation Loss: 0.6915625047599224\n",
            "Epoch 17940  \tTraining Loss: 0.6915692584283915\tValidation Loss: 0.6915624427491606\n",
            "Epoch 17941  \tTraining Loss: 0.6915691969289847\tValidation Loss: 0.6915623807376072\n",
            "Epoch 17942  \tTraining Loss: 0.6915691354287864\tValidation Loss: 0.6915623187252625\n",
            "Epoch 17943  \tTraining Loss: 0.6915690739277965\tValidation Loss: 0.6915622567121266\n",
            "Epoch 17944  \tTraining Loss: 0.6915690124260151\tValidation Loss: 0.6915621946981989\n",
            "Epoch 17945  \tTraining Loss: 0.6915689509234417\tValidation Loss: 0.6915621326834798\n",
            "Epoch 17946  \tTraining Loss: 0.6915688894200769\tValidation Loss: 0.691562070667969\n",
            "Epoch 17947  \tTraining Loss: 0.6915688279159202\tValidation Loss: 0.6915620086516667\n",
            "Epoch 17948  \tTraining Loss: 0.6915687664109719\tValidation Loss: 0.6915619466345728\n",
            "Epoch 17949  \tTraining Loss: 0.6915687049052317\tValidation Loss: 0.6915618846166872\n",
            "Epoch 17950  \tTraining Loss: 0.6915686433986997\tValidation Loss: 0.6915618225980099\n",
            "Epoch 17951  \tTraining Loss: 0.6915685818913758\tValidation Loss: 0.6915617605785408\n",
            "Epoch 17952  \tTraining Loss: 0.6915685203832599\tValidation Loss: 0.6915616985582801\n",
            "Epoch 17953  \tTraining Loss: 0.6915684588743521\tValidation Loss: 0.6915616365372276\n",
            "Epoch 17954  \tTraining Loss: 0.6915683973646525\tValidation Loss: 0.6915615745153831\n",
            "Epoch 17955  \tTraining Loss: 0.6915683358541608\tValidation Loss: 0.6915615124927469\n",
            "Epoch 17956  \tTraining Loss: 0.691568274342877\tValidation Loss: 0.6915614504693187\n",
            "Epoch 17957  \tTraining Loss: 0.6915682128308012\tValidation Loss: 0.6915613884450987\n",
            "Epoch 17958  \tTraining Loss: 0.6915681513179334\tValidation Loss: 0.6915613264200867\n",
            "Epoch 17959  \tTraining Loss: 0.6915680898042732\tValidation Loss: 0.6915612643942826\n",
            "Epoch 17960  \tTraining Loss: 0.691568028289821\tValidation Loss: 0.6915612023676867\n",
            "Epoch 17961  \tTraining Loss: 0.6915679667745767\tValidation Loss: 0.6915611403402985\n",
            "Epoch 17962  \tTraining Loss: 0.6915679052585398\tValidation Loss: 0.6915610783121183\n",
            "Epoch 17963  \tTraining Loss: 0.6915678437417111\tValidation Loss: 0.6915610162831459\n",
            "Epoch 17964  \tTraining Loss: 0.6915677822240899\tValidation Loss: 0.6915609542533815\n",
            "Epoch 17965  \tTraining Loss: 0.6915677207056763\tValidation Loss: 0.6915608922228248\n",
            "Epoch 17966  \tTraining Loss: 0.6915676591864702\tValidation Loss: 0.6915608301914759\n",
            "Epoch 17967  \tTraining Loss: 0.691567597666472\tValidation Loss: 0.6915607681593346\n",
            "Epoch 17968  \tTraining Loss: 0.6915675361456812\tValidation Loss: 0.6915607061264012\n",
            "Epoch 17969  \tTraining Loss: 0.691567474624098\tValidation Loss: 0.6915606440926754\n",
            "Epoch 17970  \tTraining Loss: 0.6915674131017222\tValidation Loss: 0.6915605820581573\n",
            "Epoch 17971  \tTraining Loss: 0.691567351578554\tValidation Loss: 0.6915605200228466\n",
            "Epoch 17972  \tTraining Loss: 0.6915672900545932\tValidation Loss: 0.6915604579867438\n",
            "Epoch 17973  \tTraining Loss: 0.6915672285298395\tValidation Loss: 0.6915603959498483\n",
            "Epoch 17974  \tTraining Loss: 0.6915671670042934\tValidation Loss: 0.6915603339121603\n",
            "Epoch 17975  \tTraining Loss: 0.6915671054779547\tValidation Loss: 0.6915602718736799\n",
            "Epoch 17976  \tTraining Loss: 0.6915670439508234\tValidation Loss: 0.6915602098344069\n",
            "Epoch 17977  \tTraining Loss: 0.691566982422899\tValidation Loss: 0.6915601477943413\n",
            "Epoch 17978  \tTraining Loss: 0.6915669208941821\tValidation Loss: 0.691560085753483\n",
            "Epoch 17979  \tTraining Loss: 0.6915668593646723\tValidation Loss: 0.6915600237118322\n",
            "Epoch 17980  \tTraining Loss: 0.6915667978343697\tValidation Loss: 0.6915599616693886\n",
            "Epoch 17981  \tTraining Loss: 0.6915667363032741\tValidation Loss: 0.6915598996261523\n",
            "Epoch 17982  \tTraining Loss: 0.6915666747713858\tValidation Loss: 0.6915598375821232\n",
            "Epoch 17983  \tTraining Loss: 0.6915666132387044\tValidation Loss: 0.6915597755373013\n",
            "Epoch 17984  \tTraining Loss: 0.6915665517052301\tValidation Loss: 0.6915597134916867\n",
            "Epoch 17985  \tTraining Loss: 0.6915664901709628\tValidation Loss: 0.6915596514452792\n",
            "Epoch 17986  \tTraining Loss: 0.6915664286359025\tValidation Loss: 0.6915595893980786\n",
            "Epoch 17987  \tTraining Loss: 0.691566367100049\tValidation Loss: 0.6915595273500853\n",
            "Epoch 17988  \tTraining Loss: 0.6915663055634024\tValidation Loss: 0.6915594653012989\n",
            "Epoch 17989  \tTraining Loss: 0.6915662440259628\tValidation Loss: 0.6915594032517197\n",
            "Epoch 17990  \tTraining Loss: 0.6915661824877299\tValidation Loss: 0.6915593412013473\n",
            "Epoch 17991  \tTraining Loss: 0.6915661209487038\tValidation Loss: 0.6915592791501819\n",
            "Epoch 17992  \tTraining Loss: 0.6915660594088846\tValidation Loss: 0.6915592170982235\n",
            "Epoch 17993  \tTraining Loss: 0.6915659978682719\tValidation Loss: 0.6915591550454718\n",
            "Epoch 17994  \tTraining Loss: 0.691565936326866\tValidation Loss: 0.6915590929919271\n",
            "Epoch 17995  \tTraining Loss: 0.6915658747846668\tValidation Loss: 0.6915590309375891\n",
            "Epoch 17996  \tTraining Loss: 0.6915658132416743\tValidation Loss: 0.6915589688824577\n",
            "Epoch 17997  \tTraining Loss: 0.6915657516978881\tValidation Loss: 0.6915589068265334\n",
            "Epoch 17998  \tTraining Loss: 0.6915656901533088\tValidation Loss: 0.6915588447698157\n",
            "Epoch 17999  \tTraining Loss: 0.6915656286079359\tValidation Loss: 0.6915587827123045\n",
            "Epoch 18000  \tTraining Loss: 0.6915655670617694\tValidation Loss: 0.6915587206540001\n",
            "Epoch 18001  \tTraining Loss: 0.6915655055148096\tValidation Loss: 0.6915586585949023\n",
            "Epoch 18002  \tTraining Loss: 0.6915654439670559\tValidation Loss: 0.6915585965350111\n",
            "Epoch 18003  \tTraining Loss: 0.6915653824185087\tValidation Loss: 0.6915585344743264\n",
            "Epoch 18004  \tTraining Loss: 0.6915653208691679\tValidation Loss: 0.6915584724128481\n",
            "Epoch 18005  \tTraining Loss: 0.6915652593190335\tValidation Loss: 0.6915584103505764\n",
            "Epoch 18006  \tTraining Loss: 0.6915651977681053\tValidation Loss: 0.6915583482875111\n",
            "Epoch 18007  \tTraining Loss: 0.6915651362163834\tValidation Loss: 0.6915582862236522\n",
            "Epoch 18008  \tTraining Loss: 0.6915650746638675\tValidation Loss: 0.6915582241589996\n",
            "Epoch 18009  \tTraining Loss: 0.6915650131105581\tValidation Loss: 0.6915581620935536\n",
            "Epoch 18010  \tTraining Loss: 0.6915649515564548\tValidation Loss: 0.6915581000273135\n",
            "Epoch 18011  \tTraining Loss: 0.6915648900015575\tValidation Loss: 0.69155803796028\n",
            "Epoch 18012  \tTraining Loss: 0.6915648284458663\tValidation Loss: 0.6915579758924527\n",
            "Epoch 18013  \tTraining Loss: 0.6915647668893813\tValidation Loss: 0.6915579138238316\n",
            "Epoch 18014  \tTraining Loss: 0.6915647053321021\tValidation Loss: 0.6915578517544166\n",
            "Epoch 18015  \tTraining Loss: 0.6915646437740292\tValidation Loss: 0.6915577896842078\n",
            "Epoch 18016  \tTraining Loss: 0.6915645822151619\tValidation Loss: 0.6915577276132051\n",
            "Epoch 18017  \tTraining Loss: 0.6915645206555008\tValidation Loss: 0.6915576655414084\n",
            "Epoch 18018  \tTraining Loss: 0.6915644590950454\tValidation Loss: 0.6915576034688178\n",
            "Epoch 18019  \tTraining Loss: 0.6915643975337961\tValidation Loss: 0.6915575413954332\n",
            "Epoch 18020  \tTraining Loss: 0.6915643359717524\tValidation Loss: 0.6915574793212546\n",
            "Epoch 18021  \tTraining Loss: 0.6915642744089144\tValidation Loss: 0.691557417246282\n",
            "Epoch 18022  \tTraining Loss: 0.6915642128452824\tValidation Loss: 0.6915573551705151\n",
            "Epoch 18023  \tTraining Loss: 0.691564151280856\tValidation Loss: 0.6915572930939542\n",
            "Epoch 18024  \tTraining Loss: 0.6915640897156353\tValidation Loss: 0.6915572310165992\n",
            "Epoch 18025  \tTraining Loss: 0.6915640281496203\tValidation Loss: 0.6915571689384499\n",
            "Epoch 18026  \tTraining Loss: 0.6915639665828109\tValidation Loss: 0.6915571068595063\n",
            "Epoch 18027  \tTraining Loss: 0.691563905015207\tValidation Loss: 0.6915570447797686\n",
            "Epoch 18028  \tTraining Loss: 0.6915638434468088\tValidation Loss: 0.6915569826992366\n",
            "Epoch 18029  \tTraining Loss: 0.691563781877616\tValidation Loss: 0.6915569206179103\n",
            "Epoch 18030  \tTraining Loss: 0.6915637203076287\tValidation Loss: 0.6915568585357895\n",
            "Epoch 18031  \tTraining Loss: 0.6915636587368469\tValidation Loss: 0.6915567964528744\n",
            "Epoch 18032  \tTraining Loss: 0.6915635971652705\tValidation Loss: 0.6915567343691649\n",
            "Epoch 18033  \tTraining Loss: 0.6915635355928994\tValidation Loss: 0.6915566722846608\n",
            "Epoch 18034  \tTraining Loss: 0.6915634740197338\tValidation Loss: 0.6915566101993624\n",
            "Epoch 18035  \tTraining Loss: 0.6915634124457735\tValidation Loss: 0.6915565481132693\n",
            "Epoch 18036  \tTraining Loss: 0.6915633508710185\tValidation Loss: 0.6915564860263818\n",
            "Epoch 18037  \tTraining Loss: 0.6915632892954685\tValidation Loss: 0.6915564239386995\n",
            "Epoch 18038  \tTraining Loss: 0.6915632277191239\tValidation Loss: 0.6915563618502227\n",
            "Epoch 18039  \tTraining Loss: 0.6915631661419847\tValidation Loss: 0.6915562997609512\n",
            "Epoch 18040  \tTraining Loss: 0.6915631045640503\tValidation Loss: 0.6915562376708853\n",
            "Epoch 18041  \tTraining Loss: 0.6915630429853212\tValidation Loss: 0.6915561755800244\n",
            "Epoch 18042  \tTraining Loss: 0.6915629814057972\tValidation Loss: 0.6915561134883688\n",
            "Epoch 18043  \tTraining Loss: 0.6915629198254781\tValidation Loss: 0.6915560513959182\n",
            "Epoch 18044  \tTraining Loss: 0.6915628582443641\tValidation Loss: 0.6915559893026729\n",
            "Epoch 18045  \tTraining Loss: 0.6915627966624551\tValidation Loss: 0.6915559272086329\n",
            "Epoch 18046  \tTraining Loss: 0.6915627350797511\tValidation Loss: 0.6915558651137979\n",
            "Epoch 18047  \tTraining Loss: 0.6915626734962519\tValidation Loss: 0.691555803018168\n",
            "Epoch 18048  \tTraining Loss: 0.6915626119119577\tValidation Loss: 0.691555740921743\n",
            "Epoch 18049  \tTraining Loss: 0.6915625503268683\tValidation Loss: 0.6915556788245231\n",
            "Epoch 18050  \tTraining Loss: 0.6915624887409838\tValidation Loss: 0.6915556167265082\n",
            "Epoch 18051  \tTraining Loss: 0.6915624271543038\tValidation Loss: 0.6915555546276982\n",
            "Epoch 18052  \tTraining Loss: 0.6915623655668288\tValidation Loss: 0.6915554925280931\n",
            "Epoch 18053  \tTraining Loss: 0.6915623039785584\tValidation Loss: 0.691555430427693\n",
            "Epoch 18054  \tTraining Loss: 0.6915622423894928\tValidation Loss: 0.6915553683264974\n",
            "Epoch 18055  \tTraining Loss: 0.6915621807996317\tValidation Loss: 0.6915553062245071\n",
            "Epoch 18056  \tTraining Loss: 0.6915621192089754\tValidation Loss: 0.6915552441217213\n",
            "Epoch 18057  \tTraining Loss: 0.6915620576175235\tValidation Loss: 0.6915551820181403\n",
            "Epoch 18058  \tTraining Loss: 0.6915619960252762\tValidation Loss: 0.6915551199137638\n",
            "Epoch 18059  \tTraining Loss: 0.6915619344322336\tValidation Loss: 0.6915550578085922\n",
            "Epoch 18060  \tTraining Loss: 0.6915618728383953\tValidation Loss: 0.6915549957026251\n",
            "Epoch 18061  \tTraining Loss: 0.6915618112437615\tValidation Loss: 0.6915549335958627\n",
            "Epoch 18062  \tTraining Loss: 0.6915617496483321\tValidation Loss: 0.6915548714883049\n",
            "Epoch 18063  \tTraining Loss: 0.6915616880521069\tValidation Loss: 0.6915548093799516\n",
            "Epoch 18064  \tTraining Loss: 0.6915616264550863\tValidation Loss: 0.6915547472708029\n",
            "Epoch 18065  \tTraining Loss: 0.6915615648572699\tValidation Loss: 0.6915546851608585\n",
            "Epoch 18066  \tTraining Loss: 0.6915615032586578\tValidation Loss: 0.6915546230501186\n",
            "Epoch 18067  \tTraining Loss: 0.6915614416592499\tValidation Loss: 0.6915545609385833\n",
            "Epoch 18068  \tTraining Loss: 0.6915613800590462\tValidation Loss: 0.6915544988262521\n",
            "Epoch 18069  \tTraining Loss: 0.6915613184580468\tValidation Loss: 0.6915544367131252\n",
            "Epoch 18070  \tTraining Loss: 0.6915612568562515\tValidation Loss: 0.6915543745992029\n",
            "Epoch 18071  \tTraining Loss: 0.6915611952536602\tValidation Loss: 0.6915543124844846\n",
            "Epoch 18072  \tTraining Loss: 0.691561133650273\tValidation Loss: 0.6915542503689707\n",
            "Epoch 18073  \tTraining Loss: 0.69156107204609\tValidation Loss: 0.6915541882526609\n",
            "Epoch 18074  \tTraining Loss: 0.6915610104411111\tValidation Loss: 0.6915541261355553\n",
            "Epoch 18075  \tTraining Loss: 0.6915609488353358\tValidation Loss: 0.691554064017654\n",
            "Epoch 18076  \tTraining Loss: 0.6915608872287646\tValidation Loss: 0.6915540018989565\n",
            "Epoch 18077  \tTraining Loss: 0.6915608256213974\tValidation Loss: 0.6915539397794632\n",
            "Epoch 18078  \tTraining Loss: 0.6915607640132341\tValidation Loss: 0.6915538776591741\n",
            "Epoch 18079  \tTraining Loss: 0.6915607024042746\tValidation Loss: 0.6915538155380889\n",
            "Epoch 18080  \tTraining Loss: 0.6915606407945188\tValidation Loss: 0.6915537534162076\n",
            "Epoch 18081  \tTraining Loss: 0.6915605791839667\tValidation Loss: 0.6915536912935304\n",
            "Epoch 18082  \tTraining Loss: 0.6915605175726186\tValidation Loss: 0.691553629170057\n",
            "Epoch 18083  \tTraining Loss: 0.6915604559604742\tValidation Loss: 0.6915535670457874\n",
            "Epoch 18084  \tTraining Loss: 0.6915603943475335\tValidation Loss: 0.6915535049207218\n",
            "Epoch 18085  \tTraining Loss: 0.6915603327337961\tValidation Loss: 0.69155344279486\n",
            "Epoch 18086  \tTraining Loss: 0.6915602711192627\tValidation Loss: 0.6915533806682018\n",
            "Epoch 18087  \tTraining Loss: 0.6915602095039326\tValidation Loss: 0.6915533185407474\n",
            "Epoch 18088  \tTraining Loss: 0.6915601478878061\tValidation Loss: 0.6915532564124969\n",
            "Epoch 18089  \tTraining Loss: 0.6915600862708832\tValidation Loss: 0.6915531942834497\n",
            "Epoch 18090  \tTraining Loss: 0.6915600246531638\tValidation Loss: 0.6915531321536064\n",
            "Epoch 18091  \tTraining Loss: 0.6915599630346477\tValidation Loss: 0.6915530700229666\n",
            "Epoch 18092  \tTraining Loss: 0.6915599014153351\tValidation Loss: 0.6915530078915304\n",
            "Epoch 18093  \tTraining Loss: 0.6915598397952258\tValidation Loss: 0.6915529457592978\n",
            "Epoch 18094  \tTraining Loss: 0.6915597781743199\tValidation Loss: 0.6915528836262688\n",
            "Epoch 18095  \tTraining Loss: 0.6915597165526174\tValidation Loss: 0.691552821492443\n",
            "Epoch 18096  \tTraining Loss: 0.6915596549301181\tValidation Loss: 0.6915527593578208\n",
            "Epoch 18097  \tTraining Loss: 0.6915595933068219\tValidation Loss: 0.691552697222402\n",
            "Epoch 18098  \tTraining Loss: 0.691559531682729\tValidation Loss: 0.6915526350861866\n",
            "Epoch 18099  \tTraining Loss: 0.6915594700578392\tValidation Loss: 0.6915525729491744\n",
            "Epoch 18100  \tTraining Loss: 0.6915594084321527\tValidation Loss: 0.6915525108113656\n",
            "Epoch 18101  \tTraining Loss: 0.6915593468056691\tValidation Loss: 0.69155244867276\n",
            "Epoch 18102  \tTraining Loss: 0.6915592851783887\tValidation Loss: 0.6915523865333577\n",
            "Epoch 18103  \tTraining Loss: 0.6915592235503114\tValidation Loss: 0.6915523243931586\n",
            "Epoch 18104  \tTraining Loss: 0.6915591619214371\tValidation Loss: 0.6915522622521627\n",
            "Epoch 18105  \tTraining Loss: 0.6915591002917657\tValidation Loss: 0.6915522001103699\n",
            "Epoch 18106  \tTraining Loss: 0.6915590386612972\tValidation Loss: 0.6915521379677803\n",
            "Epoch 18107  \tTraining Loss: 0.6915589770300316\tValidation Loss: 0.6915520758243936\n",
            "Epoch 18108  \tTraining Loss: 0.6915589153979689\tValidation Loss: 0.69155201368021\n",
            "Epoch 18109  \tTraining Loss: 0.6915588537651091\tValidation Loss: 0.6915519515352295\n",
            "Epoch 18110  \tTraining Loss: 0.6915587921314521\tValidation Loss: 0.6915518893894518\n",
            "Epoch 18111  \tTraining Loss: 0.6915587304969978\tValidation Loss: 0.6915518272428771\n",
            "Epoch 18112  \tTraining Loss: 0.6915586688617462\tValidation Loss: 0.6915517650955053\n",
            "Epoch 18113  \tTraining Loss: 0.6915586072256974\tValidation Loss: 0.6915517029473365\n",
            "Epoch 18114  \tTraining Loss: 0.6915585455888513\tValidation Loss: 0.6915516407983704\n",
            "Epoch 18115  \tTraining Loss: 0.6915584839512077\tValidation Loss: 0.6915515786486072\n",
            "Epoch 18116  \tTraining Loss: 0.6915584223127668\tValidation Loss: 0.6915515164980467\n",
            "Epoch 18117  \tTraining Loss: 0.6915583606735284\tValidation Loss: 0.6915514543466889\n",
            "Epoch 18118  \tTraining Loss: 0.6915582990334925\tValidation Loss: 0.6915513921945339\n",
            "Epoch 18119  \tTraining Loss: 0.6915582373926592\tValidation Loss: 0.6915513300415815\n",
            "Epoch 18120  \tTraining Loss: 0.6915581757510283\tValidation Loss: 0.6915512678878318\n",
            "Epoch 18121  \tTraining Loss: 0.6915581141085999\tValidation Loss: 0.6915512057332847\n",
            "Epoch 18122  \tTraining Loss: 0.6915580524653738\tValidation Loss: 0.6915511435779401\n",
            "Epoch 18123  \tTraining Loss: 0.6915579908213502\tValidation Loss: 0.6915510814217981\n",
            "Epoch 18124  \tTraining Loss: 0.6915579291765288\tValidation Loss: 0.6915510192648585\n",
            "Epoch 18125  \tTraining Loss: 0.6915578675309098\tValidation Loss: 0.6915509571071214\n",
            "Epoch 18126  \tTraining Loss: 0.691557805884493\tValidation Loss: 0.6915508949485868\n",
            "Epoch 18127  \tTraining Loss: 0.6915577442372786\tValidation Loss: 0.6915508327892546\n",
            "Epoch 18128  \tTraining Loss: 0.6915576825892661\tValidation Loss: 0.6915507706291247\n",
            "Epoch 18129  \tTraining Loss: 0.6915576209404559\tValidation Loss: 0.6915507084681973\n",
            "Epoch 18130  \tTraining Loss: 0.691557559290848\tValidation Loss: 0.691550646306472\n",
            "Epoch 18131  \tTraining Loss: 0.6915574976404419\tValidation Loss: 0.691550584143949\n",
            "Epoch 18132  \tTraining Loss: 0.6915574359892381\tValidation Loss: 0.6915505219806283\n",
            "Epoch 18133  \tTraining Loss: 0.6915573743372363\tValidation Loss: 0.6915504598165099\n",
            "Epoch 18134  \tTraining Loss: 0.6915573126844364\tValidation Loss: 0.6915503976515934\n",
            "Epoch 18135  \tTraining Loss: 0.6915572510308384\tValidation Loss: 0.6915503354858793\n",
            "Epoch 18136  \tTraining Loss: 0.6915571893764425\tValidation Loss: 0.6915502733193671\n",
            "Epoch 18137  \tTraining Loss: 0.6915571277212484\tValidation Loss: 0.6915502111520572\n",
            "Epoch 18138  \tTraining Loss: 0.6915570660652562\tValidation Loss: 0.6915501489839491\n",
            "Epoch 18139  \tTraining Loss: 0.6915570044084658\tValidation Loss: 0.6915500868150432\n",
            "Epoch 18140  \tTraining Loss: 0.6915569427508772\tValidation Loss: 0.691550024645339\n",
            "Epoch 18141  \tTraining Loss: 0.6915568810924905\tValidation Loss: 0.6915499624748369\n",
            "Epoch 18142  \tTraining Loss: 0.6915568194333054\tValidation Loss: 0.6915499003035367\n",
            "Epoch 18143  \tTraining Loss: 0.6915567577733219\tValidation Loss: 0.6915498381314384\n",
            "Epoch 18144  \tTraining Loss: 0.6915566961125402\tValidation Loss: 0.6915497759585418\n",
            "Epoch 18145  \tTraining Loss: 0.6915566344509602\tValidation Loss: 0.6915497137848472\n",
            "Epoch 18146  \tTraining Loss: 0.6915565727885816\tValidation Loss: 0.6915496516103543\n",
            "Epoch 18147  \tTraining Loss: 0.6915565111254047\tValidation Loss: 0.6915495894350631\n",
            "Epoch 18148  \tTraining Loss: 0.6915564494614291\tValidation Loss: 0.6915495272589736\n",
            "Epoch 18149  \tTraining Loss: 0.6915563877966552\tValidation Loss: 0.6915494650820857\n",
            "Epoch 18150  \tTraining Loss: 0.6915563261310828\tValidation Loss: 0.6915494029043995\n",
            "Epoch 18151  \tTraining Loss: 0.6915562644647117\tValidation Loss: 0.6915493407259149\n",
            "Epoch 18152  \tTraining Loss: 0.691556202797542\tValidation Loss: 0.691549278546632\n",
            "Epoch 18153  \tTraining Loss: 0.6915561411295739\tValidation Loss: 0.6915492163665504\n",
            "Epoch 18154  \tTraining Loss: 0.6915560794608068\tValidation Loss: 0.6915491541856703\n",
            "Epoch 18155  \tTraining Loss: 0.6915560177912411\tValidation Loss: 0.691549092003992\n",
            "Epoch 18156  \tTraining Loss: 0.6915559561208767\tValidation Loss: 0.6915490298215148\n",
            "Epoch 18157  \tTraining Loss: 0.6915558944497134\tValidation Loss: 0.6915489676382391\n",
            "Epoch 18158  \tTraining Loss: 0.6915558327777513\tValidation Loss: 0.6915489054541648\n",
            "Epoch 18159  \tTraining Loss: 0.6915557711049904\tValidation Loss: 0.6915488432692918\n",
            "Epoch 18160  \tTraining Loss: 0.6915557094314307\tValidation Loss: 0.69154878108362\n",
            "Epoch 18161  \tTraining Loss: 0.6915556477570722\tValidation Loss: 0.6915487188971496\n",
            "Epoch 18162  \tTraining Loss: 0.6915555860819146\tValidation Loss: 0.6915486567098804\n",
            "Epoch 18163  \tTraining Loss: 0.691555524405958\tValidation Loss: 0.6915485945218124\n",
            "Epoch 18164  \tTraining Loss: 0.6915554627292024\tValidation Loss: 0.6915485323329456\n",
            "Epoch 18165  \tTraining Loss: 0.6915554010516478\tValidation Loss: 0.6915484701432799\n",
            "Epoch 18166  \tTraining Loss: 0.691555339373294\tValidation Loss: 0.6915484079528152\n",
            "Epoch 18167  \tTraining Loss: 0.6915552776941413\tValidation Loss: 0.6915483457615518\n",
            "Epoch 18168  \tTraining Loss: 0.6915552160141892\tValidation Loss: 0.691548283569489\n",
            "Epoch 18169  \tTraining Loss: 0.6915551543334382\tValidation Loss: 0.6915482213766276\n",
            "Epoch 18170  \tTraining Loss: 0.6915550926518879\tValidation Loss: 0.6915481591829671\n",
            "Epoch 18171  \tTraining Loss: 0.6915550309695383\tValidation Loss: 0.6915480969885075\n",
            "Epoch 18172  \tTraining Loss: 0.6915549692863894\tValidation Loss: 0.6915480347932488\n",
            "Epoch 18173  \tTraining Loss: 0.6915549076024411\tValidation Loss: 0.6915479725971909\n",
            "Epoch 18174  \tTraining Loss: 0.6915548459176936\tValidation Loss: 0.6915479104003339\n",
            "Epoch 18175  \tTraining Loss: 0.6915547842321467\tValidation Loss: 0.6915478482026776\n",
            "Epoch 18176  \tTraining Loss: 0.6915547225458004\tValidation Loss: 0.6915477860042222\n",
            "Epoch 18177  \tTraining Loss: 0.6915546608586547\tValidation Loss: 0.6915477238049675\n",
            "Epoch 18178  \tTraining Loss: 0.6915545991707093\tValidation Loss: 0.6915476616049134\n",
            "Epoch 18179  \tTraining Loss: 0.6915545374819645\tValidation Loss: 0.69154759940406\n",
            "Epoch 18180  \tTraining Loss: 0.6915544757924202\tValidation Loss: 0.6915475372024072\n",
            "Epoch 18181  \tTraining Loss: 0.6915544141020763\tValidation Loss: 0.6915474749999552\n",
            "Epoch 18182  \tTraining Loss: 0.6915543524109328\tValidation Loss: 0.6915474127967036\n",
            "Epoch 18183  \tTraining Loss: 0.6915542907189896\tValidation Loss: 0.6915473505926525\n",
            "Epoch 18184  \tTraining Loss: 0.6915542290262467\tValidation Loss: 0.6915472883878019\n",
            "Epoch 18185  \tTraining Loss: 0.6915541673327043\tValidation Loss: 0.6915472261821518\n",
            "Epoch 18186  \tTraining Loss: 0.6915541056383618\tValidation Loss: 0.6915471639757023\n",
            "Epoch 18187  \tTraining Loss: 0.6915540439432197\tValidation Loss: 0.6915471017684529\n",
            "Epoch 18188  \tTraining Loss: 0.6915539822472778\tValidation Loss: 0.691547039560404\n",
            "Epoch 18189  \tTraining Loss: 0.691553920550536\tValidation Loss: 0.6915469773515555\n",
            "Epoch 18190  \tTraining Loss: 0.6915538588529944\tValidation Loss: 0.6915469151419071\n",
            "Epoch 18191  \tTraining Loss: 0.6915537971546528\tValidation Loss: 0.6915468529314591\n",
            "Epoch 18192  \tTraining Loss: 0.6915537354555112\tValidation Loss: 0.6915467907202113\n",
            "Epoch 18193  \tTraining Loss: 0.6915536737555696\tValidation Loss: 0.6915467285081638\n",
            "Epoch 18194  \tTraining Loss: 0.6915536120548282\tValidation Loss: 0.6915466662953162\n",
            "Epoch 18195  \tTraining Loss: 0.6915535503532866\tValidation Loss: 0.691546604081669\n",
            "Epoch 18196  \tTraining Loss: 0.6915534886509449\tValidation Loss: 0.6915465418672219\n",
            "Epoch 18197  \tTraining Loss: 0.6915534269478031\tValidation Loss: 0.6915464796519747\n",
            "Epoch 18198  \tTraining Loss: 0.6915533652438611\tValidation Loss: 0.6915464174359275\n",
            "Epoch 18199  \tTraining Loss: 0.691553303539119\tValidation Loss: 0.6915463552190803\n",
            "Epoch 18200  \tTraining Loss: 0.6915532418335767\tValidation Loss: 0.6915462930014332\n",
            "Epoch 18201  \tTraining Loss: 0.691553180127234\tValidation Loss: 0.6915462307829859\n",
            "Epoch 18202  \tTraining Loss: 0.6915531184200912\tValidation Loss: 0.6915461685637386\n",
            "Epoch 18203  \tTraining Loss: 0.691553056712148\tValidation Loss: 0.6915461063436911\n",
            "Epoch 18204  \tTraining Loss: 0.6915529950034043\tValidation Loss: 0.6915460441228435\n",
            "Epoch 18205  \tTraining Loss: 0.6915529332938604\tValidation Loss: 0.6915459819011956\n",
            "Epoch 18206  \tTraining Loss: 0.691552871583516\tValidation Loss: 0.6915459196787476\n",
            "Epoch 18207  \tTraining Loss: 0.6915528098723711\tValidation Loss: 0.6915458574554992\n",
            "Epoch 18208  \tTraining Loss: 0.6915527481604258\tValidation Loss: 0.6915457952314504\n",
            "Epoch 18209  \tTraining Loss: 0.6915526864476798\tValidation Loss: 0.6915457330066015\n",
            "Epoch 18210  \tTraining Loss: 0.6915526247341334\tValidation Loss: 0.6915456707809521\n",
            "Epoch 18211  \tTraining Loss: 0.6915525630197864\tValidation Loss: 0.6915456085545022\n",
            "Epoch 18212  \tTraining Loss: 0.6915525013046387\tValidation Loss: 0.6915455463272521\n",
            "Epoch 18213  \tTraining Loss: 0.6915524395886905\tValidation Loss: 0.6915454840992012\n",
            "Epoch 18214  \tTraining Loss: 0.6915523778719416\tValidation Loss: 0.6915454218703502\n",
            "Epoch 18215  \tTraining Loss: 0.6915523161543918\tValidation Loss: 0.6915453596406984\n",
            "Epoch 18216  \tTraining Loss: 0.6915522544360413\tValidation Loss: 0.691545297410246\n",
            "Epoch 18217  \tTraining Loss: 0.6915521927168901\tValidation Loss: 0.691545235178993\n",
            "Epoch 18218  \tTraining Loss: 0.6915521309969379\tValidation Loss: 0.6915451729469395\n",
            "Epoch 18219  \tTraining Loss: 0.691552069276185\tValidation Loss: 0.6915451107140852\n",
            "Epoch 18220  \tTraining Loss: 0.6915520075546312\tValidation Loss: 0.6915450484804302\n",
            "Epoch 18221  \tTraining Loss: 0.6915519458322764\tValidation Loss: 0.6915449862459745\n",
            "Epoch 18222  \tTraining Loss: 0.6915518841091207\tValidation Loss: 0.691544924010718\n",
            "Epoch 18223  \tTraining Loss: 0.691551822385164\tValidation Loss: 0.6915448617746607\n",
            "Epoch 18224  \tTraining Loss: 0.6915517606604061\tValidation Loss: 0.6915447995378026\n",
            "Epoch 18225  \tTraining Loss: 0.6915516989348472\tValidation Loss: 0.6915447373001435\n",
            "Epoch 18226  \tTraining Loss: 0.6915516372084874\tValidation Loss: 0.6915446750616836\n",
            "Epoch 18227  \tTraining Loss: 0.6915515754813263\tValidation Loss: 0.6915446128224227\n",
            "Epoch 18228  \tTraining Loss: 0.6915515137533641\tValidation Loss: 0.691544550582361\n",
            "Epoch 18229  \tTraining Loss: 0.6915514520246008\tValidation Loss: 0.691544488341498\n",
            "Epoch 18230  \tTraining Loss: 0.6915513902950361\tValidation Loss: 0.6915444260998342\n",
            "Epoch 18231  \tTraining Loss: 0.6915513285646703\tValidation Loss: 0.6915443638573692\n",
            "Epoch 18232  \tTraining Loss: 0.6915512668335031\tValidation Loss: 0.6915443016141031\n",
            "Epoch 18233  \tTraining Loss: 0.6915512051015346\tValidation Loss: 0.6915442393700358\n",
            "Epoch 18234  \tTraining Loss: 0.6915511433687646\tValidation Loss: 0.6915441771251675\n",
            "Epoch 18235  \tTraining Loss: 0.6915510816351933\tValidation Loss: 0.691544114879498\n",
            "Epoch 18236  \tTraining Loss: 0.6915510199008206\tValidation Loss: 0.691544052633027\n",
            "Epoch 18237  \tTraining Loss: 0.6915509581656465\tValidation Loss: 0.6915439903857548\n",
            "Epoch 18238  \tTraining Loss: 0.6915508964296707\tValidation Loss: 0.6915439281376814\n",
            "Epoch 18239  \tTraining Loss: 0.6915508346928935\tValidation Loss: 0.6915438658888067\n",
            "Epoch 18240  \tTraining Loss: 0.6915507729553148\tValidation Loss: 0.6915438036391305\n",
            "Epoch 18241  \tTraining Loss: 0.6915507112169343\tValidation Loss: 0.6915437413886528\n",
            "Epoch 18242  \tTraining Loss: 0.6915506494777524\tValidation Loss: 0.6915436791373738\n",
            "Epoch 18243  \tTraining Loss: 0.6915505877377686\tValidation Loss: 0.6915436168852933\n",
            "Epoch 18244  \tTraining Loss: 0.6915505259969832\tValidation Loss: 0.6915435546324112\n",
            "Epoch 18245  \tTraining Loss: 0.691550464255396\tValidation Loss: 0.6915434923787277\n",
            "Epoch 18246  \tTraining Loss: 0.6915504025130073\tValidation Loss: 0.6915434301242427\n",
            "Epoch 18247  \tTraining Loss: 0.6915503407698165\tValidation Loss: 0.6915433678689559\n",
            "Epoch 18248  \tTraining Loss: 0.691550279025824\tValidation Loss: 0.6915433056128675\n",
            "Epoch 18249  \tTraining Loss: 0.6915502172810295\tValidation Loss: 0.6915432433559774\n",
            "Epoch 18250  \tTraining Loss: 0.6915501555354333\tValidation Loss: 0.6915431810982856\n",
            "Epoch 18251  \tTraining Loss: 0.691550093789035\tValidation Loss: 0.691543118839792\n",
            "Epoch 18252  \tTraining Loss: 0.6915500320418349\tValidation Loss: 0.6915430565804968\n",
            "Epoch 18253  \tTraining Loss: 0.6915499702938327\tValidation Loss: 0.6915429943203996\n",
            "Epoch 18254  \tTraining Loss: 0.6915499085450284\tValidation Loss: 0.6915429320595006\n",
            "Epoch 18255  \tTraining Loss: 0.691549846795422\tValidation Loss: 0.6915428697977997\n",
            "Epoch 18256  \tTraining Loss: 0.6915497850450136\tValidation Loss: 0.691542807535297\n",
            "Epoch 18257  \tTraining Loss: 0.691549723293803\tValidation Loss: 0.6915427452719922\n",
            "Epoch 18258  \tTraining Loss: 0.6915496615417903\tValidation Loss: 0.6915426830078856\n",
            "Epoch 18259  \tTraining Loss: 0.6915495997889753\tValidation Loss: 0.6915426207429769\n",
            "Epoch 18260  \tTraining Loss: 0.6915495380353581\tValidation Loss: 0.6915425584772661\n",
            "Epoch 18261  \tTraining Loss: 0.6915494762809388\tValidation Loss: 0.6915424962107531\n",
            "Epoch 18262  \tTraining Loss: 0.6915494145257169\tValidation Loss: 0.6915424339434382\n",
            "Epoch 18263  \tTraining Loss: 0.6915493527696929\tValidation Loss: 0.6915423716753213\n",
            "Epoch 18264  \tTraining Loss: 0.6915492910128663\tValidation Loss: 0.6915423094064019\n",
            "Epoch 18265  \tTraining Loss: 0.6915492292552374\tValidation Loss: 0.6915422471366803\n",
            "Epoch 18266  \tTraining Loss: 0.6915491674968062\tValidation Loss: 0.6915421848661566\n",
            "Epoch 18267  \tTraining Loss: 0.6915491057375723\tValidation Loss: 0.6915421225948306\n",
            "Epoch 18268  \tTraining Loss: 0.691549043977536\tValidation Loss: 0.6915420603227023\n",
            "Epoch 18269  \tTraining Loss: 0.6915489822166971\tValidation Loss: 0.6915419980497715\n",
            "Epoch 18270  \tTraining Loss: 0.6915489204550558\tValidation Loss: 0.6915419357760384\n",
            "Epoch 18271  \tTraining Loss: 0.6915488586926117\tValidation Loss: 0.691541873501503\n",
            "Epoch 18272  \tTraining Loss: 0.691548796929365\tValidation Loss: 0.691541811226165\n",
            "Epoch 18273  \tTraining Loss: 0.6915487351653157\tValidation Loss: 0.6915417489500246\n",
            "Epoch 18274  \tTraining Loss: 0.6915486734004636\tValidation Loss: 0.6915416866730816\n",
            "Epoch 18275  \tTraining Loss: 0.6915486116348087\tValidation Loss: 0.6915416243953361\n",
            "Epoch 18276  \tTraining Loss: 0.6915485498683511\tValidation Loss: 0.691541562116788\n",
            "Epoch 18277  \tTraining Loss: 0.6915484881010907\tValidation Loss: 0.6915414998374372\n",
            "Epoch 18278  \tTraining Loss: 0.6915484263330275\tValidation Loss: 0.6915414375572839\n",
            "Epoch 18279  \tTraining Loss: 0.6915483645641614\tValidation Loss: 0.6915413752763279\n",
            "Epoch 18280  \tTraining Loss: 0.6915483027944923\tValidation Loss: 0.6915413129945691\n",
            "Epoch 18281  \tTraining Loss: 0.6915482410240203\tValidation Loss: 0.6915412507120076\n",
            "Epoch 18282  \tTraining Loss: 0.6915481792527454\tValidation Loss: 0.6915411884286433\n",
            "Epoch 18283  \tTraining Loss: 0.6915481174806674\tValidation Loss: 0.6915411261444762\n",
            "Epoch 18284  \tTraining Loss: 0.6915480557077863\tValidation Loss: 0.6915410638595062\n",
            "Epoch 18285  \tTraining Loss: 0.6915479939341022\tValidation Loss: 0.6915410015737333\n",
            "Epoch 18286  \tTraining Loss: 0.691547932159615\tValidation Loss: 0.6915409392871577\n",
            "Epoch 18287  \tTraining Loss: 0.6915478703843246\tValidation Loss: 0.6915408769997788\n",
            "Epoch 18288  \tTraining Loss: 0.6915478086082311\tValidation Loss: 0.6915408147115972\n",
            "Epoch 18289  \tTraining Loss: 0.6915477468313342\tValidation Loss: 0.6915407524226124\n",
            "Epoch 18290  \tTraining Loss: 0.6915476850536342\tValidation Loss: 0.6915406901328246\n",
            "Epoch 18291  \tTraining Loss: 0.691547623275131\tValidation Loss: 0.6915406278422337\n",
            "Epoch 18292  \tTraining Loss: 0.6915475614958243\tValidation Loss: 0.6915405655508398\n",
            "Epoch 18293  \tTraining Loss: 0.6915474997157143\tValidation Loss: 0.6915405032586426\n",
            "Epoch 18294  \tTraining Loss: 0.6915474379348009\tValidation Loss: 0.6915404409656424\n",
            "Epoch 18295  \tTraining Loss: 0.6915473761530841\tValidation Loss: 0.6915403786718388\n",
            "Epoch 18296  \tTraining Loss: 0.6915473143705639\tValidation Loss: 0.6915403163772319\n",
            "Epoch 18297  \tTraining Loss: 0.6915472525872403\tValidation Loss: 0.6915402540818217\n",
            "Epoch 18298  \tTraining Loss: 0.691547190803113\tValidation Loss: 0.6915401917856084\n",
            "Epoch 18299  \tTraining Loss: 0.6915471290181822\tValidation Loss: 0.6915401294885916\n",
            "Epoch 18300  \tTraining Loss: 0.6915470672324477\tValidation Loss: 0.6915400671907715\n",
            "Epoch 18301  \tTraining Loss: 0.6915470054459097\tValidation Loss: 0.6915400048921478\n",
            "Epoch 18302  \tTraining Loss: 0.6915469436585682\tValidation Loss: 0.6915399425927208\n",
            "Epoch 18303  \tTraining Loss: 0.6915468818704228\tValidation Loss: 0.6915398802924902\n",
            "Epoch 18304  \tTraining Loss: 0.6915468200814737\tValidation Loss: 0.6915398179914561\n",
            "Epoch 18305  \tTraining Loss: 0.691546758291721\tValidation Loss: 0.6915397556896187\n",
            "Epoch 18306  \tTraining Loss: 0.6915466965011644\tValidation Loss: 0.6915396933869774\n",
            "Epoch 18307  \tTraining Loss: 0.6915466347098038\tValidation Loss: 0.6915396310835327\n",
            "Epoch 18308  \tTraining Loss: 0.6915465729176395\tValidation Loss: 0.6915395687792841\n",
            "Epoch 18309  \tTraining Loss: 0.6915465111246715\tValidation Loss: 0.6915395064742319\n",
            "Epoch 18310  \tTraining Loss: 0.6915464493308994\tValidation Loss: 0.691539444168376\n",
            "Epoch 18311  \tTraining Loss: 0.6915463875363232\tValidation Loss: 0.6915393818617165\n",
            "Epoch 18312  \tTraining Loss: 0.6915463257409433\tValidation Loss: 0.6915393195542531\n",
            "Epoch 18313  \tTraining Loss: 0.6915462639447592\tValidation Loss: 0.6915392572459856\n",
            "Epoch 18314  \tTraining Loss: 0.6915462021477711\tValidation Loss: 0.6915391949369145\n",
            "Epoch 18315  \tTraining Loss: 0.6915461403499789\tValidation Loss: 0.6915391326270396\n",
            "Epoch 18316  \tTraining Loss: 0.6915460785513825\tValidation Loss: 0.6915390703163606\n",
            "Epoch 18317  \tTraining Loss: 0.6915460167519819\tValidation Loss: 0.6915390080048778\n",
            "Epoch 18318  \tTraining Loss: 0.6915459549517772\tValidation Loss: 0.6915389456925908\n",
            "Epoch 18319  \tTraining Loss: 0.6915458931507683\tValidation Loss: 0.6915388833795\n",
            "Epoch 18320  \tTraining Loss: 0.6915458313489551\tValidation Loss: 0.6915388210656049\n",
            "Epoch 18321  \tTraining Loss: 0.6915457695463377\tValidation Loss: 0.6915387587509059\n",
            "Epoch 18322  \tTraining Loss: 0.6915457077429158\tValidation Loss: 0.6915386964354027\n",
            "Epoch 18323  \tTraining Loss: 0.6915456459386897\tValidation Loss: 0.6915386341190952\n",
            "Epoch 18324  \tTraining Loss: 0.6915455841336592\tValidation Loss: 0.6915385718019837\n",
            "Epoch 18325  \tTraining Loss: 0.6915455223278242\tValidation Loss: 0.691538509484068\n",
            "Epoch 18326  \tTraining Loss: 0.6915454605211847\tValidation Loss: 0.6915384471653478\n",
            "Epoch 18327  \tTraining Loss: 0.6915453987137408\tValidation Loss: 0.6915383848458236\n",
            "Epoch 18328  \tTraining Loss: 0.6915453369054922\tValidation Loss: 0.691538322525495\n",
            "Epoch 18329  \tTraining Loss: 0.6915452750964394\tValidation Loss: 0.6915382602043619\n",
            "Epoch 18330  \tTraining Loss: 0.6915452132865816\tValidation Loss: 0.6915381978824245\n",
            "Epoch 18331  \tTraining Loss: 0.6915451514759193\tValidation Loss: 0.6915381355596826\n",
            "Epoch 18332  \tTraining Loss: 0.6915450896644525\tValidation Loss: 0.6915380732361364\n",
            "Epoch 18333  \tTraining Loss: 0.6915450278521809\tValidation Loss: 0.6915380109117856\n",
            "Epoch 18334  \tTraining Loss: 0.6915449660391044\tValidation Loss: 0.6915379485866302\n",
            "Epoch 18335  \tTraining Loss: 0.6915449042252233\tValidation Loss: 0.6915378862606703\n",
            "Epoch 18336  \tTraining Loss: 0.6915448424105374\tValidation Loss: 0.6915378239339057\n",
            "Epoch 18337  \tTraining Loss: 0.6915447805950466\tValidation Loss: 0.6915377616063366\n",
            "Epoch 18338  \tTraining Loss: 0.691544718778751\tValidation Loss: 0.6915376992779628\n",
            "Epoch 18339  \tTraining Loss: 0.6915446569616505\tValidation Loss: 0.6915376369487843\n",
            "Epoch 18340  \tTraining Loss: 0.6915445951437449\tValidation Loss: 0.6915375746188013\n",
            "Epoch 18341  \tTraining Loss: 0.6915445333250346\tValidation Loss: 0.6915375122880133\n",
            "Epoch 18342  \tTraining Loss: 0.6915444715055192\tValidation Loss: 0.6915374499564205\n",
            "Epoch 18343  \tTraining Loss: 0.6915444096851987\tValidation Loss: 0.6915373876240228\n",
            "Epoch 18344  \tTraining Loss: 0.691544347864073\tValidation Loss: 0.6915373252908203\n",
            "Epoch 18345  \tTraining Loss: 0.6915442860421425\tValidation Loss: 0.691537262956813\n",
            "Epoch 18346  \tTraining Loss: 0.6915442242194066\tValidation Loss: 0.6915372006220007\n",
            "Epoch 18347  \tTraining Loss: 0.6915441623958658\tValidation Loss: 0.6915371382863835\n",
            "Epoch 18348  \tTraining Loss: 0.6915441005715196\tValidation Loss: 0.6915370759499612\n",
            "Epoch 18349  \tTraining Loss: 0.6915440387463682\tValidation Loss: 0.6915370136127339\n",
            "Epoch 18350  \tTraining Loss: 0.6915439769204116\tValidation Loss: 0.6915369512747015\n",
            "Epoch 18351  \tTraining Loss: 0.6915439150936495\tValidation Loss: 0.6915368889358642\n",
            "Epoch 18352  \tTraining Loss: 0.6915438532660823\tValidation Loss: 0.6915368265962215\n",
            "Epoch 18353  \tTraining Loss: 0.6915437914377096\tValidation Loss: 0.6915367642557737\n",
            "Epoch 18354  \tTraining Loss: 0.6915437296085315\tValidation Loss: 0.6915367019145208\n",
            "Epoch 18355  \tTraining Loss: 0.691543667778548\tValidation Loss: 0.6915366395724626\n",
            "Epoch 18356  \tTraining Loss: 0.6915436059477589\tValidation Loss: 0.691536577229599\n",
            "Epoch 18357  \tTraining Loss: 0.6915435441161645\tValidation Loss: 0.6915365148859304\n",
            "Epoch 18358  \tTraining Loss: 0.6915434822837643\tValidation Loss: 0.6915364525414562\n",
            "Epoch 18359  \tTraining Loss: 0.6915434204505587\tValidation Loss: 0.6915363901961767\n",
            "Epoch 18360  \tTraining Loss: 0.6915433586165476\tValidation Loss: 0.6915363278500919\n",
            "Epoch 18361  \tTraining Loss: 0.6915432967817308\tValidation Loss: 0.6915362655032016\n",
            "Epoch 18362  \tTraining Loss: 0.6915432349461081\tValidation Loss: 0.6915362031555058\n",
            "Epoch 18363  \tTraining Loss: 0.6915431731096798\tValidation Loss: 0.6915361408070045\n",
            "Epoch 18364  \tTraining Loss: 0.6915431112724458\tValidation Loss: 0.6915360784576977\n",
            "Epoch 18365  \tTraining Loss: 0.691543049434406\tValidation Loss: 0.6915360161075854\n",
            "Epoch 18366  \tTraining Loss: 0.6915429875955605\tValidation Loss: 0.6915359537566674\n",
            "Epoch 18367  \tTraining Loss: 0.691542925755909\tValidation Loss: 0.6915358914049438\n",
            "Epoch 18368  \tTraining Loss: 0.6915428639154517\tValidation Loss: 0.6915358290524145\n",
            "Epoch 18369  \tTraining Loss: 0.6915428020741885\tValidation Loss: 0.6915357666990795\n",
            "Epoch 18370  \tTraining Loss: 0.6915427402321194\tValidation Loss: 0.6915357043449387\n",
            "Epoch 18371  \tTraining Loss: 0.6915426783892442\tValidation Loss: 0.6915356419899923\n",
            "Epoch 18372  \tTraining Loss: 0.691542616545563\tValidation Loss: 0.69153557963424\n",
            "Epoch 18373  \tTraining Loss: 0.6915425547010757\tValidation Loss: 0.6915355172776818\n",
            "Epoch 18374  \tTraining Loss: 0.6915424928557825\tValidation Loss: 0.6915354549203179\n",
            "Epoch 18375  \tTraining Loss: 0.6915424310096832\tValidation Loss: 0.6915353925621478\n",
            "Epoch 18376  \tTraining Loss: 0.6915423691627776\tValidation Loss: 0.691535330203172\n",
            "Epoch 18377  \tTraining Loss: 0.6915423073150657\tValidation Loss: 0.6915352678433903\n",
            "Epoch 18378  \tTraining Loss: 0.691542245466548\tValidation Loss: 0.6915352054828022\n",
            "Epoch 18379  \tTraining Loss: 0.6915421836172236\tValidation Loss: 0.6915351431214085\n",
            "Epoch 18380  \tTraining Loss: 0.6915421217670932\tValidation Loss: 0.6915350807592086\n",
            "Epoch 18381  \tTraining Loss: 0.6915420599161564\tValidation Loss: 0.6915350183962025\n",
            "Epoch 18382  \tTraining Loss: 0.6915419980644132\tValidation Loss: 0.6915349560323902\n",
            "Epoch 18383  \tTraining Loss: 0.6915419362118637\tValidation Loss: 0.6915348936677719\n",
            "Epoch 18384  \tTraining Loss: 0.6915418743585078\tValidation Loss: 0.6915348313023473\n",
            "Epoch 18385  \tTraining Loss: 0.6915418125043453\tValidation Loss: 0.6915347689361164\n",
            "Epoch 18386  \tTraining Loss: 0.6915417506493764\tValidation Loss: 0.6915347065690793\n",
            "Epoch 18387  \tTraining Loss: 0.691541688793601\tValidation Loss: 0.691534644201236\n",
            "Epoch 18388  \tTraining Loss: 0.691541626937019\tValidation Loss: 0.6915345818325862\n",
            "Epoch 18389  \tTraining Loss: 0.6915415650796305\tValidation Loss: 0.69153451946313\n",
            "Epoch 18390  \tTraining Loss: 0.6915415032214353\tValidation Loss: 0.6915344570928675\n",
            "Epoch 18391  \tTraining Loss: 0.6915414413624336\tValidation Loss: 0.6915343947217986\n",
            "Epoch 18392  \tTraining Loss: 0.6915413795026251\tValidation Loss: 0.6915343323499231\n",
            "Epoch 18393  \tTraining Loss: 0.6915413176420098\tValidation Loss: 0.6915342699772412\n",
            "Epoch 18394  \tTraining Loss: 0.6915412557805879\tValidation Loss: 0.6915342076037526\n",
            "Epoch 18395  \tTraining Loss: 0.691541193918359\tValidation Loss: 0.6915341452294577\n",
            "Epoch 18396  \tTraining Loss: 0.6915411320553233\tValidation Loss: 0.6915340828543558\n",
            "Epoch 18397  \tTraining Loss: 0.6915410701914809\tValidation Loss: 0.6915340204784476\n",
            "Epoch 18398  \tTraining Loss: 0.6915410083268315\tValidation Loss: 0.6915339581017326\n",
            "Epoch 18399  \tTraining Loss: 0.6915409464613752\tValidation Loss: 0.6915338957242109\n",
            "Epoch 18400  \tTraining Loss: 0.6915408845951121\tValidation Loss: 0.6915338333458824\n",
            "Epoch 18401  \tTraining Loss: 0.6915408227280418\tValidation Loss: 0.6915337709667472\n",
            "Epoch 18402  \tTraining Loss: 0.6915407608601646\tValidation Loss: 0.6915337085868052\n",
            "Epoch 18403  \tTraining Loss: 0.6915406989914803\tValidation Loss: 0.6915336462060562\n",
            "Epoch 18404  \tTraining Loss: 0.6915406371219889\tValidation Loss: 0.6915335838245004\n",
            "Epoch 18405  \tTraining Loss: 0.6915405752516904\tValidation Loss: 0.6915335214421376\n",
            "Epoch 18406  \tTraining Loss: 0.6915405133805846\tValidation Loss: 0.6915334590589681\n",
            "Epoch 18407  \tTraining Loss: 0.6915404515086717\tValidation Loss: 0.6915333966749914\n",
            "Epoch 18408  \tTraining Loss: 0.6915403896359517\tValidation Loss: 0.6915333342902078\n",
            "Epoch 18409  \tTraining Loss: 0.6915403277624242\tValidation Loss: 0.6915332719046171\n",
            "Epoch 18410  \tTraining Loss: 0.6915402658880896\tValidation Loss: 0.6915332095182193\n",
            "Epoch 18411  \tTraining Loss: 0.6915402040129477\tValidation Loss: 0.6915331471310144\n",
            "Epoch 18412  \tTraining Loss: 0.6915401421369982\tValidation Loss: 0.6915330847430025\n",
            "Epoch 18413  \tTraining Loss: 0.6915400802602415\tValidation Loss: 0.6915330223541832\n",
            "Epoch 18414  \tTraining Loss: 0.6915400183826773\tValidation Loss: 0.6915329599645568\n",
            "Epoch 18415  \tTraining Loss: 0.6915399565043058\tValidation Loss: 0.6915328975741231\n",
            "Epoch 18416  \tTraining Loss: 0.6915398946251265\tValidation Loss: 0.6915328351828822\n",
            "Epoch 18417  \tTraining Loss: 0.6915398327451399\tValidation Loss: 0.6915327727908338\n",
            "Epoch 18418  \tTraining Loss: 0.6915397708643456\tValidation Loss: 0.6915327103979781\n",
            "Epoch 18419  \tTraining Loss: 0.6915397089827438\tValidation Loss: 0.6915326480043152\n",
            "Epoch 18420  \tTraining Loss: 0.6915396471003342\tValidation Loss: 0.6915325856098448\n",
            "Epoch 18421  \tTraining Loss: 0.691539585217117\tValidation Loss: 0.691532523214567\n",
            "Epoch 18422  \tTraining Loss: 0.6915395233330922\tValidation Loss: 0.6915324608184816\n",
            "Epoch 18423  \tTraining Loss: 0.6915394614482597\tValidation Loss: 0.6915323984215886\n",
            "Epoch 18424  \tTraining Loss: 0.6915393995626193\tValidation Loss: 0.6915323360238883\n",
            "Epoch 18425  \tTraining Loss: 0.6915393376761712\tValidation Loss: 0.6915322736253803\n",
            "Epoch 18426  \tTraining Loss: 0.6915392757889152\tValidation Loss: 0.6915322112260646\n",
            "Epoch 18427  \tTraining Loss: 0.6915392139008513\tValidation Loss: 0.6915321488259414\n",
            "Epoch 18428  \tTraining Loss: 0.6915391520119796\tValidation Loss: 0.6915320864250104\n",
            "Epoch 18429  \tTraining Loss: 0.6915390901222999\tValidation Loss: 0.6915320240232717\n",
            "Epoch 18430  \tTraining Loss: 0.6915390282318122\tValidation Loss: 0.6915319616207253\n",
            "Epoch 18431  \tTraining Loss: 0.6915389663405166\tValidation Loss: 0.691531899217371\n",
            "Epoch 18432  \tTraining Loss: 0.6915389044484128\tValidation Loss: 0.691531836813209\n",
            "Epoch 18433  \tTraining Loss: 0.6915388425555011\tValidation Loss: 0.691531774408239\n",
            "Epoch 18434  \tTraining Loss: 0.6915387806617811\tValidation Loss: 0.6915317120024613\n",
            "Epoch 18435  \tTraining Loss: 0.6915387187672531\tValidation Loss: 0.6915316495958755\n",
            "Epoch 18436  \tTraining Loss: 0.691538656871917\tValidation Loss: 0.6915315871884818\n",
            "Epoch 18437  \tTraining Loss: 0.6915385949757725\tValidation Loss: 0.6915315247802802\n",
            "Epoch 18438  \tTraining Loss: 0.69153853307882\tValidation Loss: 0.6915314623712706\n",
            "Epoch 18439  \tTraining Loss: 0.691538471181059\tValidation Loss: 0.6915313999614529\n",
            "Epoch 18440  \tTraining Loss: 0.6915384092824898\tValidation Loss: 0.6915313375508271\n",
            "Epoch 18441  \tTraining Loss: 0.6915383473831122\tValidation Loss: 0.691531275139393\n",
            "Epoch 18442  \tTraining Loss: 0.6915382854829263\tValidation Loss: 0.6915312127271509\n",
            "Epoch 18443  \tTraining Loss: 0.6915382235819318\tValidation Loss: 0.6915311503141006\n",
            "Epoch 18444  \tTraining Loss: 0.691538161680129\tValidation Loss: 0.6915310879002421\n",
            "Epoch 18445  \tTraining Loss: 0.6915380997775179\tValidation Loss: 0.6915310254855753\n",
            "Epoch 18446  \tTraining Loss: 0.6915380378740981\tValidation Loss: 0.6915309630701003\n",
            "Epoch 18447  \tTraining Loss: 0.6915379759698697\tValidation Loss: 0.691530900653817\n",
            "Epoch 18448  \tTraining Loss: 0.6915379140648329\tValidation Loss: 0.6915308382367252\n",
            "Epoch 18449  \tTraining Loss: 0.6915378521589873\tValidation Loss: 0.6915307758188249\n",
            "Epoch 18450  \tTraining Loss: 0.6915377902523331\tValidation Loss: 0.6915307134001164\n",
            "Epoch 18451  \tTraining Loss: 0.6915377283448703\tValidation Loss: 0.6915306509805994\n",
            "Epoch 18452  \tTraining Loss: 0.6915376664365986\tValidation Loss: 0.6915305885602739\n",
            "Epoch 18453  \tTraining Loss: 0.6915376045275183\tValidation Loss: 0.6915305261391399\n",
            "Epoch 18454  \tTraining Loss: 0.6915375426176293\tValidation Loss: 0.6915304637171972\n",
            "Epoch 18455  \tTraining Loss: 0.6915374807069314\tValidation Loss: 0.6915304012944461\n",
            "Epoch 18456  \tTraining Loss: 0.6915374187954246\tValidation Loss: 0.6915303388708862\n",
            "Epoch 18457  \tTraining Loss: 0.691537356883109\tValidation Loss: 0.6915302764465178\n",
            "Epoch 18458  \tTraining Loss: 0.6915372949699845\tValidation Loss: 0.6915302140213404\n",
            "Epoch 18459  \tTraining Loss: 0.6915372330560511\tValidation Loss: 0.6915301515953547\n",
            "Epoch 18460  \tTraining Loss: 0.6915371711413086\tValidation Loss: 0.6915300891685601\n",
            "Epoch 18461  \tTraining Loss: 0.6915371092257571\tValidation Loss: 0.6915300267409565\n",
            "Epoch 18462  \tTraining Loss: 0.6915370473093965\tValidation Loss: 0.6915299643125442\n",
            "Epoch 18463  \tTraining Loss: 0.6915369853922267\tValidation Loss: 0.691529901883323\n",
            "Epoch 18464  \tTraining Loss: 0.6915369234742481\tValidation Loss: 0.6915298394532929\n",
            "Epoch 18465  \tTraining Loss: 0.6915368615554601\tValidation Loss: 0.691529777022454\n",
            "Epoch 18466  \tTraining Loss: 0.6915367996358631\tValidation Loss: 0.6915297145908061\n",
            "Epoch 18467  \tTraining Loss: 0.6915367377154568\tValidation Loss: 0.6915296521583492\n",
            "Epoch 18468  \tTraining Loss: 0.6915366757942412\tValidation Loss: 0.691529589725083\n",
            "Epoch 18469  \tTraining Loss: 0.6915366138722163\tValidation Loss: 0.691529527291008\n",
            "Epoch 18470  \tTraining Loss: 0.6915365519493821\tValidation Loss: 0.6915294648561239\n",
            "Epoch 18471  \tTraining Loss: 0.6915364900257386\tValidation Loss: 0.6915294024204307\n",
            "Epoch 18472  \tTraining Loss: 0.6915364281012857\tValidation Loss: 0.6915293399839282\n",
            "Epoch 18473  \tTraining Loss: 0.6915363661760233\tValidation Loss: 0.6915292775466165\n",
            "Epoch 18474  \tTraining Loss: 0.6915363042499516\tValidation Loss: 0.6915292151084956\n",
            "Epoch 18475  \tTraining Loss: 0.6915362423230702\tValidation Loss: 0.6915291526695655\n",
            "Epoch 18476  \tTraining Loss: 0.6915361803953795\tValidation Loss: 0.6915290902298261\n",
            "Epoch 18477  \tTraining Loss: 0.6915361184668791\tValidation Loss: 0.6915290277892773\n",
            "Epoch 18478  \tTraining Loss: 0.6915360565375691\tValidation Loss: 0.6915289653479192\n",
            "Epoch 18479  \tTraining Loss: 0.6915359946074495\tValidation Loss: 0.6915289029057516\n",
            "Epoch 18480  \tTraining Loss: 0.6915359326765202\tValidation Loss: 0.6915288404627746\n",
            "Epoch 18481  \tTraining Loss: 0.6915358707447813\tValidation Loss: 0.6915287780189883\n",
            "Epoch 18482  \tTraining Loss: 0.6915358088122328\tValidation Loss: 0.6915287155743923\n",
            "Epoch 18483  \tTraining Loss: 0.6915357468788742\tValidation Loss: 0.6915286531289868\n",
            "Epoch 18484  \tTraining Loss: 0.6915356849447061\tValidation Loss: 0.6915285906827717\n",
            "Epoch 18485  \tTraining Loss: 0.6915356230097279\tValidation Loss: 0.6915285282357472\n",
            "Epoch 18486  \tTraining Loss: 0.69153556107394\tValidation Loss: 0.6915284657879128\n",
            "Epoch 18487  \tTraining Loss: 0.6915354991373421\tValidation Loss: 0.6915284033392688\n",
            "Epoch 18488  \tTraining Loss: 0.6915354371999345\tValidation Loss: 0.6915283408898151\n",
            "Epoch 18489  \tTraining Loss: 0.6915353752617167\tValidation Loss: 0.6915282784395518\n",
            "Epoch 18490  \tTraining Loss: 0.691535313322689\tValidation Loss: 0.6915282159884787\n",
            "Epoch 18491  \tTraining Loss: 0.6915352513828513\tValidation Loss: 0.6915281535365956\n",
            "Epoch 18492  \tTraining Loss: 0.6915351894422034\tValidation Loss: 0.6915280910839028\n",
            "Epoch 18493  \tTraining Loss: 0.6915351275007455\tValidation Loss: 0.6915280286304002\n",
            "Epoch 18494  \tTraining Loss: 0.6915350655584775\tValidation Loss: 0.6915279661760875\n",
            "Epoch 18495  \tTraining Loss: 0.6915350036153993\tValidation Loss: 0.691527903720965\n",
            "Epoch 18496  \tTraining Loss: 0.691534941671511\tValidation Loss: 0.6915278412650324\n",
            "Epoch 18497  \tTraining Loss: 0.6915348797268124\tValidation Loss: 0.69152777880829\n",
            "Epoch 18498  \tTraining Loss: 0.6915348177813035\tValidation Loss: 0.6915277163507374\n",
            "Epoch 18499  \tTraining Loss: 0.6915347558349844\tValidation Loss: 0.6915276538923748\n",
            "Epoch 18500  \tTraining Loss: 0.6915346938878547\tValidation Loss: 0.691527591433202\n",
            "Epoch 18501  \tTraining Loss: 0.6915346319399148\tValidation Loss: 0.691527528973219\n",
            "Epoch 18502  \tTraining Loss: 0.6915345699911645\tValidation Loss: 0.691527466512426\n",
            "Epoch 18503  \tTraining Loss: 0.6915345080416037\tValidation Loss: 0.6915274040508227\n",
            "Epoch 18504  \tTraining Loss: 0.6915344460912326\tValidation Loss: 0.6915273415884093\n",
            "Epoch 18505  \tTraining Loss: 0.6915343841400509\tValidation Loss: 0.6915272791251855\n",
            "Epoch 18506  \tTraining Loss: 0.6915343221880584\tValidation Loss: 0.6915272166611515\n",
            "Epoch 18507  \tTraining Loss: 0.6915342602352558\tValidation Loss: 0.691527154196307\n",
            "Epoch 18508  \tTraining Loss: 0.6915341982816424\tValidation Loss: 0.6915270917306521\n",
            "Epoch 18509  \tTraining Loss: 0.6915341363272183\tValidation Loss: 0.6915270292641869\n",
            "Epoch 18510  \tTraining Loss: 0.6915340743719834\tValidation Loss: 0.6915269667969113\n",
            "Epoch 18511  \tTraining Loss: 0.691534012415938\tValidation Loss: 0.6915269043288251\n",
            "Epoch 18512  \tTraining Loss: 0.6915339504590818\tValidation Loss: 0.6915268418599283\n",
            "Epoch 18513  \tTraining Loss: 0.6915338885014148\tValidation Loss: 0.6915267793902212\n",
            "Epoch 18514  \tTraining Loss: 0.691533826542937\tValidation Loss: 0.6915267169197035\n",
            "Epoch 18515  \tTraining Loss: 0.6915337645836483\tValidation Loss: 0.691526654448375\n",
            "Epoch 18516  \tTraining Loss: 0.691533702623549\tValidation Loss: 0.691526591976236\n",
            "Epoch 18517  \tTraining Loss: 0.6915336406626384\tValidation Loss: 0.6915265295032863\n",
            "Epoch 18518  \tTraining Loss: 0.6915335787009171\tValidation Loss: 0.6915264670295258\n",
            "Epoch 18519  \tTraining Loss: 0.6915335167383848\tValidation Loss: 0.6915264045549546\n",
            "Epoch 18520  \tTraining Loss: 0.6915334547750416\tValidation Loss: 0.6915263420795726\n",
            "Epoch 18521  \tTraining Loss: 0.6915333928108871\tValidation Loss: 0.6915262796033799\n",
            "Epoch 18522  \tTraining Loss: 0.6915333308459216\tValidation Loss: 0.6915262171263762\n",
            "Epoch 18523  \tTraining Loss: 0.6915332688801451\tValidation Loss: 0.6915261546485618\n",
            "Epoch 18524  \tTraining Loss: 0.6915332069135572\tValidation Loss: 0.6915260921699363\n",
            "Epoch 18525  \tTraining Loss: 0.6915331449461584\tValidation Loss: 0.6915260296904999\n",
            "Epoch 18526  \tTraining Loss: 0.6915330829779482\tValidation Loss: 0.6915259672102526\n",
            "Epoch 18527  \tTraining Loss: 0.6915330210089269\tValidation Loss: 0.6915259047291942\n",
            "Epoch 18528  \tTraining Loss: 0.6915329590390943\tValidation Loss: 0.6915258422473248\n",
            "Epoch 18529  \tTraining Loss: 0.6915328970684502\tValidation Loss: 0.6915257797646442\n",
            "Epoch 18530  \tTraining Loss: 0.6915328350969949\tValidation Loss: 0.6915257172811526\n",
            "Epoch 18531  \tTraining Loss: 0.6915327731247283\tValidation Loss: 0.6915256547968499\n",
            "Epoch 18532  \tTraining Loss: 0.69153271115165\tValidation Loss: 0.6915255923117358\n",
            "Epoch 18533  \tTraining Loss: 0.6915326491777605\tValidation Loss: 0.6915255298258106\n",
            "Epoch 18534  \tTraining Loss: 0.6915325872030594\tValidation Loss: 0.6915254673390743\n",
            "Epoch 18535  \tTraining Loss: 0.6915325252275468\tValidation Loss: 0.6915254048515265\n",
            "Epoch 18536  \tTraining Loss: 0.6915324632512228\tValidation Loss: 0.6915253423631674\n",
            "Epoch 18537  \tTraining Loss: 0.691532401274087\tValidation Loss: 0.691525279873997\n",
            "Epoch 18538  \tTraining Loss: 0.6915323392961397\tValidation Loss: 0.6915252173840152\n",
            "Epoch 18539  \tTraining Loss: 0.6915322773173807\tValidation Loss: 0.691525154893222\n",
            "Epoch 18540  \tTraining Loss: 0.6915322153378101\tValidation Loss: 0.6915250924016173\n",
            "Epoch 18541  \tTraining Loss: 0.6915321533574277\tValidation Loss: 0.6915250299092012\n",
            "Epoch 18542  \tTraining Loss: 0.6915320913762335\tValidation Loss: 0.6915249674159735\n",
            "Epoch 18543  \tTraining Loss: 0.6915320293942275\tValidation Loss: 0.6915249049219343\n",
            "Epoch 18544  \tTraining Loss: 0.6915319674114098\tValidation Loss: 0.6915248424270835\n",
            "Epoch 18545  \tTraining Loss: 0.6915319054277801\tValidation Loss: 0.691524779931421\n",
            "Epoch 18546  \tTraining Loss: 0.6915318434433386\tValidation Loss: 0.6915247174349468\n",
            "Epoch 18547  \tTraining Loss: 0.691531781458085\tValidation Loss: 0.6915246549376611\n",
            "Epoch 18548  \tTraining Loss: 0.6915317194720197\tValidation Loss: 0.6915245924395637\n",
            "Epoch 18549  \tTraining Loss: 0.6915316574851423\tValidation Loss: 0.6915245299406543\n",
            "Epoch 18550  \tTraining Loss: 0.6915315954974528\tValidation Loss: 0.6915244674409332\n",
            "Epoch 18551  \tTraining Loss: 0.6915315335089515\tValidation Loss: 0.6915244049404004\n",
            "Epoch 18552  \tTraining Loss: 0.6915314715196378\tValidation Loss: 0.6915243424390556\n",
            "Epoch 18553  \tTraining Loss: 0.6915314095295121\tValidation Loss: 0.691524279936899\n",
            "Epoch 18554  \tTraining Loss: 0.6915313475385743\tValidation Loss: 0.6915242174339303\n",
            "Epoch 18555  \tTraining Loss: 0.6915312855468242\tValidation Loss: 0.6915241549301497\n",
            "Epoch 18556  \tTraining Loss: 0.6915312235542619\tValidation Loss: 0.6915240924255572\n",
            "Epoch 18557  \tTraining Loss: 0.6915311615608873\tValidation Loss: 0.6915240299201527\n",
            "Epoch 18558  \tTraining Loss: 0.6915310995667004\tValidation Loss: 0.6915239674139361\n",
            "Epoch 18559  \tTraining Loss: 0.6915310375717013\tValidation Loss: 0.6915239049069074\n",
            "Epoch 18560  \tTraining Loss: 0.6915309755758898\tValidation Loss: 0.6915238423990666\n",
            "Epoch 18561  \tTraining Loss: 0.6915309135792658\tValidation Loss: 0.6915237798904135\n",
            "Epoch 18562  \tTraining Loss: 0.6915308515818295\tValidation Loss: 0.6915237173809483\n",
            "Epoch 18563  \tTraining Loss: 0.6915307895835806\tValidation Loss: 0.6915236548706708\n",
            "Epoch 18564  \tTraining Loss: 0.6915307275845193\tValidation Loss: 0.6915235923595812\n",
            "Epoch 18565  \tTraining Loss: 0.6915306655846454\tValidation Loss: 0.6915235298476792\n",
            "Epoch 18566  \tTraining Loss: 0.6915306035839589\tValidation Loss: 0.691523467334965\n",
            "Epoch 18567  \tTraining Loss: 0.69153054158246\tValidation Loss: 0.6915234048214383\n",
            "Epoch 18568  \tTraining Loss: 0.6915304795801481\tValidation Loss: 0.691523342307099\n",
            "Epoch 18569  \tTraining Loss: 0.6915304175770238\tValidation Loss: 0.6915232797919476\n",
            "Epoch 18570  \tTraining Loss: 0.6915303555730867\tValidation Loss: 0.6915232172759835\n",
            "Epoch 18571  \tTraining Loss: 0.691530293568337\tValidation Loss: 0.6915231547592071\n",
            "Epoch 18572  \tTraining Loss: 0.6915302315627743\tValidation Loss: 0.691523092241618\n",
            "Epoch 18573  \tTraining Loss: 0.691530169556399\tValidation Loss: 0.6915230297232166\n",
            "Epoch 18574  \tTraining Loss: 0.6915301075492108\tValidation Loss: 0.6915229672040023\n",
            "Epoch 18575  \tTraining Loss: 0.6915300455412098\tValidation Loss: 0.6915229046839755\n",
            "Epoch 18576  \tTraining Loss: 0.6915299835323957\tValidation Loss: 0.691522842163136\n",
            "Epoch 18577  \tTraining Loss: 0.6915299215227688\tValidation Loss: 0.6915227796414838\n",
            "Epoch 18578  \tTraining Loss: 0.6915298595123289\tValidation Loss: 0.6915227171190189\n",
            "Epoch 18579  \tTraining Loss: 0.691529797501076\tValidation Loss: 0.6915226545957412\n",
            "Epoch 18580  \tTraining Loss: 0.6915297354890101\tValidation Loss: 0.6915225920716508\n",
            "Epoch 18581  \tTraining Loss: 0.691529673476131\tValidation Loss: 0.6915225295467474\n",
            "Epoch 18582  \tTraining Loss: 0.6915296114624389\tValidation Loss: 0.6915224670210313\n",
            "Epoch 18583  \tTraining Loss: 0.6915295494479337\tValidation Loss: 0.6915224044945021\n",
            "Epoch 18584  \tTraining Loss: 0.6915294874326151\tValidation Loss: 0.69152234196716\n",
            "Epoch 18585  \tTraining Loss: 0.6915294254164834\tValidation Loss: 0.691522279439005\n",
            "Epoch 18586  \tTraining Loss: 0.6915293633995385\tValidation Loss: 0.691522216910037\n",
            "Epoch 18587  \tTraining Loss: 0.6915293013817804\tValidation Loss: 0.6915221543802559\n",
            "Epoch 18588  \tTraining Loss: 0.6915292393632089\tValidation Loss: 0.6915220918496617\n",
            "Epoch 18589  \tTraining Loss: 0.691529177343824\tValidation Loss: 0.6915220293182546\n",
            "Epoch 18590  \tTraining Loss: 0.6915291153236258\tValidation Loss: 0.6915219667860342\n",
            "Epoch 18591  \tTraining Loss: 0.6915290533026142\tValidation Loss: 0.6915219042530006\n",
            "Epoch 18592  \tTraining Loss: 0.691528991280789\tValidation Loss: 0.6915218417191539\n",
            "Epoch 18593  \tTraining Loss: 0.6915289292581505\tValidation Loss: 0.6915217791844939\n",
            "Epoch 18594  \tTraining Loss: 0.6915288672346983\tValidation Loss: 0.6915217166490205\n",
            "Epoch 18595  \tTraining Loss: 0.6915288052104327\tValidation Loss: 0.691521654112734\n",
            "Epoch 18596  \tTraining Loss: 0.6915287431853535\tValidation Loss: 0.691521591575634\n",
            "Epoch 18597  \tTraining Loss: 0.6915286811594606\tValidation Loss: 0.6915215290377208\n",
            "Epoch 18598  \tTraining Loss: 0.6915286191327541\tValidation Loss: 0.6915214664989939\n",
            "Epoch 18599  \tTraining Loss: 0.6915285571052339\tValidation Loss: 0.6915214039594538\n",
            "Epoch 18600  \tTraining Loss: 0.6915284950769\tValidation Loss: 0.6915213414191002\n",
            "Epoch 18601  \tTraining Loss: 0.6915284330477524\tValidation Loss: 0.691521278877933\n",
            "Epoch 18602  \tTraining Loss: 0.6915283710177909\tValidation Loss: 0.6915212163359524\n",
            "Epoch 18603  \tTraining Loss: 0.6915283089870156\tValidation Loss: 0.691521153793158\n",
            "Epoch 18604  \tTraining Loss: 0.6915282469554266\tValidation Loss: 0.6915210912495501\n",
            "Epoch 18605  \tTraining Loss: 0.6915281849230236\tValidation Loss: 0.6915210287051287\n",
            "Epoch 18606  \tTraining Loss: 0.6915281228898066\tValidation Loss: 0.6915209661598934\n",
            "Epoch 18607  \tTraining Loss: 0.6915280608557758\tValidation Loss: 0.6915209036138446\n",
            "Epoch 18608  \tTraining Loss: 0.691527998820931\tValidation Loss: 0.6915208410669819\n",
            "Epoch 18609  \tTraining Loss: 0.691527936785272\tValidation Loss: 0.6915207785193054\n",
            "Epoch 18610  \tTraining Loss: 0.691527874748799\tValidation Loss: 0.6915207159708151\n",
            "Epoch 18611  \tTraining Loss: 0.691527812711512\tValidation Loss: 0.691520653421511\n",
            "Epoch 18612  \tTraining Loss: 0.6915277506734109\tValidation Loss: 0.6915205908713931\n",
            "Epoch 18613  \tTraining Loss: 0.6915276886344954\tValidation Loss: 0.6915205283204611\n",
            "Epoch 18614  \tTraining Loss: 0.6915276265947659\tValidation Loss: 0.6915204657687152\n",
            "Epoch 18615  \tTraining Loss: 0.6915275645542222\tValidation Loss: 0.6915204032161554\n",
            "Epoch 18616  \tTraining Loss: 0.6915275025128642\tValidation Loss: 0.6915203406627815\n",
            "Epoch 18617  \tTraining Loss: 0.6915274404706919\tValidation Loss: 0.6915202781085936\n",
            "Epoch 18618  \tTraining Loss: 0.6915273784277053\tValidation Loss: 0.6915202155535916\n",
            "Epoch 18619  \tTraining Loss: 0.6915273163839043\tValidation Loss: 0.6915201529977755\n",
            "Epoch 18620  \tTraining Loss: 0.6915272543392889\tValidation Loss: 0.6915200904411452\n",
            "Epoch 18621  \tTraining Loss: 0.6915271922938591\tValidation Loss: 0.6915200278837007\n",
            "Epoch 18622  \tTraining Loss: 0.6915271302476147\tValidation Loss: 0.6915199653254421\n",
            "Epoch 18623  \tTraining Loss: 0.691527068200556\tValidation Loss: 0.6915199027663691\n",
            "Epoch 18624  \tTraining Loss: 0.6915270061526826\tValidation Loss: 0.6915198402064819\n",
            "Epoch 18625  \tTraining Loss: 0.6915269441039947\tValidation Loss: 0.6915197776457803\n",
            "Epoch 18626  \tTraining Loss: 0.6915268820544923\tValidation Loss: 0.6915197150842646\n",
            "Epoch 18627  \tTraining Loss: 0.6915268200041751\tValidation Loss: 0.6915196525219341\n",
            "Epoch 18628  \tTraining Loss: 0.6915267579530434\tValidation Loss: 0.6915195899587896\n",
            "Epoch 18629  \tTraining Loss: 0.6915266959010968\tValidation Loss: 0.6915195273948302\n",
            "Epoch 18630  \tTraining Loss: 0.6915266338483357\tValidation Loss: 0.6915194648300567\n",
            "Epoch 18631  \tTraining Loss: 0.6915265717947596\tValidation Loss: 0.6915194022644685\n",
            "Epoch 18632  \tTraining Loss: 0.6915265097403689\tValidation Loss: 0.6915193396980657\n",
            "Epoch 18633  \tTraining Loss: 0.6915264476851631\tValidation Loss: 0.6915192771308485\n",
            "Epoch 18634  \tTraining Loss: 0.6915263856291427\tValidation Loss: 0.6915192145628165\n",
            "Epoch 18635  \tTraining Loss: 0.6915263235723074\tValidation Loss: 0.69151915199397\n",
            "Epoch 18636  \tTraining Loss: 0.6915262615146569\tValidation Loss: 0.6915190894243086\n",
            "Epoch 18637  \tTraining Loss: 0.6915261994561915\tValidation Loss: 0.6915190268538326\n",
            "Epoch 18638  \tTraining Loss: 0.6915261373969113\tValidation Loss: 0.6915189642825418\n",
            "Epoch 18639  \tTraining Loss: 0.691526075336816\tValidation Loss: 0.6915189017104362\n",
            "Epoch 18640  \tTraining Loss: 0.6915260132759055\tValidation Loss: 0.6915188391375159\n",
            "Epoch 18641  \tTraining Loss: 0.69152595121418\tValidation Loss: 0.6915187765637806\n",
            "Epoch 18642  \tTraining Loss: 0.6915258891516394\tValidation Loss: 0.6915187139892304\n",
            "Epoch 18643  \tTraining Loss: 0.6915258270882834\tValidation Loss: 0.6915186514138654\n",
            "Epoch 18644  \tTraining Loss: 0.6915257650241122\tValidation Loss: 0.6915185888376852\n",
            "Epoch 18645  \tTraining Loss: 0.6915257029591261\tValidation Loss: 0.6915185262606902\n",
            "Epoch 18646  \tTraining Loss: 0.6915256408933245\tValidation Loss: 0.6915184636828802\n",
            "Epoch 18647  \tTraining Loss: 0.6915255788267074\tValidation Loss: 0.691518401104255\n",
            "Epoch 18648  \tTraining Loss: 0.6915255167592753\tValidation Loss: 0.6915183385248148\n",
            "Epoch 18649  \tTraining Loss: 0.6915254546910277\tValidation Loss: 0.6915182759445594\n",
            "Epoch 18650  \tTraining Loss: 0.6915253926219646\tValidation Loss: 0.691518213363489\n",
            "Epoch 18651  \tTraining Loss: 0.691525330552086\tValidation Loss: 0.6915181507816031\n",
            "Epoch 18652  \tTraining Loss: 0.6915252684813921\tValidation Loss: 0.6915180881989021\n",
            "Epoch 18653  \tTraining Loss: 0.6915252064098826\tValidation Loss: 0.691518025615386\n",
            "Epoch 18654  \tTraining Loss: 0.6915251443375576\tValidation Loss: 0.6915179630310545\n",
            "Epoch 18655  \tTraining Loss: 0.691525082264417\tValidation Loss: 0.6915179004459076\n",
            "Epoch 18656  \tTraining Loss: 0.6915250201904607\tValidation Loss: 0.6915178378599453\n",
            "Epoch 18657  \tTraining Loss: 0.6915249581156888\tValidation Loss: 0.6915177752731677\n",
            "Epoch 18658  \tTraining Loss: 0.6915248960401011\tValidation Loss: 0.6915177126855746\n",
            "Epoch 18659  \tTraining Loss: 0.6915248339636979\tValidation Loss: 0.6915176500971661\n",
            "Epoch 18660  \tTraining Loss: 0.691524771886479\tValidation Loss: 0.691517587507942\n",
            "Epoch 18661  \tTraining Loss: 0.691524709808444\tValidation Loss: 0.6915175249179025\n",
            "Epoch 18662  \tTraining Loss: 0.6915246477295934\tValidation Loss: 0.6915174623270474\n",
            "Epoch 18663  \tTraining Loss: 0.6915245856499269\tValidation Loss: 0.6915173997353765\n",
            "Epoch 18664  \tTraining Loss: 0.6915245235694445\tValidation Loss: 0.6915173371428903\n",
            "Epoch 18665  \tTraining Loss: 0.6915244614881463\tValidation Loss: 0.6915172745495881\n",
            "Epoch 18666  \tTraining Loss: 0.6915243994060319\tValidation Loss: 0.6915172119554704\n",
            "Epoch 18667  \tTraining Loss: 0.6915243373231017\tValidation Loss: 0.691517149360537\n",
            "Epoch 18668  \tTraining Loss: 0.6915242752393553\tValidation Loss: 0.6915170867647876\n",
            "Epoch 18669  \tTraining Loss: 0.6915242131547931\tValidation Loss: 0.6915170241682226\n",
            "Epoch 18670  \tTraining Loss: 0.6915241510694147\tValidation Loss: 0.6915169615708416\n",
            "Epoch 18671  \tTraining Loss: 0.69152408898322\tValidation Loss: 0.6915168989726448\n",
            "Epoch 18672  \tTraining Loss: 0.6915240268962094\tValidation Loss: 0.691516836373632\n",
            "Epoch 18673  \tTraining Loss: 0.6915239648083824\tValidation Loss: 0.6915167737738036\n",
            "Epoch 18674  \tTraining Loss: 0.6915239027197393\tValidation Loss: 0.6915167111731589\n",
            "Epoch 18675  \tTraining Loss: 0.6915238406302799\tValidation Loss: 0.6915166485716983\n",
            "Epoch 18676  \tTraining Loss: 0.6915237785400042\tValidation Loss: 0.6915165859694217\n",
            "Epoch 18677  \tTraining Loss: 0.6915237164489122\tValidation Loss: 0.6915165233663289\n",
            "Epoch 18678  \tTraining Loss: 0.6915236543570038\tValidation Loss: 0.69151646076242\n",
            "Epoch 18679  \tTraining Loss: 0.6915235922642791\tValidation Loss: 0.6915163981576952\n",
            "Epoch 18680  \tTraining Loss: 0.6915235301707379\tValidation Loss: 0.6915163355521539\n",
            "Epoch 18681  \tTraining Loss: 0.6915234680763802\tValidation Loss: 0.6915162729457965\n",
            "Epoch 18682  \tTraining Loss: 0.6915234059812061\tValidation Loss: 0.6915162103386228\n",
            "Epoch 18683  \tTraining Loss: 0.6915233438852154\tValidation Loss: 0.691516147730633\n",
            "Epoch 18684  \tTraining Loss: 0.691523281788408\tValidation Loss: 0.6915160851218267\n",
            "Epoch 18685  \tTraining Loss: 0.6915232196907842\tValidation Loss: 0.6915160225122042\n",
            "Epoch 18686  \tTraining Loss: 0.6915231575923436\tValidation Loss: 0.6915159599017653\n",
            "Epoch 18687  \tTraining Loss: 0.6915230954930864\tValidation Loss: 0.6915158972905098\n",
            "Epoch 18688  \tTraining Loss: 0.6915230333930126\tValidation Loss: 0.691515834678438\n",
            "Epoch 18689  \tTraining Loss: 0.691522971292122\tValidation Loss: 0.6915157720655497\n",
            "Epoch 18690  \tTraining Loss: 0.6915229091904146\tValidation Loss: 0.6915157094518449\n",
            "Epoch 18691  \tTraining Loss: 0.6915228470878905\tValidation Loss: 0.6915156468373236\n",
            "Epoch 18692  \tTraining Loss: 0.6915227849845494\tValidation Loss: 0.6915155842219857\n",
            "Epoch 18693  \tTraining Loss: 0.6915227228803915\tValidation Loss: 0.6915155216058311\n",
            "Epoch 18694  \tTraining Loss: 0.6915226607754168\tValidation Loss: 0.6915154589888599\n",
            "Epoch 18695  \tTraining Loss: 0.6915225986696251\tValidation Loss: 0.691515396371072\n",
            "Epoch 18696  \tTraining Loss: 0.6915225365630163\tValidation Loss: 0.6915153337524674\n",
            "Epoch 18697  \tTraining Loss: 0.6915224744555906\tValidation Loss: 0.691515271133046\n",
            "Epoch 18698  \tTraining Loss: 0.6915224123473478\tValidation Loss: 0.6915152085128079\n",
            "Epoch 18699  \tTraining Loss: 0.6915223502382879\tValidation Loss: 0.6915151458917529\n",
            "Epoch 18700  \tTraining Loss: 0.6915222881284111\tValidation Loss: 0.6915150832698811\n",
            "Epoch 18701  \tTraining Loss: 0.691522226017717\tValidation Loss: 0.6915150206471924\n",
            "Epoch 18702  \tTraining Loss: 0.6915221639062058\tValidation Loss: 0.6915149580236868\n",
            "Epoch 18703  \tTraining Loss: 0.6915221017938773\tValidation Loss: 0.6915148953993641\n",
            "Epoch 18704  \tTraining Loss: 0.6915220396807314\tValidation Loss: 0.6915148327742245\n",
            "Epoch 18705  \tTraining Loss: 0.6915219775667685\tValidation Loss: 0.6915147701482679\n",
            "Epoch 18706  \tTraining Loss: 0.6915219154519883\tValidation Loss: 0.6915147075214944\n",
            "Epoch 18707  \tTraining Loss: 0.6915218533363906\tValidation Loss: 0.6915146448939035\n",
            "Epoch 18708  \tTraining Loss: 0.6915217912199756\tValidation Loss: 0.6915145822654957\n",
            "Epoch 18709  \tTraining Loss: 0.6915217291027431\tValidation Loss: 0.6915145196362706\n",
            "Epoch 18710  \tTraining Loss: 0.6915216669846932\tValidation Loss: 0.6915144570062284\n",
            "Epoch 18711  \tTraining Loss: 0.6915216048658258\tValidation Loss: 0.6915143943753691\n",
            "Epoch 18712  \tTraining Loss: 0.691521542746141\tValidation Loss: 0.6915143317436923\n",
            "Epoch 18713  \tTraining Loss: 0.6915214806256385\tValidation Loss: 0.6915142691111983\n",
            "Epoch 18714  \tTraining Loss: 0.6915214185043185\tValidation Loss: 0.691514206477887\n",
            "Epoch 18715  \tTraining Loss: 0.6915213563821809\tValidation Loss: 0.6915141438437582\n",
            "Epoch 18716  \tTraining Loss: 0.6915212942592255\tValidation Loss: 0.6915140812088122\n",
            "Epoch 18717  \tTraining Loss: 0.6915212321354526\tValidation Loss: 0.6915140185730486\n",
            "Epoch 18718  \tTraining Loss: 0.6915211700108618\tValidation Loss: 0.6915139559364677\n",
            "Epoch 18719  \tTraining Loss: 0.6915211078854534\tValidation Loss: 0.6915138932990693\n",
            "Epoch 18720  \tTraining Loss: 0.6915210457592272\tValidation Loss: 0.6915138306608533\n",
            "Epoch 18721  \tTraining Loss: 0.6915209836321831\tValidation Loss: 0.6915137680218196\n",
            "Epoch 18722  \tTraining Loss: 0.6915209215043212\tValidation Loss: 0.6915137053819685\n",
            "Epoch 18723  \tTraining Loss: 0.6915208593756415\tValidation Loss: 0.6915136427412998\n",
            "Epoch 18724  \tTraining Loss: 0.6915207972461436\tValidation Loss: 0.6915135800998133\n",
            "Epoch 18725  \tTraining Loss: 0.6915207351158279\tValidation Loss: 0.6915135174575092\n",
            "Epoch 18726  \tTraining Loss: 0.6915206729846942\tValidation Loss: 0.6915134548143873\n",
            "Epoch 18727  \tTraining Loss: 0.6915206108527426\tValidation Loss: 0.6915133921704476\n",
            "Epoch 18728  \tTraining Loss: 0.6915205487199728\tValidation Loss: 0.6915133295256901\n",
            "Epoch 18729  \tTraining Loss: 0.6915204865863849\tValidation Loss: 0.6915132668801149\n",
            "Epoch 18730  \tTraining Loss: 0.691520424451979\tValidation Loss: 0.6915132042337218\n",
            "Epoch 18731  \tTraining Loss: 0.6915203623167548\tValidation Loss: 0.6915131415865108\n",
            "Epoch 18732  \tTraining Loss: 0.6915203001807124\tValidation Loss: 0.6915130789384818\n",
            "Epoch 18733  \tTraining Loss: 0.691520238043852\tValidation Loss: 0.6915130162896348\n",
            "Epoch 18734  \tTraining Loss: 0.691520175906173\tValidation Loss: 0.6915129536399698\n",
            "Epoch 18735  \tTraining Loss: 0.691520113767676\tValidation Loss: 0.6915128909894869\n",
            "Epoch 18736  \tTraining Loss: 0.6915200516283605\tValidation Loss: 0.6915128283381858\n",
            "Epoch 18737  \tTraining Loss: 0.6915199894882267\tValidation Loss: 0.6915127656860666\n",
            "Epoch 18738  \tTraining Loss: 0.6915199273472744\tValidation Loss: 0.6915127030331294\n",
            "Epoch 18739  \tTraining Loss: 0.6915198652055038\tValidation Loss: 0.6915126403793738\n",
            "Epoch 18740  \tTraining Loss: 0.6915198030629147\tValidation Loss: 0.6915125777248001\n",
            "Epoch 18741  \tTraining Loss: 0.6915197409195071\tValidation Loss: 0.6915125150694083\n",
            "Epoch 18742  \tTraining Loss: 0.6915196787752809\tValidation Loss: 0.691512452413198\n",
            "Epoch 18743  \tTraining Loss: 0.6915196166302362\tValidation Loss: 0.6915123897561695\n",
            "Epoch 18744  \tTraining Loss: 0.6915195544843727\tValidation Loss: 0.6915123270983227\n",
            "Epoch 18745  \tTraining Loss: 0.6915194923376908\tValidation Loss: 0.6915122644396574\n",
            "Epoch 18746  \tTraining Loss: 0.6915194301901902\tValidation Loss: 0.6915122017801738\n",
            "Epoch 18747  \tTraining Loss: 0.6915193680418709\tValidation Loss: 0.6915121391198717\n",
            "Epoch 18748  \tTraining Loss: 0.6915193058927328\tValidation Loss: 0.6915120764587511\n",
            "Epoch 18749  \tTraining Loss: 0.6915192437427758\tValidation Loss: 0.691512013796812\n",
            "Epoch 18750  \tTraining Loss: 0.6915191815920003\tValidation Loss: 0.6915119511340544\n",
            "Epoch 18751  \tTraining Loss: 0.6915191194404058\tValidation Loss: 0.6915118884704783\n",
            "Epoch 18752  \tTraining Loss: 0.6915190572879926\tValidation Loss: 0.6915118258060835\n",
            "Epoch 18753  \tTraining Loss: 0.6915189951347602\tValidation Loss: 0.69151176314087\n",
            "Epoch 18754  \tTraining Loss: 0.6915189329807089\tValidation Loss: 0.6915117004748379\n",
            "Epoch 18755  \tTraining Loss: 0.6915188708258387\tValidation Loss: 0.6915116378079871\n",
            "Epoch 18756  \tTraining Loss: 0.6915188086701496\tValidation Loss: 0.6915115751403176\n",
            "Epoch 18757  \tTraining Loss: 0.6915187465136412\tValidation Loss: 0.6915115124718292\n",
            "Epoch 18758  \tTraining Loss: 0.691518684356314\tValidation Loss: 0.691511449802522\n",
            "Epoch 18759  \tTraining Loss: 0.6915186221981675\tValidation Loss: 0.6915113871323959\n",
            "Epoch 18760  \tTraining Loss: 0.691518560039202\tValidation Loss: 0.691511324461451\n",
            "Epoch 18761  \tTraining Loss: 0.6915184978794172\tValidation Loss: 0.6915112617896874\n",
            "Epoch 18762  \tTraining Loss: 0.6915184357188133\tValidation Loss: 0.6915111991171045\n",
            "Epoch 18763  \tTraining Loss: 0.6915183735573901\tValidation Loss: 0.6915111364437028\n",
            "Epoch 18764  \tTraining Loss: 0.6915183113951475\tValidation Loss: 0.691511073769482\n",
            "Epoch 18765  \tTraining Loss: 0.6915182492320856\tValidation Loss: 0.6915110110944421\n",
            "Epoch 18766  \tTraining Loss: 0.6915181870682044\tValidation Loss: 0.6915109484185832\n",
            "Epoch 18767  \tTraining Loss: 0.6915181249035041\tValidation Loss: 0.6915108857419051\n",
            "Epoch 18768  \tTraining Loss: 0.691518062737984\tValidation Loss: 0.691510823064408\n",
            "Epoch 18769  \tTraining Loss: 0.6915180005716446\tValidation Loss: 0.6915107603860916\n",
            "Epoch 18770  \tTraining Loss: 0.6915179384044857\tValidation Loss: 0.691510697706956\n",
            "Epoch 18771  \tTraining Loss: 0.6915178762365072\tValidation Loss: 0.6915106350270013\n",
            "Epoch 18772  \tTraining Loss: 0.6915178140677093\tValidation Loss: 0.691510572346227\n",
            "Epoch 18773  \tTraining Loss: 0.6915177518980917\tValidation Loss: 0.6915105096646336\n",
            "Epoch 18774  \tTraining Loss: 0.6915176897276546\tValidation Loss: 0.6915104469822209\n",
            "Epoch 18775  \tTraining Loss: 0.6915176275563976\tValidation Loss: 0.6915103842989886\n",
            "Epoch 18776  \tTraining Loss: 0.6915175653843212\tValidation Loss: 0.691510321614937\n",
            "Epoch 18777  \tTraining Loss: 0.6915175032114249\tValidation Loss: 0.6915102589300659\n",
            "Epoch 18778  \tTraining Loss: 0.6915174410377088\tValidation Loss: 0.6915101962443753\n",
            "Epoch 18779  \tTraining Loss: 0.6915173788631731\tValidation Loss: 0.6915101335578652\n",
            "Epoch 18780  \tTraining Loss: 0.6915173166878175\tValidation Loss: 0.6915100708705355\n",
            "Epoch 18781  \tTraining Loss: 0.6915172545116419\tValidation Loss: 0.6915100081823863\n",
            "Epoch 18782  \tTraining Loss: 0.6915171923346466\tValidation Loss: 0.6915099454934174\n",
            "Epoch 18783  \tTraining Loss: 0.6915171301568311\tValidation Loss: 0.6915098828036289\n",
            "Epoch 18784  \tTraining Loss: 0.6915170679781959\tValidation Loss: 0.6915098201130206\n",
            "Epoch 18785  \tTraining Loss: 0.6915170057987408\tValidation Loss: 0.6915097574215927\n",
            "Epoch 18786  \tTraining Loss: 0.6915169436184654\tValidation Loss: 0.691509694729345\n",
            "Epoch 18787  \tTraining Loss: 0.6915168814373701\tValidation Loss: 0.6915096320362774\n",
            "Epoch 18788  \tTraining Loss: 0.6915168192554546\tValidation Loss: 0.69150956934239\n",
            "Epoch 18789  \tTraining Loss: 0.691516757072719\tValidation Loss: 0.6915095066476828\n",
            "Epoch 18790  \tTraining Loss: 0.6915166948891632\tValidation Loss: 0.6915094439521557\n",
            "Epoch 18791  \tTraining Loss: 0.6915166327047872\tValidation Loss: 0.6915093812558087\n",
            "Epoch 18792  \tTraining Loss: 0.6915165705195911\tValidation Loss: 0.6915093185586416\n",
            "Epoch 18793  \tTraining Loss: 0.6915165083335746\tValidation Loss: 0.6915092558606547\n",
            "Epoch 18794  \tTraining Loss: 0.6915164461467379\tValidation Loss: 0.6915091931618477\n",
            "Epoch 18795  \tTraining Loss: 0.6915163839590808\tValidation Loss: 0.6915091304622205\n",
            "Epoch 18796  \tTraining Loss: 0.6915163217706034\tValidation Loss: 0.6915090677617733\n",
            "Epoch 18797  \tTraining Loss: 0.6915162595813056\tValidation Loss: 0.6915090050605059\n",
            "Epoch 18798  \tTraining Loss: 0.6915161973911873\tValidation Loss: 0.6915089423584185\n",
            "Epoch 18799  \tTraining Loss: 0.6915161352002486\tValidation Loss: 0.6915088796555107\n",
            "Epoch 18800  \tTraining Loss: 0.6915160730084892\tValidation Loss: 0.6915088169517828\n",
            "Epoch 18801  \tTraining Loss: 0.6915160108159094\tValidation Loss: 0.6915087542472346\n",
            "Epoch 18802  \tTraining Loss: 0.691515948622509\tValidation Loss: 0.6915086915418661\n",
            "Epoch 18803  \tTraining Loss: 0.691515886428288\tValidation Loss: 0.6915086288356771\n",
            "Epoch 18804  \tTraining Loss: 0.6915158242332464\tValidation Loss: 0.691508566128668\n",
            "Epoch 18805  \tTraining Loss: 0.691515762037384\tValidation Loss: 0.6915085034208382\n",
            "Epoch 18806  \tTraining Loss: 0.691515699840701\tValidation Loss: 0.6915084407121883\n",
            "Epoch 18807  \tTraining Loss: 0.6915156376431972\tValidation Loss: 0.6915083780027177\n",
            "Epoch 18808  \tTraining Loss: 0.6915155754448727\tValidation Loss: 0.6915083152924265\n",
            "Epoch 18809  \tTraining Loss: 0.6915155132457275\tValidation Loss: 0.691508252581315\n",
            "Epoch 18810  \tTraining Loss: 0.6915154510457613\tValidation Loss: 0.6915081898693828\n",
            "Epoch 18811  \tTraining Loss: 0.6915153888449742\tValidation Loss: 0.69150812715663\n",
            "Epoch 18812  \tTraining Loss: 0.6915153266433662\tValidation Loss: 0.6915080644430566\n",
            "Epoch 18813  \tTraining Loss: 0.6915152644409374\tValidation Loss: 0.6915080017286624\n",
            "Epoch 18814  \tTraining Loss: 0.6915152022376874\tValidation Loss: 0.6915079390134476\n",
            "Epoch 18815  \tTraining Loss: 0.6915151400336165\tValidation Loss: 0.6915078762974122\n",
            "Epoch 18816  \tTraining Loss: 0.6915150778287245\tValidation Loss: 0.6915078135805558\n",
            "Epoch 18817  \tTraining Loss: 0.6915150156230114\tValidation Loss: 0.6915077508628786\n",
            "Epoch 18818  \tTraining Loss: 0.6915149534164774\tValidation Loss: 0.6915076881443807\n",
            "Epoch 18819  \tTraining Loss: 0.6915148912091219\tValidation Loss: 0.6915076254250618\n",
            "Epoch 18820  \tTraining Loss: 0.6915148290009454\tValidation Loss: 0.691507562704922\n",
            "Epoch 18821  \tTraining Loss: 0.6915147667919478\tValidation Loss: 0.6915074999839612\n",
            "Epoch 18822  \tTraining Loss: 0.6915147045821287\tValidation Loss: 0.6915074372621796\n",
            "Epoch 18823  \tTraining Loss: 0.6915146423714885\tValidation Loss: 0.6915073745395767\n",
            "Epoch 18824  \tTraining Loss: 0.6915145801600269\tValidation Loss: 0.691507311816153\n",
            "Epoch 18825  \tTraining Loss: 0.6915145179477439\tValidation Loss: 0.6915072490919082\n",
            "Epoch 18826  \tTraining Loss: 0.6915144557346397\tValidation Loss: 0.6915071863668422\n",
            "Epoch 18827  \tTraining Loss: 0.6915143935207138\tValidation Loss: 0.6915071236409549\n",
            "Epoch 18828  \tTraining Loss: 0.6915143313059666\tValidation Loss: 0.6915070609142467\n",
            "Epoch 18829  \tTraining Loss: 0.691514269090398\tValidation Loss: 0.6915069981867171\n",
            "Epoch 18830  \tTraining Loss: 0.6915142068740078\tValidation Loss: 0.6915069354583664\n",
            "Epoch 18831  \tTraining Loss: 0.6915141446567961\tValidation Loss: 0.6915068727291943\n",
            "Epoch 18832  \tTraining Loss: 0.6915140824387627\tValidation Loss: 0.6915068099992009\n",
            "Epoch 18833  \tTraining Loss: 0.6915140202199076\tValidation Loss: 0.6915067472683861\n",
            "Epoch 18834  \tTraining Loss: 0.691513958000231\tValidation Loss: 0.69150668453675\n",
            "Epoch 18835  \tTraining Loss: 0.6915138957797327\tValidation Loss: 0.6915066218042925\n",
            "Epoch 18836  \tTraining Loss: 0.6915138335584124\tValidation Loss: 0.6915065590710134\n",
            "Epoch 18837  \tTraining Loss: 0.6915137713362707\tValidation Loss: 0.6915064963369129\n",
            "Epoch 18838  \tTraining Loss: 0.691513709113307\tValidation Loss: 0.6915064336019909\n",
            "Epoch 18839  \tTraining Loss: 0.6915136468895217\tValidation Loss: 0.6915063708662474\n",
            "Epoch 18840  \tTraining Loss: 0.6915135846649142\tValidation Loss: 0.6915063081296822\n",
            "Epoch 18841  \tTraining Loss: 0.6915135224394849\tValidation Loss: 0.6915062453922953\n",
            "Epoch 18842  \tTraining Loss: 0.6915134602132337\tValidation Loss: 0.6915061826540869\n",
            "Epoch 18843  \tTraining Loss: 0.6915133979861606\tValidation Loss: 0.6915061199150567\n",
            "Epoch 18844  \tTraining Loss: 0.6915133357582655\tValidation Loss: 0.6915060571752049\n",
            "Epoch 18845  \tTraining Loss: 0.6915132735295483\tValidation Loss: 0.6915059944345312\n",
            "Epoch 18846  \tTraining Loss: 0.6915132113000091\tValidation Loss: 0.6915059316930358\n",
            "Epoch 18847  \tTraining Loss: 0.6915131490696477\tValidation Loss: 0.6915058689507185\n",
            "Epoch 18848  \tTraining Loss: 0.6915130868384642\tValidation Loss: 0.6915058062075794\n",
            "Epoch 18849  \tTraining Loss: 0.6915130246064586\tValidation Loss: 0.6915057434636184\n",
            "Epoch 18850  \tTraining Loss: 0.6915129623736307\tValidation Loss: 0.6915056807188354\n",
            "Epoch 18851  \tTraining Loss: 0.6915129001399807\tValidation Loss: 0.6915056179732305\n",
            "Epoch 18852  \tTraining Loss: 0.6915128379055082\tValidation Loss: 0.6915055552268036\n",
            "Epoch 18853  \tTraining Loss: 0.6915127756702136\tValidation Loss: 0.6915054924795546\n",
            "Epoch 18854  \tTraining Loss: 0.6915127134340966\tValidation Loss: 0.6915054297314837\n",
            "Epoch 18855  \tTraining Loss: 0.6915126511971572\tValidation Loss: 0.6915053669825906\n",
            "Epoch 18856  \tTraining Loss: 0.6915125889593954\tValidation Loss: 0.6915053042328753\n",
            "Epoch 18857  \tTraining Loss: 0.6915125267208111\tValidation Loss: 0.6915052414823379\n",
            "Epoch 18858  \tTraining Loss: 0.6915124644814044\tValidation Loss: 0.6915051787309783\n",
            "Epoch 18859  \tTraining Loss: 0.6915124022411752\tValidation Loss: 0.6915051159787964\n",
            "Epoch 18860  \tTraining Loss: 0.6915123400001233\tValidation Loss: 0.6915050532257924\n",
            "Epoch 18861  \tTraining Loss: 0.6915122777582491\tValidation Loss: 0.6915049904719659\n",
            "Epoch 18862  \tTraining Loss: 0.691512215515552\tValidation Loss: 0.6915049277173172\n",
            "Epoch 18863  \tTraining Loss: 0.6915121532720324\tValidation Loss: 0.6915048649618459\n",
            "Epoch 18864  \tTraining Loss: 0.6915120910276902\tValidation Loss: 0.6915048022055524\n",
            "Epoch 18865  \tTraining Loss: 0.691512028782525\tValidation Loss: 0.6915047394484365\n",
            "Epoch 18866  \tTraining Loss: 0.6915119665365372\tValidation Loss: 0.6915046766904981\n",
            "Epoch 18867  \tTraining Loss: 0.6915119042897268\tValidation Loss: 0.6915046139317372\n",
            "Epoch 18868  \tTraining Loss: 0.6915118420420934\tValidation Loss: 0.6915045511721536\n",
            "Epoch 18869  \tTraining Loss: 0.6915117797936371\tValidation Loss: 0.6915044884117475\n",
            "Epoch 18870  \tTraining Loss: 0.691511717544358\tValidation Loss: 0.691504425650519\n",
            "Epoch 18871  \tTraining Loss: 0.691511655294256\tValidation Loss: 0.6915043628884675\n",
            "Epoch 18872  \tTraining Loss: 0.6915115930433311\tValidation Loss: 0.6915043001255936\n",
            "Epoch 18873  \tTraining Loss: 0.6915115307915831\tValidation Loss: 0.6915042373618968\n",
            "Epoch 18874  \tTraining Loss: 0.691511468539012\tValidation Loss: 0.6915041745973775\n",
            "Epoch 18875  \tTraining Loss: 0.691511406285618\tValidation Loss: 0.6915041118320352\n",
            "Epoch 18876  \tTraining Loss: 0.6915113440314009\tValidation Loss: 0.6915040490658702\n",
            "Epoch 18877  \tTraining Loss: 0.6915112817763606\tValidation Loss: 0.6915039862988823\n",
            "Epoch 18878  \tTraining Loss: 0.6915112195204972\tValidation Loss: 0.6915039235310716\n",
            "Epoch 18879  \tTraining Loss: 0.6915111572638105\tValidation Loss: 0.691503860762438\n",
            "Epoch 18880  \tTraining Loss: 0.6915110950063007\tValidation Loss: 0.6915037979929813\n",
            "Epoch 18881  \tTraining Loss: 0.6915110327479677\tValidation Loss: 0.6915037352227017\n",
            "Epoch 18882  \tTraining Loss: 0.6915109704888112\tValidation Loss: 0.691503672451599\n",
            "Epoch 18883  \tTraining Loss: 0.6915109082288315\tValidation Loss: 0.6915036096796734\n",
            "Epoch 18884  \tTraining Loss: 0.6915108459680284\tValidation Loss: 0.6915035469069246\n",
            "Epoch 18885  \tTraining Loss: 0.691510783706402\tValidation Loss: 0.6915034841333528\n",
            "Epoch 18886  \tTraining Loss: 0.6915107214439521\tValidation Loss: 0.6915034213589578\n",
            "Epoch 18887  \tTraining Loss: 0.6915106591806787\tValidation Loss: 0.6915033585837396\n",
            "Epoch 18888  \tTraining Loss: 0.6915105969165818\tValidation Loss: 0.6915032958076982\n",
            "Epoch 18889  \tTraining Loss: 0.6915105346516615\tValidation Loss: 0.6915032330308336\n",
            "Epoch 18890  \tTraining Loss: 0.6915104723859175\tValidation Loss: 0.6915031702531456\n",
            "Epoch 18891  \tTraining Loss: 0.6915104101193499\tValidation Loss: 0.6915031074746343\n",
            "Epoch 18892  \tTraining Loss: 0.6915103478519586\tValidation Loss: 0.6915030446952997\n",
            "Epoch 18893  \tTraining Loss: 0.6915102855837437\tValidation Loss: 0.6915029819151416\n",
            "Epoch 18894  \tTraining Loss: 0.6915102233147052\tValidation Loss: 0.6915029191341602\n",
            "Epoch 18895  \tTraining Loss: 0.6915101610448429\tValidation Loss: 0.6915028563523552\n",
            "Epoch 18896  \tTraining Loss: 0.6915100987741567\tValidation Loss: 0.6915027935697269\n",
            "Epoch 18897  \tTraining Loss: 0.691510036502647\tValidation Loss: 0.691502730786275\n",
            "Epoch 18898  \tTraining Loss: 0.6915099742303131\tValidation Loss: 0.6915026680019996\n",
            "Epoch 18899  \tTraining Loss: 0.6915099119571556\tValidation Loss: 0.6915026052169004\n",
            "Epoch 18900  \tTraining Loss: 0.6915098496831741\tValidation Loss: 0.6915025424309779\n",
            "Epoch 18901  \tTraining Loss: 0.6915097874083685\tValidation Loss: 0.6915024796442315\n",
            "Epoch 18902  \tTraining Loss: 0.6915097251327392\tValidation Loss: 0.6915024168566615\n",
            "Epoch 18903  \tTraining Loss: 0.6915096628562858\tValidation Loss: 0.6915023540682677\n",
            "Epoch 18904  \tTraining Loss: 0.6915096005790082\tValidation Loss: 0.6915022912790502\n",
            "Epoch 18905  \tTraining Loss: 0.6915095383009067\tValidation Loss: 0.6915022284890089\n",
            "Epoch 18906  \tTraining Loss: 0.691509476021981\tValidation Loss: 0.6915021656981437\n",
            "Epoch 18907  \tTraining Loss: 0.6915094137422312\tValidation Loss: 0.6915021029064548\n",
            "Epoch 18908  \tTraining Loss: 0.6915093514616573\tValidation Loss: 0.6915020401139419\n",
            "Epoch 18909  \tTraining Loss: 0.6915092891802591\tValidation Loss: 0.6915019773206049\n",
            "Epoch 18910  \tTraining Loss: 0.6915092268980365\tValidation Loss: 0.6915019145264442\n",
            "Epoch 18911  \tTraining Loss: 0.6915091646149898\tValidation Loss: 0.6915018517314593\n",
            "Epoch 18912  \tTraining Loss: 0.6915091023311188\tValidation Loss: 0.6915017889356505\n",
            "Epoch 18913  \tTraining Loss: 0.6915090400464234\tValidation Loss: 0.6915017261390176\n",
            "Epoch 18914  \tTraining Loss: 0.6915089777609036\tValidation Loss: 0.6915016633415606\n",
            "Epoch 18915  \tTraining Loss: 0.6915089154745594\tValidation Loss: 0.6915016005432795\n",
            "Epoch 18916  \tTraining Loss: 0.6915088531873907\tValidation Loss: 0.6915015377441741\n",
            "Epoch 18917  \tTraining Loss: 0.6915087908993977\tValidation Loss: 0.6915014749442446\n",
            "Epoch 18918  \tTraining Loss: 0.69150872861058\tValidation Loss: 0.6915014121434909\n",
            "Epoch 18919  \tTraining Loss: 0.6915086663209377\tValidation Loss: 0.6915013493419129\n",
            "Epoch 18920  \tTraining Loss: 0.691508604030471\tValidation Loss: 0.6915012865395106\n",
            "Epoch 18921  \tTraining Loss: 0.6915085417391795\tValidation Loss: 0.6915012237362839\n",
            "Epoch 18922  \tTraining Loss: 0.6915084794470635\tValidation Loss: 0.6915011609322329\n",
            "Epoch 18923  \tTraining Loss: 0.6915084171541227\tValidation Loss: 0.6915010981273574\n",
            "Epoch 18924  \tTraining Loss: 0.6915083548603572\tValidation Loss: 0.6915010353216575\n",
            "Epoch 18925  \tTraining Loss: 0.6915082925657668\tValidation Loss: 0.6915009725151332\n",
            "Epoch 18926  \tTraining Loss: 0.6915082302703519\tValidation Loss: 0.6915009097077844\n",
            "Epoch 18927  \tTraining Loss: 0.691508167974112\tValidation Loss: 0.691500846899611\n",
            "Epoch 18928  \tTraining Loss: 0.6915081056770472\tValidation Loss: 0.6915007840906131\n",
            "Epoch 18929  \tTraining Loss: 0.6915080433791576\tValidation Loss: 0.6915007212807904\n",
            "Epoch 18930  \tTraining Loss: 0.6915079810804431\tValidation Loss: 0.6915006584701433\n",
            "Epoch 18931  \tTraining Loss: 0.6915079187809036\tValidation Loss: 0.6915005956586713\n",
            "Epoch 18932  \tTraining Loss: 0.691507856480539\tValidation Loss: 0.6915005328463747\n",
            "Epoch 18933  \tTraining Loss: 0.6915077941793495\tValidation Loss: 0.6915004700332533\n",
            "Epoch 18934  \tTraining Loss: 0.6915077318773348\tValidation Loss: 0.6915004072193072\n",
            "Epoch 18935  \tTraining Loss: 0.6915076695744952\tValidation Loss: 0.6915003444045362\n",
            "Epoch 18936  \tTraining Loss: 0.6915076072708303\tValidation Loss: 0.6915002815889404\n",
            "Epoch 18937  \tTraining Loss: 0.6915075449663404\tValidation Loss: 0.6915002187725197\n",
            "Epoch 18938  \tTraining Loss: 0.6915074826610251\tValidation Loss: 0.6915001559552741\n",
            "Epoch 18939  \tTraining Loss: 0.6915074203548847\tValidation Loss: 0.6915000931372036\n",
            "Epoch 18940  \tTraining Loss: 0.691507358047919\tValidation Loss: 0.691500030318308\n",
            "Epoch 18941  \tTraining Loss: 0.6915072957401279\tValidation Loss: 0.6914999674985876\n",
            "Epoch 18942  \tTraining Loss: 0.6915072334315115\tValidation Loss: 0.691499904678042\n",
            "Epoch 18943  \tTraining Loss: 0.6915071711220698\tValidation Loss: 0.6914998418566712\n",
            "Epoch 18944  \tTraining Loss: 0.6915071088118027\tValidation Loss: 0.6914997790344753\n",
            "Epoch 18945  \tTraining Loss: 0.6915070465007102\tValidation Loss: 0.6914997162114545\n",
            "Epoch 18946  \tTraining Loss: 0.691506984188792\tValidation Loss: 0.6914996533876083\n",
            "Epoch 18947  \tTraining Loss: 0.6915069218760485\tValidation Loss: 0.691499590562937\n",
            "Epoch 18948  \tTraining Loss: 0.6915068595624794\tValidation Loss: 0.6914995277374404\n",
            "Epoch 18949  \tTraining Loss: 0.6915067972480846\tValidation Loss: 0.6914994649111184\n",
            "Epoch 18950  \tTraining Loss: 0.6915067349328644\tValidation Loss: 0.6914994020839712\n",
            "Epoch 18951  \tTraining Loss: 0.6915066726168184\tValidation Loss: 0.6914993392559987\n",
            "Epoch 18952  \tTraining Loss: 0.6915066102999469\tValidation Loss: 0.6914992764272007\n",
            "Epoch 18953  \tTraining Loss: 0.6915065479822495\tValidation Loss: 0.6914992135975773\n",
            "Epoch 18954  \tTraining Loss: 0.6915064856637264\tValidation Loss: 0.6914991507671284\n",
            "Epoch 18955  \tTraining Loss: 0.6915064233443776\tValidation Loss: 0.691499087935854\n",
            "Epoch 18956  \tTraining Loss: 0.6915063610242029\tValidation Loss: 0.691499025103754\n",
            "Epoch 18957  \tTraining Loss: 0.6915062987032023\tValidation Loss: 0.6914989622708287\n",
            "Epoch 18958  \tTraining Loss: 0.691506236381376\tValidation Loss: 0.6914988994370777\n",
            "Epoch 18959  \tTraining Loss: 0.6915061740587237\tValidation Loss: 0.6914988366025011\n",
            "Epoch 18960  \tTraining Loss: 0.6915061117352455\tValidation Loss: 0.6914987737670988\n",
            "Epoch 18961  \tTraining Loss: 0.6915060494109411\tValidation Loss: 0.6914987109308708\n",
            "Epoch 18962  \tTraining Loss: 0.6915059870858109\tValidation Loss: 0.691498648093817\n",
            "Epoch 18963  \tTraining Loss: 0.6915059247598545\tValidation Loss: 0.6914985852559374\n",
            "Epoch 18964  \tTraining Loss: 0.6915058624330721\tValidation Loss: 0.6914985224172322\n",
            "Epoch 18965  \tTraining Loss: 0.6915058001054636\tValidation Loss: 0.6914984595777011\n",
            "Epoch 18966  \tTraining Loss: 0.6915057377770288\tValidation Loss: 0.691498396737344\n",
            "Epoch 18967  \tTraining Loss: 0.691505675447768\tValidation Loss: 0.6914983338961611\n",
            "Epoch 18968  \tTraining Loss: 0.6915056131176809\tValidation Loss: 0.6914982710541522\n",
            "Epoch 18969  \tTraining Loss: 0.6915055507867676\tValidation Loss: 0.6914982082113175\n",
            "Epoch 18970  \tTraining Loss: 0.6915054884550279\tValidation Loss: 0.6914981453676566\n",
            "Epoch 18971  \tTraining Loss: 0.6915054261224619\tValidation Loss: 0.6914980825231698\n",
            "Epoch 18972  \tTraining Loss: 0.6915053637890696\tValidation Loss: 0.6914980196778568\n",
            "Epoch 18973  \tTraining Loss: 0.6915053014548508\tValidation Loss: 0.6914979568317179\n",
            "Epoch 18974  \tTraining Loss: 0.6915052391198057\tValidation Loss: 0.6914978939847527\n",
            "Epoch 18975  \tTraining Loss: 0.691505176783934\tValidation Loss: 0.6914978311369613\n",
            "Epoch 18976  \tTraining Loss: 0.6915051144472358\tValidation Loss: 0.6914977682883439\n",
            "Epoch 18977  \tTraining Loss: 0.6915050521097112\tValidation Loss: 0.6914977054389\n",
            "Epoch 18978  \tTraining Loss: 0.69150498977136\tValidation Loss: 0.69149764258863\n",
            "Epoch 18979  \tTraining Loss: 0.6915049274321822\tValidation Loss: 0.6914975797375336\n",
            "Epoch 18980  \tTraining Loss: 0.6915048650921777\tValidation Loss: 0.6914975168856109\n",
            "Epoch 18981  \tTraining Loss: 0.6915048027513466\tValidation Loss: 0.6914974540328619\n",
            "Epoch 18982  \tTraining Loss: 0.6915047404096888\tValidation Loss: 0.6914973911792863\n",
            "Epoch 18983  \tTraining Loss: 0.6915046780672042\tValidation Loss: 0.6914973283248844\n",
            "Epoch 18984  \tTraining Loss: 0.6915046157238929\tValidation Loss: 0.6914972654696561\n",
            "Epoch 18985  \tTraining Loss: 0.6915045533797548\tValidation Loss: 0.6914972026136011\n",
            "Epoch 18986  \tTraining Loss: 0.6915044910347897\tValidation Loss: 0.6914971397567197\n",
            "Epoch 18987  \tTraining Loss: 0.6915044286889979\tValidation Loss: 0.6914970768990116\n",
            "Epoch 18988  \tTraining Loss: 0.6915043663423792\tValidation Loss: 0.6914970140404769\n",
            "Epoch 18989  \tTraining Loss: 0.6915043039949335\tValidation Loss: 0.6914969511811155\n",
            "Epoch 18990  \tTraining Loss: 0.6915042416466608\tValidation Loss: 0.6914968883209277\n",
            "Epoch 18991  \tTraining Loss: 0.6915041792975611\tValidation Loss: 0.6914968254599129\n",
            "Epoch 18992  \tTraining Loss: 0.6915041169476345\tValidation Loss: 0.6914967625980716\n",
            "Epoch 18993  \tTraining Loss: 0.6915040545968806\tValidation Loss: 0.6914966997354032\n",
            "Epoch 18994  \tTraining Loss: 0.6915039922452998\tValidation Loss: 0.6914966368719082\n",
            "Epoch 18995  \tTraining Loss: 0.6915039298928918\tValidation Loss: 0.6914965740075862\n",
            "Epoch 18996  \tTraining Loss: 0.6915038675396564\tValidation Loss: 0.6914965111424375\n",
            "Epoch 18997  \tTraining Loss: 0.6915038051855941\tValidation Loss: 0.6914964482764617\n",
            "Epoch 18998  \tTraining Loss: 0.6915037428307043\tValidation Loss: 0.6914963854096592\n",
            "Epoch 18999  \tTraining Loss: 0.6915036804749873\tValidation Loss: 0.6914963225420294\n",
            "Epoch 19000  \tTraining Loss: 0.6915036181184431\tValidation Loss: 0.6914962596735729\n",
            "Epoch 19001  \tTraining Loss: 0.6915035557610714\tValidation Loss: 0.6914961968042891\n",
            "Epoch 19002  \tTraining Loss: 0.6915034934028724\tValidation Loss: 0.6914961339341782\n",
            "Epoch 19003  \tTraining Loss: 0.691503431043846\tValidation Loss: 0.6914960710632403\n",
            "Epoch 19004  \tTraining Loss: 0.6915033686839922\tValidation Loss: 0.6914960081914753\n",
            "Epoch 19005  \tTraining Loss: 0.6915033063233107\tValidation Loss: 0.6914959453188829\n",
            "Epoch 19006  \tTraining Loss: 0.6915032439618017\tValidation Loss: 0.6914958824454635\n",
            "Epoch 19007  \tTraining Loss: 0.6915031815994654\tValidation Loss: 0.6914958195712166\n",
            "Epoch 19008  \tTraining Loss: 0.6915031192363014\tValidation Loss: 0.6914957566961426\n",
            "Epoch 19009  \tTraining Loss: 0.6915030568723097\tValidation Loss: 0.6914956938202412\n",
            "Epoch 19010  \tTraining Loss: 0.6915029945074903\tValidation Loss: 0.6914956309435124\n",
            "Epoch 19011  \tTraining Loss: 0.6915029321418433\tValidation Loss: 0.6914955680659564\n",
            "Epoch 19012  \tTraining Loss: 0.6915028697753686\tValidation Loss: 0.6914955051875727\n",
            "Epoch 19013  \tTraining Loss: 0.6915028074080662\tValidation Loss: 0.6914954423083617\n",
            "Epoch 19014  \tTraining Loss: 0.6915027450399359\tValidation Loss: 0.6914953794283232\n",
            "Epoch 19015  \tTraining Loss: 0.6915026826709778\tValidation Loss: 0.6914953165474571\n",
            "Epoch 19016  \tTraining Loss: 0.6915026203011917\tValidation Loss: 0.6914952536657635\n",
            "Epoch 19017  \tTraining Loss: 0.6915025579305779\tValidation Loss: 0.6914951907832422\n",
            "Epoch 19018  \tTraining Loss: 0.6915024955591361\tValidation Loss: 0.6914951278998933\n",
            "Epoch 19019  \tTraining Loss: 0.6915024331868663\tValidation Loss: 0.6914950650157168\n",
            "Epoch 19020  \tTraining Loss: 0.6915023708137686\tValidation Loss: 0.6914950021307126\n",
            "Epoch 19021  \tTraining Loss: 0.6915023084398427\tValidation Loss: 0.6914949392448806\n",
            "Epoch 19022  \tTraining Loss: 0.6915022460650889\tValidation Loss: 0.6914948763582208\n",
            "Epoch 19023  \tTraining Loss: 0.6915021836895069\tValidation Loss: 0.6914948134707333\n",
            "Epoch 19024  \tTraining Loss: 0.6915021213130969\tValidation Loss: 0.6914947505824179\n",
            "Epoch 19025  \tTraining Loss: 0.6915020589358587\tValidation Loss: 0.6914946876932747\n",
            "Epoch 19026  \tTraining Loss: 0.6915019965577922\tValidation Loss: 0.6914946248033035\n",
            "Epoch 19027  \tTraining Loss: 0.6915019341788974\tValidation Loss: 0.6914945619125044\n",
            "Epoch 19028  \tTraining Loss: 0.6915018717991746\tValidation Loss: 0.6914944990208773\n",
            "Epoch 19029  \tTraining Loss: 0.6915018094186233\tValidation Loss: 0.6914944361284222\n",
            "Epoch 19030  \tTraining Loss: 0.6915017470372437\tValidation Loss: 0.6914943732351391\n",
            "Epoch 19031  \tTraining Loss: 0.6915016846550356\tValidation Loss: 0.6914943103410279\n",
            "Epoch 19032  \tTraining Loss: 0.6915016222719993\tValidation Loss: 0.6914942474460888\n",
            "Epoch 19033  \tTraining Loss: 0.6915015598881344\tValidation Loss: 0.6914941845503212\n",
            "Epoch 19034  \tTraining Loss: 0.691501497503441\tValidation Loss: 0.6914941216537256\n",
            "Epoch 19035  \tTraining Loss: 0.6915014351179193\tValidation Loss: 0.6914940587563017\n",
            "Epoch 19036  \tTraining Loss: 0.691501372731569\tValidation Loss: 0.6914939958580496\n",
            "Epoch 19037  \tTraining Loss: 0.69150131034439\tValidation Loss: 0.6914939329589693\n",
            "Epoch 19038  \tTraining Loss: 0.6915012479563825\tValidation Loss: 0.6914938700590606\n",
            "Epoch 19039  \tTraining Loss: 0.6915011855675461\tValidation Loss: 0.6914938071583235\n",
            "Epoch 19040  \tTraining Loss: 0.6915011231778813\tValidation Loss: 0.6914937442567581\n",
            "Epoch 19041  \tTraining Loss: 0.6915010607873877\tValidation Loss: 0.6914936813543644\n",
            "Epoch 19042  \tTraining Loss: 0.6915009983960653\tValidation Loss: 0.6914936184511421\n",
            "Epoch 19043  \tTraining Loss: 0.6915009360039142\tValidation Loss: 0.6914935555470914\n",
            "Epoch 19044  \tTraining Loss: 0.6915008736109343\tValidation Loss: 0.6914934926422122\n",
            "Epoch 19045  \tTraining Loss: 0.6915008112171256\tValidation Loss: 0.6914934297365043\n",
            "Epoch 19046  \tTraining Loss: 0.6915007488224877\tValidation Loss: 0.6914933668299679\n",
            "Epoch 19047  \tTraining Loss: 0.6915006864270212\tValidation Loss: 0.6914933039226029\n",
            "Epoch 19048  \tTraining Loss: 0.6915006240307257\tValidation Loss: 0.6914932410144091\n",
            "Epoch 19049  \tTraining Loss: 0.6915005616336012\tValidation Loss: 0.6914931781053867\n",
            "Epoch 19050  \tTraining Loss: 0.6915004992356475\tValidation Loss: 0.6914931151955357\n",
            "Epoch 19051  \tTraining Loss: 0.6915004368368649\tValidation Loss: 0.6914930522848558\n",
            "Epoch 19052  \tTraining Loss: 0.6915003744372532\tValidation Loss: 0.6914929893733472\n",
            "Epoch 19053  \tTraining Loss: 0.6915003120368123\tValidation Loss: 0.6914929264610097\n",
            "Epoch 19054  \tTraining Loss: 0.6915002496355424\tValidation Loss: 0.6914928635478433\n",
            "Epoch 19055  \tTraining Loss: 0.6915001872334433\tValidation Loss: 0.6914928006338482\n",
            "Epoch 19056  \tTraining Loss: 0.6915001248305148\tValidation Loss: 0.691492737719024\n",
            "Epoch 19057  \tTraining Loss: 0.691500062426757\tValidation Loss: 0.6914926748033711\n",
            "Epoch 19058  \tTraining Loss: 0.6915000000221702\tValidation Loss: 0.6914926118868888\n",
            "Epoch 19059  \tTraining Loss: 0.6914999376167538\tValidation Loss: 0.6914925489695777\n",
            "Epoch 19060  \tTraining Loss: 0.6914998752105082\tValidation Loss: 0.6914924860514375\n",
            "Epoch 19061  \tTraining Loss: 0.6914998128034331\tValidation Loss: 0.6914924231324682\n",
            "Epoch 19062  \tTraining Loss: 0.6914997503955286\tValidation Loss: 0.6914923602126698\n",
            "Epoch 19063  \tTraining Loss: 0.6914996879867946\tValidation Loss: 0.6914922972920423\n",
            "Epoch 19064  \tTraining Loss: 0.6914996255772311\tValidation Loss: 0.6914922343705855\n",
            "Epoch 19065  \tTraining Loss: 0.6914995631668382\tValidation Loss: 0.6914921714482994\n",
            "Epoch 19066  \tTraining Loss: 0.6914995007556156\tValidation Loss: 0.6914921085251843\n",
            "Epoch 19067  \tTraining Loss: 0.6914994383435635\tValidation Loss: 0.6914920456012397\n",
            "Epoch 19068  \tTraining Loss: 0.6914993759306817\tValidation Loss: 0.6914919826764657\n",
            "Epoch 19069  \tTraining Loss: 0.6914993135169701\tValidation Loss: 0.6914919197508624\n",
            "Epoch 19070  \tTraining Loss: 0.6914992511024289\tValidation Loss: 0.6914918568244298\n",
            "Epoch 19071  \tTraining Loss: 0.691499188687058\tValidation Loss: 0.6914917938971676\n",
            "Epoch 19072  \tTraining Loss: 0.6914991262708574\tValidation Loss: 0.6914917309690761\n",
            "Epoch 19073  \tTraining Loss: 0.6914990638538269\tValidation Loss: 0.691491668040155\n",
            "Epoch 19074  \tTraining Loss: 0.6914990014359665\tValidation Loss: 0.6914916051104045\n",
            "Epoch 19075  \tTraining Loss: 0.6914989390172763\tValidation Loss: 0.6914915421798242\n",
            "Epoch 19076  \tTraining Loss: 0.6914988765977562\tValidation Loss: 0.6914914792484144\n",
            "Epoch 19077  \tTraining Loss: 0.6914988141774061\tValidation Loss: 0.6914914163161751\n",
            "Epoch 19078  \tTraining Loss: 0.6914987517562261\tValidation Loss: 0.6914913533831059\n",
            "Epoch 19079  \tTraining Loss: 0.691498689334216\tValidation Loss: 0.691491290449207\n",
            "Epoch 19080  \tTraining Loss: 0.6914986269113759\tValidation Loss: 0.6914912275144784\n",
            "Epoch 19081  \tTraining Loss: 0.6914985644877056\tValidation Loss: 0.6914911645789201\n",
            "Epoch 19082  \tTraining Loss: 0.6914985020632054\tValidation Loss: 0.6914911016425319\n",
            "Epoch 19083  \tTraining Loss: 0.6914984396378749\tValidation Loss: 0.6914910387053139\n",
            "Epoch 19084  \tTraining Loss: 0.6914983772117143\tValidation Loss: 0.691490975767266\n",
            "Epoch 19085  \tTraining Loss: 0.6914983147847235\tValidation Loss: 0.6914909128283883\n",
            "Epoch 19086  \tTraining Loss: 0.6914982523569023\tValidation Loss: 0.6914908498886805\n",
            "Epoch 19087  \tTraining Loss: 0.691498189928251\tValidation Loss: 0.6914907869481428\n",
            "Epoch 19088  \tTraining Loss: 0.6914981274987693\tValidation Loss: 0.691490724006775\n",
            "Epoch 19089  \tTraining Loss: 0.6914980650684571\tValidation Loss: 0.6914906610645773\n",
            "Epoch 19090  \tTraining Loss: 0.6914980026373146\tValidation Loss: 0.6914905981215494\n",
            "Epoch 19091  \tTraining Loss: 0.6914979402053418\tValidation Loss: 0.6914905351776914\n",
            "Epoch 19092  \tTraining Loss: 0.6914978777725385\tValidation Loss: 0.6914904722330032\n",
            "Epoch 19093  \tTraining Loss: 0.6914978153389048\tValidation Loss: 0.691490409287485\n",
            "Epoch 19094  \tTraining Loss: 0.6914977529044403\tValidation Loss: 0.6914903463411365\n",
            "Epoch 19095  \tTraining Loss: 0.6914976904691453\tValidation Loss: 0.6914902833939576\n",
            "Epoch 19096  \tTraining Loss: 0.6914976280330198\tValidation Loss: 0.6914902204459485\n",
            "Epoch 19097  \tTraining Loss: 0.6914975655960637\tValidation Loss: 0.6914901574971091\n",
            "Epoch 19098  \tTraining Loss: 0.691497503158277\tValidation Loss: 0.6914900945474395\n",
            "Epoch 19099  \tTraining Loss: 0.6914974407196595\tValidation Loss: 0.6914900315969392\n",
            "Epoch 19100  \tTraining Loss: 0.6914973782802112\tValidation Loss: 0.6914899686456087\n",
            "Epoch 19101  \tTraining Loss: 0.6914973158399322\tValidation Loss: 0.6914899056934476\n",
            "Epoch 19102  \tTraining Loss: 0.6914972533988224\tValidation Loss: 0.6914898427404561\n",
            "Epoch 19103  \tTraining Loss: 0.6914971909568818\tValidation Loss: 0.6914897797866341\n",
            "Epoch 19104  \tTraining Loss: 0.6914971285141104\tValidation Loss: 0.6914897168319815\n",
            "Epoch 19105  \tTraining Loss: 0.691497066070508\tValidation Loss: 0.6914896538764983\n",
            "Epoch 19106  \tTraining Loss: 0.6914970036260747\tValidation Loss: 0.6914895909201844\n",
            "Epoch 19107  \tTraining Loss: 0.6914969411808105\tValidation Loss: 0.6914895279630399\n",
            "Epoch 19108  \tTraining Loss: 0.6914968787347151\tValidation Loss: 0.6914894650050647\n",
            "Epoch 19109  \tTraining Loss: 0.6914968162877888\tValidation Loss: 0.6914894020462589\n",
            "Epoch 19110  \tTraining Loss: 0.6914967538400314\tValidation Loss: 0.6914893390866221\n",
            "Epoch 19111  \tTraining Loss: 0.6914966913914429\tValidation Loss: 0.6914892761261546\n",
            "Epoch 19112  \tTraining Loss: 0.6914966289420232\tValidation Loss: 0.6914892131648563\n",
            "Epoch 19113  \tTraining Loss: 0.6914965664917725\tValidation Loss: 0.6914891502027272\n",
            "Epoch 19114  \tTraining Loss: 0.6914965040406904\tValidation Loss: 0.691489087239767\n",
            "Epoch 19115  \tTraining Loss: 0.6914964415887771\tValidation Loss: 0.691489024275976\n",
            "Epoch 19116  \tTraining Loss: 0.6914963791360326\tValidation Loss: 0.691488961311354\n",
            "Epoch 19117  \tTraining Loss: 0.6914963166824567\tValidation Loss: 0.691488898345901\n",
            "Epoch 19118  \tTraining Loss: 0.6914962542280495\tValidation Loss: 0.691488835379617\n",
            "Epoch 19119  \tTraining Loss: 0.6914961917728109\tValidation Loss: 0.6914887724125018\n",
            "Epoch 19120  \tTraining Loss: 0.6914961293167409\tValidation Loss: 0.6914887094445556\n",
            "Epoch 19121  \tTraining Loss: 0.6914960668598394\tValidation Loss: 0.6914886464757783\n",
            "Epoch 19122  \tTraining Loss: 0.6914960044021066\tValidation Loss: 0.6914885835061698\n",
            "Epoch 19123  \tTraining Loss: 0.6914959419435421\tValidation Loss: 0.6914885205357301\n",
            "Epoch 19124  \tTraining Loss: 0.6914958794841461\tValidation Loss: 0.6914884575644592\n",
            "Epoch 19125  \tTraining Loss: 0.6914958170239185\tValidation Loss: 0.6914883945923569\n",
            "Epoch 19126  \tTraining Loss: 0.6914957545628593\tValidation Loss: 0.6914883316194234\n",
            "Epoch 19127  \tTraining Loss: 0.6914956921009685\tValidation Loss: 0.6914882686456585\n",
            "Epoch 19128  \tTraining Loss: 0.6914956296382458\tValidation Loss: 0.6914882056710622\n",
            "Epoch 19129  \tTraining Loss: 0.6914955671746916\tValidation Loss: 0.6914881426956345\n",
            "Epoch 19130  \tTraining Loss: 0.6914955047103056\tValidation Loss: 0.6914880797193753\n",
            "Epoch 19131  \tTraining Loss: 0.6914954422450879\tValidation Loss: 0.6914880167422848\n",
            "Epoch 19132  \tTraining Loss: 0.6914953797790382\tValidation Loss: 0.6914879537643627\n",
            "Epoch 19133  \tTraining Loss: 0.6914953173121567\tValidation Loss: 0.6914878907856089\n",
            "Epoch 19134  \tTraining Loss: 0.6914952548444433\tValidation Loss: 0.6914878278060237\n",
            "Epoch 19135  \tTraining Loss: 0.691495192375898\tValidation Loss: 0.6914877648256068\n",
            "Epoch 19136  \tTraining Loss: 0.6914951299065207\tValidation Loss: 0.6914877018443583\n",
            "Epoch 19137  \tTraining Loss: 0.6914950674363115\tValidation Loss: 0.6914876388622782\n",
            "Epoch 19138  \tTraining Loss: 0.6914950049652702\tValidation Loss: 0.6914875758793662\n",
            "Epoch 19139  \tTraining Loss: 0.6914949424933967\tValidation Loss: 0.6914875128956226\n",
            "Epoch 19140  \tTraining Loss: 0.6914948800206914\tValidation Loss: 0.691487449911047\n",
            "Epoch 19141  \tTraining Loss: 0.6914948175471538\tValidation Loss: 0.6914873869256398\n",
            "Epoch 19142  \tTraining Loss: 0.691494755072784\tValidation Loss: 0.6914873239394007\n",
            "Epoch 19143  \tTraining Loss: 0.691494692597582\tValidation Loss: 0.6914872609523297\n",
            "Epoch 19144  \tTraining Loss: 0.6914946301215479\tValidation Loss: 0.6914871979644267\n",
            "Epoch 19145  \tTraining Loss: 0.6914945676446813\tValidation Loss: 0.6914871349756918\n",
            "Epoch 19146  \tTraining Loss: 0.6914945051669826\tValidation Loss: 0.6914870719861249\n",
            "Epoch 19147  \tTraining Loss: 0.6914944426884516\tValidation Loss: 0.6914870089957259\n",
            "Epoch 19148  \tTraining Loss: 0.691494380209088\tValidation Loss: 0.691486946004495\n",
            "Epoch 19149  \tTraining Loss: 0.6914943177288922\tValidation Loss: 0.691486883012432\n",
            "Epoch 19150  \tTraining Loss: 0.6914942552478639\tValidation Loss: 0.6914868200195367\n",
            "Epoch 19151  \tTraining Loss: 0.6914941927660031\tValidation Loss: 0.6914867570258093\n",
            "Epoch 19152  \tTraining Loss: 0.6914941302833097\tValidation Loss: 0.6914866940312497\n",
            "Epoch 19153  \tTraining Loss: 0.691494067799784\tValidation Loss: 0.6914866310358581\n",
            "Epoch 19154  \tTraining Loss: 0.6914940053154255\tValidation Loss: 0.6914865680396338\n",
            "Epoch 19155  \tTraining Loss: 0.6914939428302345\tValidation Loss: 0.6914865050425776\n",
            "Epoch 19156  \tTraining Loss: 0.6914938803442108\tValidation Loss: 0.6914864420446889\n",
            "Epoch 19157  \tTraining Loss: 0.6914938178573544\tValidation Loss: 0.6914863790459678\n",
            "Epoch 19158  \tTraining Loss: 0.6914937553696654\tValidation Loss: 0.6914863160464144\n",
            "Epoch 19159  \tTraining Loss: 0.6914936928811436\tValidation Loss: 0.6914862530460285\n",
            "Epoch 19160  \tTraining Loss: 0.6914936303917889\tValidation Loss: 0.6914861900448102\n",
            "Epoch 19161  \tTraining Loss: 0.6914935679016015\tValidation Loss: 0.6914861270427594\n",
            "Epoch 19162  \tTraining Loss: 0.6914935054105813\tValidation Loss: 0.6914860640398759\n",
            "Epoch 19163  \tTraining Loss: 0.691493442918728\tValidation Loss: 0.69148600103616\n",
            "Epoch 19164  \tTraining Loss: 0.6914933804260421\tValidation Loss: 0.6914859380316114\n",
            "Epoch 19165  \tTraining Loss: 0.691493317932523\tValidation Loss: 0.6914858750262303\n",
            "Epoch 19166  \tTraining Loss: 0.6914932554381711\tValidation Loss: 0.6914858120200162\n",
            "Epoch 19167  \tTraining Loss: 0.6914931929429861\tValidation Loss: 0.6914857490129698\n",
            "Epoch 19168  \tTraining Loss: 0.6914931304469679\tValidation Loss: 0.6914856860050903\n",
            "Epoch 19169  \tTraining Loss: 0.6914930679501168\tValidation Loss: 0.6914856229963783\n",
            "Epoch 19170  \tTraining Loss: 0.6914930054524324\tValidation Loss: 0.6914855599868335\n",
            "Epoch 19171  \tTraining Loss: 0.691492942953915\tValidation Loss: 0.6914854969764556\n",
            "Epoch 19172  \tTraining Loss: 0.6914928804545644\tValidation Loss: 0.6914854339652451\n",
            "Epoch 19173  \tTraining Loss: 0.6914928179543804\tValidation Loss: 0.6914853709532014\n",
            "Epoch 19174  \tTraining Loss: 0.6914927554533633\tValidation Loss: 0.6914853079403249\n",
            "Epoch 19175  \tTraining Loss: 0.6914926929515128\tValidation Loss: 0.6914852449266154\n",
            "Epoch 19176  \tTraining Loss: 0.6914926304488291\tValidation Loss: 0.691485181912073\n",
            "Epoch 19177  \tTraining Loss: 0.691492567945312\tValidation Loss: 0.6914851188966975\n",
            "Epoch 19178  \tTraining Loss: 0.6914925054409614\tValidation Loss: 0.6914850558804888\n",
            "Epoch 19179  \tTraining Loss: 0.6914924429357775\tValidation Loss: 0.6914849928634471\n",
            "Epoch 19180  \tTraining Loss: 0.6914923804297601\tValidation Loss: 0.6914849298455722\n",
            "Epoch 19181  \tTraining Loss: 0.6914923179229092\tValidation Loss: 0.6914848668268642\n",
            "Epoch 19182  \tTraining Loss: 0.6914922554152247\tValidation Loss: 0.6914848038073228\n",
            "Epoch 19183  \tTraining Loss: 0.6914921929067066\tValidation Loss: 0.6914847407869483\n",
            "Epoch 19184  \tTraining Loss: 0.691492130397355\tValidation Loss: 0.6914846777657405\n",
            "Epoch 19185  \tTraining Loss: 0.6914920678871699\tValidation Loss: 0.6914846147436994\n",
            "Epoch 19186  \tTraining Loss: 0.6914920053761509\tValidation Loss: 0.6914845517208248\n",
            "Epoch 19187  \tTraining Loss: 0.6914919428642982\tValidation Loss: 0.691484488697117\n",
            "Epoch 19188  \tTraining Loss: 0.6914918803516118\tValidation Loss: 0.6914844256725757\n",
            "Epoch 19189  \tTraining Loss: 0.6914918178380917\tValidation Loss: 0.6914843626472009\n",
            "Epoch 19190  \tTraining Loss: 0.6914917553237377\tValidation Loss: 0.6914842996209926\n",
            "Epoch 19191  \tTraining Loss: 0.6914916928085498\tValidation Loss: 0.6914842365939509\n",
            "Epoch 19192  \tTraining Loss: 0.6914916302925282\tValidation Loss: 0.6914841735660755\n",
            "Epoch 19193  \tTraining Loss: 0.6914915677756726\tValidation Loss: 0.6914841105373666\n",
            "Epoch 19194  \tTraining Loss: 0.6914915052579831\tValidation Loss: 0.691484047507824\n",
            "Epoch 19195  \tTraining Loss: 0.6914914427394594\tValidation Loss: 0.6914839844774477\n",
            "Epoch 19196  \tTraining Loss: 0.6914913802201019\tValidation Loss: 0.6914839214462379\n",
            "Epoch 19197  \tTraining Loss: 0.6914913176999103\tValidation Loss: 0.6914838584141941\n",
            "Epoch 19198  \tTraining Loss: 0.6914912551788848\tValidation Loss: 0.6914837953813168\n",
            "Epoch 19199  \tTraining Loss: 0.6914911926570249\tValidation Loss: 0.6914837323476055\n",
            "Epoch 19200  \tTraining Loss: 0.691491130134331\tValidation Loss: 0.6914836693130604\n",
            "Epoch 19201  \tTraining Loss: 0.6914910676108027\tValidation Loss: 0.6914836062776816\n",
            "Epoch 19202  \tTraining Loss: 0.6914910050864405\tValidation Loss: 0.6914835432414688\n",
            "Epoch 19203  \tTraining Loss: 0.6914909425612438\tValidation Loss: 0.691483480204422\n",
            "Epoch 19204  \tTraining Loss: 0.6914908800352129\tValidation Loss: 0.6914834171665412\n",
            "Epoch 19205  \tTraining Loss: 0.6914908175083478\tValidation Loss: 0.6914833541278265\n",
            "Epoch 19206  \tTraining Loss: 0.6914907549806482\tValidation Loss: 0.6914832910882778\n",
            "Epoch 19207  \tTraining Loss: 0.6914906924521143\tValidation Loss: 0.6914832280478949\n",
            "Epoch 19208  \tTraining Loss: 0.6914906299227459\tValidation Loss: 0.691483165006678\n",
            "Epoch 19209  \tTraining Loss: 0.691490567392543\tValidation Loss: 0.6914831019646269\n",
            "Epoch 19210  \tTraining Loss: 0.6914905048615058\tValidation Loss: 0.6914830389217416\n",
            "Epoch 19211  \tTraining Loss: 0.6914904423296339\tValidation Loss: 0.6914829758780222\n",
            "Epoch 19212  \tTraining Loss: 0.6914903797969273\tValidation Loss: 0.6914829128334684\n",
            "Epoch 19213  \tTraining Loss: 0.6914903172633863\tValidation Loss: 0.6914828497880803\n",
            "Epoch 19214  \tTraining Loss: 0.6914902547290106\tValidation Loss: 0.691482786741858\n",
            "Epoch 19215  \tTraining Loss: 0.6914901921938003\tValidation Loss: 0.6914827236948015\n",
            "Epoch 19216  \tTraining Loss: 0.6914901296577555\tValidation Loss: 0.6914826606469104\n",
            "Epoch 19217  \tTraining Loss: 0.6914900671208757\tValidation Loss: 0.691482597598185\n",
            "Epoch 19218  \tTraining Loss: 0.691490004583161\tValidation Loss: 0.6914825345486251\n",
            "Epoch 19219  \tTraining Loss: 0.6914899420446118\tValidation Loss: 0.6914824714982307\n",
            "Epoch 19220  \tTraining Loss: 0.6914898795052277\tValidation Loss: 0.6914824084470019\n",
            "Epoch 19221  \tTraining Loss: 0.6914898169650086\tValidation Loss: 0.6914823453949385\n",
            "Epoch 19222  \tTraining Loss: 0.6914897544239547\tValidation Loss: 0.6914822823420405\n",
            "Epoch 19223  \tTraining Loss: 0.6914896918820659\tValidation Loss: 0.6914822192883079\n",
            "Epoch 19224  \tTraining Loss: 0.6914896293393419\tValidation Loss: 0.6914821562337407\n",
            "Epoch 19225  \tTraining Loss: 0.6914895667957831\tValidation Loss: 0.6914820931783385\n",
            "Epoch 19226  \tTraining Loss: 0.6914895042513892\tValidation Loss: 0.6914820301221017\n",
            "Epoch 19227  \tTraining Loss: 0.6914894417061602\tValidation Loss: 0.6914819670650305\n",
            "Epoch 19228  \tTraining Loss: 0.6914893791600961\tValidation Loss: 0.6914819040071242\n",
            "Epoch 19229  \tTraining Loss: 0.691489316613197\tValidation Loss: 0.691481840948383\n",
            "Epoch 19230  \tTraining Loss: 0.6914892540654626\tValidation Loss: 0.6914817778888073\n",
            "Epoch 19231  \tTraining Loss: 0.6914891915168929\tValidation Loss: 0.6914817148283965\n",
            "Epoch 19232  \tTraining Loss: 0.691489128967488\tValidation Loss: 0.6914816517671507\n",
            "Epoch 19233  \tTraining Loss: 0.6914890664172478\tValidation Loss: 0.69148158870507\n",
            "Epoch 19234  \tTraining Loss: 0.6914890038661724\tValidation Loss: 0.6914815256421544\n",
            "Epoch 19235  \tTraining Loss: 0.6914889413142616\tValidation Loss: 0.6914814625784037\n",
            "Epoch 19236  \tTraining Loss: 0.6914888787615154\tValidation Loss: 0.6914813995138178\n",
            "Epoch 19237  \tTraining Loss: 0.6914888162079338\tValidation Loss: 0.691481336448397\n",
            "Epoch 19238  \tTraining Loss: 0.6914887536535168\tValidation Loss: 0.691481273382141\n",
            "Epoch 19239  \tTraining Loss: 0.6914886910982642\tValidation Loss: 0.6914812103150499\n",
            "Epoch 19240  \tTraining Loss: 0.6914886285421762\tValidation Loss: 0.6914811472471235\n",
            "Epoch 19241  \tTraining Loss: 0.6914885659852524\tValidation Loss: 0.691481084178362\n",
            "Epoch 19242  \tTraining Loss: 0.6914885034274932\tValidation Loss: 0.6914810211087651\n",
            "Epoch 19243  \tTraining Loss: 0.6914884408688984\tValidation Loss: 0.691480958038333\n",
            "Epoch 19244  \tTraining Loss: 0.691488378309468\tValidation Loss: 0.6914808949670654\n",
            "Epoch 19245  \tTraining Loss: 0.6914883157492018\tValidation Loss: 0.6914808318949625\n",
            "Epoch 19246  \tTraining Loss: 0.6914882531880999\tValidation Loss: 0.6914807688220244\n",
            "Epoch 19247  \tTraining Loss: 0.6914881906261622\tValidation Loss: 0.6914807057482506\n",
            "Epoch 19248  \tTraining Loss: 0.6914881280633887\tValidation Loss: 0.6914806426736415\n",
            "Epoch 19249  \tTraining Loss: 0.6914880654997794\tValidation Loss: 0.6914805795981969\n",
            "Epoch 19250  \tTraining Loss: 0.6914880029353342\tValidation Loss: 0.6914805165219164\n",
            "Epoch 19251  \tTraining Loss: 0.6914879403700531\tValidation Loss: 0.6914804534448007\n",
            "Epoch 19252  \tTraining Loss: 0.6914878778039362\tValidation Loss: 0.6914803903668494\n",
            "Epoch 19253  \tTraining Loss: 0.6914878152369831\tValidation Loss: 0.6914803272880624\n",
            "Epoch 19254  \tTraining Loss: 0.6914877526691942\tValidation Loss: 0.6914802642084396\n",
            "Epoch 19255  \tTraining Loss: 0.6914876901005692\tValidation Loss: 0.6914802011279811\n",
            "Epoch 19256  \tTraining Loss: 0.6914876275311082\tValidation Loss: 0.691480138046687\n",
            "Epoch 19257  \tTraining Loss: 0.6914875649608109\tValidation Loss: 0.691480074964557\n",
            "Epoch 19258  \tTraining Loss: 0.6914875023896776\tValidation Loss: 0.6914800118815912\n",
            "Epoch 19259  \tTraining Loss: 0.6914874398177081\tValidation Loss: 0.6914799487977896\n",
            "Epoch 19260  \tTraining Loss: 0.6914873772449025\tValidation Loss: 0.6914798857131521\n",
            "Epoch 19261  \tTraining Loss: 0.6914873146712605\tValidation Loss: 0.6914798226276788\n",
            "Epoch 19262  \tTraining Loss: 0.6914872520967823\tValidation Loss: 0.6914797595413694\n",
            "Epoch 19263  \tTraining Loss: 0.6914871895214677\tValidation Loss: 0.691479696454224\n",
            "Epoch 19264  \tTraining Loss: 0.6914871269453169\tValidation Loss: 0.6914796333662427\n",
            "Epoch 19265  \tTraining Loss: 0.6914870643683296\tValidation Loss: 0.6914795702774252\n",
            "Epoch 19266  \tTraining Loss: 0.691487001790506\tValidation Loss: 0.6914795071877717\n",
            "Epoch 19267  \tTraining Loss: 0.6914869392118458\tValidation Loss: 0.6914794440972821\n",
            "Epoch 19268  \tTraining Loss: 0.6914868766323493\tValidation Loss: 0.6914793810059564\n",
            "Epoch 19269  \tTraining Loss: 0.6914868140520162\tValidation Loss: 0.6914793179137945\n",
            "Epoch 19270  \tTraining Loss: 0.6914867514708465\tValidation Loss: 0.6914792548207962\n",
            "Epoch 19271  \tTraining Loss: 0.6914866888888404\tValidation Loss: 0.6914791917269618\n",
            "Epoch 19272  \tTraining Loss: 0.6914866263059974\tValidation Loss: 0.6914791286322911\n",
            "Epoch 19273  \tTraining Loss: 0.6914865637223179\tValidation Loss: 0.691479065536784\n",
            "Epoch 19274  \tTraining Loss: 0.6914865011378017\tValidation Loss: 0.6914790024404406\n",
            "Epoch 19275  \tTraining Loss: 0.6914864385524488\tValidation Loss: 0.6914789393432608\n",
            "Epoch 19276  \tTraining Loss: 0.6914863759662592\tValidation Loss: 0.6914788762452445\n",
            "Epoch 19277  \tTraining Loss: 0.6914863133792327\tValidation Loss: 0.6914788131463918\n",
            "Epoch 19278  \tTraining Loss: 0.6914862507913695\tValidation Loss: 0.6914787500467027\n",
            "Epoch 19279  \tTraining Loss: 0.6914861882026693\tValidation Loss: 0.6914786869461768\n",
            "Epoch 19280  \tTraining Loss: 0.6914861256131323\tValidation Loss: 0.6914786238448145\n",
            "Epoch 19281  \tTraining Loss: 0.6914860630227584\tValidation Loss: 0.6914785607426157\n",
            "Epoch 19282  \tTraining Loss: 0.6914860004315474\tValidation Loss: 0.6914784976395801\n",
            "Epoch 19283  \tTraining Loss: 0.6914859378394995\tValidation Loss: 0.691478434535708\n",
            "Epoch 19284  \tTraining Loss: 0.6914858752466145\tValidation Loss: 0.691478371430999\n",
            "Epoch 19285  \tTraining Loss: 0.6914858126528926\tValidation Loss: 0.6914783083254533\n",
            "Epoch 19286  \tTraining Loss: 0.6914857500583333\tValidation Loss: 0.691478245219071\n",
            "Epoch 19287  \tTraining Loss: 0.6914856874629371\tValidation Loss: 0.6914781821118517\n",
            "Epoch 19288  \tTraining Loss: 0.6914856248667037\tValidation Loss: 0.6914781190037957\n",
            "Epoch 19289  \tTraining Loss: 0.6914855622696331\tValidation Loss: 0.6914780558949029\n",
            "Epoch 19290  \tTraining Loss: 0.6914854996717253\tValidation Loss: 0.6914779927851731\n",
            "Epoch 19291  \tTraining Loss: 0.6914854370729802\tValidation Loss: 0.6914779296746062\n",
            "Epoch 19292  \tTraining Loss: 0.6914853744733978\tValidation Loss: 0.6914778665632025\n",
            "Epoch 19293  \tTraining Loss: 0.691485311872978\tValidation Loss: 0.6914778034509619\n",
            "Epoch 19294  \tTraining Loss: 0.691485249271721\tValidation Loss: 0.691477740337884\n",
            "Epoch 19295  \tTraining Loss: 0.6914851866696263\tValidation Loss: 0.6914776772239691\n",
            "Epoch 19296  \tTraining Loss: 0.6914851240666944\tValidation Loss: 0.6914776141092172\n",
            "Epoch 19297  \tTraining Loss: 0.6914850614629249\tValidation Loss: 0.691477550993628\n",
            "Epoch 19298  \tTraining Loss: 0.6914849988583179\tValidation Loss: 0.6914774878772016\n",
            "Epoch 19299  \tTraining Loss: 0.6914849362528734\tValidation Loss: 0.6914774247599382\n",
            "Epoch 19300  \tTraining Loss: 0.6914848736465914\tValidation Loss: 0.6914773616418374\n",
            "Epoch 19301  \tTraining Loss: 0.6914848110394717\tValidation Loss: 0.6914772985228994\n",
            "Epoch 19302  \tTraining Loss: 0.6914847484315144\tValidation Loss: 0.6914772354031239\n",
            "Epoch 19303  \tTraining Loss: 0.6914846858227193\tValidation Loss: 0.6914771722825112\n",
            "Epoch 19304  \tTraining Loss: 0.6914846232130866\tValidation Loss: 0.6914771091610611\n",
            "Epoch 19305  \tTraining Loss: 0.6914845606026161\tValidation Loss: 0.6914770460387737\n",
            "Epoch 19306  \tTraining Loss: 0.6914844979913078\tValidation Loss: 0.6914769829156486\n",
            "Epoch 19307  \tTraining Loss: 0.6914844353791618\tValidation Loss: 0.6914769197916861\n",
            "Epoch 19308  \tTraining Loss: 0.6914843727661778\tValidation Loss: 0.6914768566668862\n",
            "Epoch 19309  \tTraining Loss: 0.6914843101523559\tValidation Loss: 0.6914767935412487\n",
            "Epoch 19310  \tTraining Loss: 0.6914842475376961\tValidation Loss: 0.6914767304147736\n",
            "Epoch 19311  \tTraining Loss: 0.6914841849221984\tValidation Loss: 0.6914766672874608\n",
            "Epoch 19312  \tTraining Loss: 0.6914841223058628\tValidation Loss: 0.6914766041593103\n",
            "Epoch 19313  \tTraining Loss: 0.691484059688689\tValidation Loss: 0.6914765410303222\n",
            "Epoch 19314  \tTraining Loss: 0.6914839970706772\tValidation Loss: 0.6914764779004965\n",
            "Epoch 19315  \tTraining Loss: 0.6914839344518274\tValidation Loss: 0.6914764147698329\n",
            "Epoch 19316  \tTraining Loss: 0.6914838718321393\tValidation Loss: 0.6914763516383314\n",
            "Epoch 19317  \tTraining Loss: 0.6914838092116131\tValidation Loss: 0.6914762885059922\n",
            "Epoch 19318  \tTraining Loss: 0.6914837465902488\tValidation Loss: 0.6914762253728151\n",
            "Epoch 19319  \tTraining Loss: 0.6914836839680463\tValidation Loss: 0.6914761622388002\n",
            "Epoch 19320  \tTraining Loss: 0.6914836213450054\tValidation Loss: 0.6914760991039471\n",
            "Epoch 19321  \tTraining Loss: 0.6914835587211262\tValidation Loss: 0.6914760359682562\n",
            "Epoch 19322  \tTraining Loss: 0.6914834960964087\tValidation Loss: 0.6914759728317275\n",
            "Epoch 19323  \tTraining Loss: 0.691483433470853\tValidation Loss: 0.6914759096943605\n",
            "Epoch 19324  \tTraining Loss: 0.6914833708444585\tValidation Loss: 0.6914758465561556\n",
            "Epoch 19325  \tTraining Loss: 0.6914833082172258\tValidation Loss: 0.6914757834171124\n",
            "Epoch 19326  \tTraining Loss: 0.6914832455891547\tValidation Loss: 0.6914757202772311\n",
            "Epoch 19327  \tTraining Loss: 0.691483182960245\tValidation Loss: 0.6914756571365118\n",
            "Epoch 19328  \tTraining Loss: 0.6914831203304969\tValidation Loss: 0.6914755939949542\n",
            "Epoch 19329  \tTraining Loss: 0.6914830576999099\tValidation Loss: 0.6914755308525583\n",
            "Epoch 19330  \tTraining Loss: 0.6914829950684847\tValidation Loss: 0.6914754677093242\n",
            "Epoch 19331  \tTraining Loss: 0.6914829324362206\tValidation Loss: 0.6914754045652518\n",
            "Epoch 19332  \tTraining Loss: 0.6914828698031179\tValidation Loss: 0.6914753414203411\n",
            "Epoch 19333  \tTraining Loss: 0.6914828071691765\tValidation Loss: 0.6914752782745919\n",
            "Epoch 19334  \tTraining Loss: 0.6914827445343963\tValidation Loss: 0.6914752151280044\n",
            "Epoch 19335  \tTraining Loss: 0.6914826818987774\tValidation Loss: 0.6914751519805783\n",
            "Epoch 19336  \tTraining Loss: 0.6914826192623196\tValidation Loss: 0.6914750888323138\n",
            "Epoch 19337  \tTraining Loss: 0.691482556625023\tValidation Loss: 0.6914750256832107\n",
            "Epoch 19338  \tTraining Loss: 0.6914824939868875\tValidation Loss: 0.6914749625332692\n",
            "Epoch 19339  \tTraining Loss: 0.6914824313479132\tValidation Loss: 0.6914748993824891\n",
            "Epoch 19340  \tTraining Loss: 0.6914823687080998\tValidation Loss: 0.6914748362308704\n",
            "Epoch 19341  \tTraining Loss: 0.6914823060674474\tValidation Loss: 0.691474773078413\n",
            "Epoch 19342  \tTraining Loss: 0.691482243425956\tValidation Loss: 0.6914747099251168\n",
            "Epoch 19343  \tTraining Loss: 0.6914821807836256\tValidation Loss: 0.691474646770982\n",
            "Epoch 19344  \tTraining Loss: 0.6914821181404561\tValidation Loss: 0.6914745836160086\n",
            "Epoch 19345  \tTraining Loss: 0.6914820554964474\tValidation Loss: 0.6914745204601962\n",
            "Epoch 19346  \tTraining Loss: 0.6914819928515996\tValidation Loss: 0.691474457303545\n",
            "Epoch 19347  \tTraining Loss: 0.6914819302059126\tValidation Loss: 0.691474394146055\n",
            "Epoch 19348  \tTraining Loss: 0.6914818675593865\tValidation Loss: 0.691474330987726\n",
            "Epoch 19349  \tTraining Loss: 0.691481804912021\tValidation Loss: 0.6914742678285583\n",
            "Epoch 19350  \tTraining Loss: 0.6914817422638163\tValidation Loss: 0.6914742046685515\n",
            "Epoch 19351  \tTraining Loss: 0.6914816796147721\tValidation Loss: 0.6914741415077057\n",
            "Epoch 19352  \tTraining Loss: 0.6914816169648887\tValidation Loss: 0.6914740783460208\n",
            "Epoch 19353  \tTraining Loss: 0.6914815543141658\tValidation Loss: 0.691474015183497\n",
            "Epoch 19354  \tTraining Loss: 0.6914814916626036\tValidation Loss: 0.691473952020134\n",
            "Epoch 19355  \tTraining Loss: 0.6914814290102018\tValidation Loss: 0.6914738888559319\n",
            "Epoch 19356  \tTraining Loss: 0.6914813663569606\tValidation Loss: 0.6914738256908906\n",
            "Epoch 19357  \tTraining Loss: 0.6914813037028797\tValidation Loss: 0.6914737625250101\n",
            "Epoch 19358  \tTraining Loss: 0.6914812410479594\tValidation Loss: 0.6914736993582905\n",
            "Epoch 19359  \tTraining Loss: 0.6914811783921995\tValidation Loss: 0.6914736361907315\n",
            "Epoch 19360  \tTraining Loss: 0.6914811157355998\tValidation Loss: 0.6914735730223333\n",
            "Epoch 19361  \tTraining Loss: 0.6914810530781605\tValidation Loss: 0.6914735098530956\n",
            "Epoch 19362  \tTraining Loss: 0.6914809904198816\tValidation Loss: 0.6914734466830187\n",
            "Epoch 19363  \tTraining Loss: 0.6914809277607629\tValidation Loss: 0.6914733835121023\n",
            "Epoch 19364  \tTraining Loss: 0.6914808651008044\tValidation Loss: 0.6914733203403465\n",
            "Epoch 19365  \tTraining Loss: 0.6914808024400061\tValidation Loss: 0.6914732571677513\n",
            "Epoch 19366  \tTraining Loss: 0.6914807397783679\tValidation Loss: 0.6914731939943165\n",
            "Epoch 19367  \tTraining Loss: 0.69148067711589\tValidation Loss: 0.6914731308200421\n",
            "Epoch 19368  \tTraining Loss: 0.691480614452572\tValidation Loss: 0.6914730676449281\n",
            "Epoch 19369  \tTraining Loss: 0.6914805517884142\tValidation Loss: 0.6914730044689748\n",
            "Epoch 19370  \tTraining Loss: 0.6914804891234163\tValidation Loss: 0.6914729412921815\n",
            "Epoch 19371  \tTraining Loss: 0.6914804264575785\tValidation Loss: 0.6914728781145487\n",
            "Epoch 19372  \tTraining Loss: 0.6914803637909005\tValidation Loss: 0.6914728149360762\n",
            "Epoch 19373  \tTraining Loss: 0.6914803011233824\tValidation Loss: 0.6914727517567638\n",
            "Epoch 19374  \tTraining Loss: 0.6914802384550244\tValidation Loss: 0.6914726885766118\n",
            "Epoch 19375  \tTraining Loss: 0.691480175785826\tValidation Loss: 0.6914726253956199\n",
            "Epoch 19376  \tTraining Loss: 0.6914801131157876\tValidation Loss: 0.6914725622137882\n",
            "Epoch 19377  \tTraining Loss: 0.6914800504449088\tValidation Loss: 0.6914724990311165\n",
            "Epoch 19378  \tTraining Loss: 0.69147998777319\tValidation Loss: 0.691472435847605\n",
            "Epoch 19379  \tTraining Loss: 0.6914799251006306\tValidation Loss: 0.6914723726632535\n",
            "Epoch 19380  \tTraining Loss: 0.6914798624272311\tValidation Loss: 0.6914723094780622\n",
            "Epoch 19381  \tTraining Loss: 0.6914797997529912\tValidation Loss: 0.6914722462920305\n",
            "Epoch 19382  \tTraining Loss: 0.6914797370779109\tValidation Loss: 0.6914721831051589\n",
            "Epoch 19383  \tTraining Loss: 0.6914796744019901\tValidation Loss: 0.6914721199174474\n",
            "Epoch 19384  \tTraining Loss: 0.6914796117252289\tValidation Loss: 0.6914720567288956\n",
            "Epoch 19385  \tTraining Loss: 0.6914795490476271\tValidation Loss: 0.6914719935395037\n",
            "Epoch 19386  \tTraining Loss: 0.691479486369185\tValidation Loss: 0.6914719303492716\n",
            "Epoch 19387  \tTraining Loss: 0.6914794236899021\tValidation Loss: 0.6914718671581992\n",
            "Epoch 19388  \tTraining Loss: 0.6914793610097787\tValidation Loss: 0.6914718039662867\n",
            "Epoch 19389  \tTraining Loss: 0.6914792983288147\tValidation Loss: 0.6914717407735339\n",
            "Epoch 19390  \tTraining Loss: 0.69147923564701\tValidation Loss: 0.6914716775799407\n",
            "Epoch 19391  \tTraining Loss: 0.6914791729643646\tValidation Loss: 0.691471614385507\n",
            "Epoch 19392  \tTraining Loss: 0.6914791102808784\tValidation Loss: 0.6914715511902331\n",
            "Epoch 19393  \tTraining Loss: 0.6914790475965515\tValidation Loss: 0.6914714879941186\n",
            "Epoch 19394  \tTraining Loss: 0.6914789849113838\tValidation Loss: 0.6914714247971637\n",
            "Epoch 19395  \tTraining Loss: 0.6914789222253752\tValidation Loss: 0.6914713615993684\n",
            "Epoch 19396  \tTraining Loss: 0.6914788595385257\tValidation Loss: 0.6914712984007325\n",
            "Epoch 19397  \tTraining Loss: 0.6914787968508355\tValidation Loss: 0.6914712352012561\n",
            "Epoch 19398  \tTraining Loss: 0.6914787341623041\tValidation Loss: 0.691471172000939\n",
            "Epoch 19399  \tTraining Loss: 0.6914786714729318\tValidation Loss: 0.6914711087997812\n",
            "Epoch 19400  \tTraining Loss: 0.6914786087827185\tValidation Loss: 0.6914710455977829\n",
            "Epoch 19401  \tTraining Loss: 0.6914785460916643\tValidation Loss: 0.6914709823949439\n",
            "Epoch 19402  \tTraining Loss: 0.6914784833997689\tValidation Loss: 0.691470919191264\n",
            "Epoch 19403  \tTraining Loss: 0.6914784207070322\tValidation Loss: 0.6914708559867434\n",
            "Epoch 19404  \tTraining Loss: 0.6914783580134546\tValidation Loss: 0.691470792781382\n",
            "Epoch 19405  \tTraining Loss: 0.6914782953190358\tValidation Loss: 0.6914707295751799\n",
            "Epoch 19406  \tTraining Loss: 0.6914782326237757\tValidation Loss: 0.6914706663681366\n",
            "Epoch 19407  \tTraining Loss: 0.6914781699276744\tValidation Loss: 0.6914706031602527\n",
            "Epoch 19408  \tTraining Loss: 0.6914781072307319\tValidation Loss: 0.6914705399515276\n",
            "Epoch 19409  \tTraining Loss: 0.6914780445329479\tValidation Loss: 0.6914704767419617\n",
            "Epoch 19410  \tTraining Loss: 0.6914779818343225\tValidation Loss: 0.6914704135315547\n",
            "Epoch 19411  \tTraining Loss: 0.6914779191348559\tValidation Loss: 0.6914703503203067\n",
            "Epoch 19412  \tTraining Loss: 0.6914778564345478\tValidation Loss: 0.6914702871082176\n",
            "Epoch 19413  \tTraining Loss: 0.6914777937333981\tValidation Loss: 0.6914702238952875\n",
            "Epoch 19414  \tTraining Loss: 0.6914777310314072\tValidation Loss: 0.6914701606815161\n",
            "Epoch 19415  \tTraining Loss: 0.6914776683285746\tValidation Loss: 0.6914700974669035\n",
            "Epoch 19416  \tTraining Loss: 0.6914776056249003\tValidation Loss: 0.6914700342514498\n",
            "Epoch 19417  \tTraining Loss: 0.6914775429203847\tValidation Loss: 0.6914699710351546\n",
            "Epoch 19418  \tTraining Loss: 0.6914774802150273\tValidation Loss: 0.6914699078180184\n",
            "Epoch 19419  \tTraining Loss: 0.6914774175088282\tValidation Loss: 0.6914698446000408\n",
            "Epoch 19420  \tTraining Loss: 0.6914773548017874\tValidation Loss: 0.6914697813812218\n",
            "Epoch 19421  \tTraining Loss: 0.691477292093905\tValidation Loss: 0.6914697181615614\n",
            "Epoch 19422  \tTraining Loss: 0.6914772293851806\tValidation Loss: 0.6914696549410597\n",
            "Epoch 19423  \tTraining Loss: 0.6914771666756148\tValidation Loss: 0.6914695917197163\n",
            "Epoch 19424  \tTraining Loss: 0.6914771039652068\tValidation Loss: 0.6914695284975316\n",
            "Epoch 19425  \tTraining Loss: 0.6914770412539569\tValidation Loss: 0.6914694652745053\n",
            "Epoch 19426  \tTraining Loss: 0.6914769785418654\tValidation Loss: 0.6914694020506376\n",
            "Epoch 19427  \tTraining Loss: 0.6914769158289318\tValidation Loss: 0.6914693388259281\n",
            "Epoch 19428  \tTraining Loss: 0.6914768531151562\tValidation Loss: 0.691469275600377\n",
            "Epoch 19429  \tTraining Loss: 0.6914767904005386\tValidation Loss: 0.6914692123739843\n",
            "Epoch 19430  \tTraining Loss: 0.691476727685079\tValidation Loss: 0.6914691491467498\n",
            "Epoch 19431  \tTraining Loss: 0.6914766649687772\tValidation Loss: 0.6914690859186737\n",
            "Epoch 19432  \tTraining Loss: 0.6914766022516334\tValidation Loss: 0.6914690226897557\n",
            "Epoch 19433  \tTraining Loss: 0.6914765395336474\tValidation Loss: 0.691468959459996\n",
            "Epoch 19434  \tTraining Loss: 0.6914764768148193\tValidation Loss: 0.6914688962293946\n",
            "Epoch 19435  \tTraining Loss: 0.6914764140951489\tValidation Loss: 0.691468832997951\n",
            "Epoch 19436  \tTraining Loss: 0.6914763513746364\tValidation Loss: 0.6914687697656657\n",
            "Epoch 19437  \tTraining Loss: 0.6914762886532814\tValidation Loss: 0.6914687065325384\n",
            "Epoch 19438  \tTraining Loss: 0.6914762259310842\tValidation Loss: 0.6914686432985692\n",
            "Epoch 19439  \tTraining Loss: 0.6914761632080446\tValidation Loss: 0.6914685800637579\n",
            "Epoch 19440  \tTraining Loss: 0.6914761004841627\tValidation Loss: 0.6914685168281047\n",
            "Epoch 19441  \tTraining Loss: 0.6914760377594383\tValidation Loss: 0.6914684535916092\n",
            "Epoch 19442  \tTraining Loss: 0.6914759750338715\tValidation Loss: 0.6914683903542719\n",
            "Epoch 19443  \tTraining Loss: 0.6914759123074621\tValidation Loss: 0.6914683271160922\n",
            "Epoch 19444  \tTraining Loss: 0.6914758495802102\tValidation Loss: 0.6914682638770703\n",
            "Epoch 19445  \tTraining Loss: 0.6914757868521159\tValidation Loss: 0.6914682006372063\n",
            "Epoch 19446  \tTraining Loss: 0.6914757241231788\tValidation Loss: 0.6914681373965001\n",
            "Epoch 19447  \tTraining Loss: 0.6914756613933992\tValidation Loss: 0.6914680741549515\n",
            "Epoch 19448  \tTraining Loss: 0.6914755986627769\tValidation Loss: 0.6914680109125607\n",
            "Epoch 19449  \tTraining Loss: 0.691475535931312\tValidation Loss: 0.6914679476693275\n",
            "Epoch 19450  \tTraining Loss: 0.6914754731990043\tValidation Loss: 0.6914678844252519\n",
            "Epoch 19451  \tTraining Loss: 0.6914754104658537\tValidation Loss: 0.6914678211803339\n",
            "Epoch 19452  \tTraining Loss: 0.6914753477318605\tValidation Loss: 0.6914677579345734\n",
            "Epoch 19453  \tTraining Loss: 0.6914752849970244\tValidation Loss: 0.6914676946879704\n",
            "Epoch 19454  \tTraining Loss: 0.6914752222613453\tValidation Loss: 0.691467631440525\n",
            "Epoch 19455  \tTraining Loss: 0.6914751595248234\tValidation Loss: 0.6914675681922369\n",
            "Epoch 19456  \tTraining Loss: 0.6914750967874586\tValidation Loss: 0.6914675049431064\n",
            "Epoch 19457  \tTraining Loss: 0.6914750340492507\tValidation Loss: 0.691467441693133\n",
            "Epoch 19458  \tTraining Loss: 0.6914749713102\tValidation Loss: 0.6914673784423171\n",
            "Epoch 19459  \tTraining Loss: 0.6914749085703061\tValidation Loss: 0.6914673151906586\n",
            "Epoch 19460  \tTraining Loss: 0.6914748458295692\tValidation Loss: 0.6914672519381572\n",
            "Epoch 19461  \tTraining Loss: 0.6914747830879892\tValidation Loss: 0.691467188684813\n",
            "Epoch 19462  \tTraining Loss: 0.6914747203455659\tValidation Loss: 0.6914671254306263\n",
            "Epoch 19463  \tTraining Loss: 0.6914746576022996\tValidation Loss: 0.6914670621755966\n",
            "Epoch 19464  \tTraining Loss: 0.6914745948581901\tValidation Loss: 0.691466998919724\n",
            "Epoch 19465  \tTraining Loss: 0.6914745321132371\tValidation Loss: 0.6914669356630084\n",
            "Epoch 19466  \tTraining Loss: 0.691474469367441\tValidation Loss: 0.6914668724054501\n",
            "Epoch 19467  \tTraining Loss: 0.6914744066208016\tValidation Loss: 0.6914668091470487\n",
            "Epoch 19468  \tTraining Loss: 0.6914743438733189\tValidation Loss: 0.6914667458878043\n",
            "Epoch 19469  \tTraining Loss: 0.6914742811249928\tValidation Loss: 0.6914666826277169\n",
            "Epoch 19470  \tTraining Loss: 0.6914742183758231\tValidation Loss: 0.6914666193667864\n",
            "Epoch 19471  \tTraining Loss: 0.69147415562581\tValidation Loss: 0.6914665561050128\n",
            "Epoch 19472  \tTraining Loss: 0.6914740928749535\tValidation Loss: 0.691466492842396\n",
            "Epoch 19473  \tTraining Loss: 0.6914740301232536\tValidation Loss: 0.6914664295789362\n",
            "Epoch 19474  \tTraining Loss: 0.69147396737071\tValidation Loss: 0.6914663663146331\n",
            "Epoch 19475  \tTraining Loss: 0.6914739046173226\tValidation Loss: 0.6914663030494868\n",
            "Epoch 19476  \tTraining Loss: 0.6914738418630919\tValidation Loss: 0.6914662397834972\n",
            "Epoch 19477  \tTraining Loss: 0.6914737791080173\tValidation Loss: 0.6914661765166643\n",
            "Epoch 19478  \tTraining Loss: 0.6914737163520993\tValidation Loss: 0.691466113248988\n",
            "Epoch 19479  \tTraining Loss: 0.6914736535953374\tValidation Loss: 0.6914660499804683\n",
            "Epoch 19480  \tTraining Loss: 0.6914735908377316\tValidation Loss: 0.6914659867111052\n",
            "Epoch 19481  \tTraining Loss: 0.6914735280792822\tValidation Loss: 0.6914659234408987\n",
            "Epoch 19482  \tTraining Loss: 0.6914734653199889\tValidation Loss: 0.6914658601698488\n",
            "Epoch 19483  \tTraining Loss: 0.6914734025598517\tValidation Loss: 0.6914657968979552\n",
            "Epoch 19484  \tTraining Loss: 0.6914733397988707\tValidation Loss: 0.6914657336252181\n",
            "Epoch 19485  \tTraining Loss: 0.6914732770370456\tValidation Loss: 0.6914656703516374\n",
            "Epoch 19486  \tTraining Loss: 0.6914732142743767\tValidation Loss: 0.6914656070772133\n",
            "Epoch 19487  \tTraining Loss: 0.6914731515108637\tValidation Loss: 0.6914655438019452\n",
            "Epoch 19488  \tTraining Loss: 0.6914730887465067\tValidation Loss: 0.6914654805258337\n",
            "Epoch 19489  \tTraining Loss: 0.6914730259813056\tValidation Loss: 0.6914654172488783\n",
            "Epoch 19490  \tTraining Loss: 0.6914729632152605\tValidation Loss: 0.6914653539710793\n",
            "Epoch 19491  \tTraining Loss: 0.6914729004483711\tValidation Loss: 0.6914652906924363\n",
            "Epoch 19492  \tTraining Loss: 0.6914728376806376\tValidation Loss: 0.6914652274129497\n",
            "Epoch 19493  \tTraining Loss: 0.6914727749120598\tValidation Loss: 0.6914651641326192\n",
            "Epoch 19494  \tTraining Loss: 0.6914727121426377\tValidation Loss: 0.6914651008514447\n",
            "Epoch 19495  \tTraining Loss: 0.6914726493723716\tValidation Loss: 0.6914650375694262\n",
            "Epoch 19496  \tTraining Loss: 0.6914725866012611\tValidation Loss: 0.691464974286564\n",
            "Epoch 19497  \tTraining Loss: 0.6914725238293061\tValidation Loss: 0.6914649110028576\n",
            "Epoch 19498  \tTraining Loss: 0.6914724610565068\tValidation Loss: 0.6914648477183072\n",
            "Epoch 19499  \tTraining Loss: 0.6914723982828631\tValidation Loss: 0.6914647844329128\n",
            "Epoch 19500  \tTraining Loss: 0.691472335508375\tValidation Loss: 0.6914647211466742\n",
            "Epoch 19501  \tTraining Loss: 0.6914722727330422\tValidation Loss: 0.6914646578595915\n",
            "Epoch 19502  \tTraining Loss: 0.691472209956865\tValidation Loss: 0.6914645945716648\n",
            "Epoch 19503  \tTraining Loss: 0.6914721471798434\tValidation Loss: 0.6914645312828938\n",
            "Epoch 19504  \tTraining Loss: 0.691472084401977\tValidation Loss: 0.6914644679932784\n",
            "Epoch 19505  \tTraining Loss: 0.6914720216232662\tValidation Loss: 0.691464404702819\n",
            "Epoch 19506  \tTraining Loss: 0.6914719588437105\tValidation Loss: 0.691464341411515\n",
            "Epoch 19507  \tTraining Loss: 0.6914718960633102\tValidation Loss: 0.6914642781193668\n",
            "Epoch 19508  \tTraining Loss: 0.6914718332820652\tValidation Loss: 0.6914642148263742\n",
            "Epoch 19509  \tTraining Loss: 0.6914717704999753\tValidation Loss: 0.6914641515325374\n",
            "Epoch 19510  \tTraining Loss: 0.6914717077170408\tValidation Loss: 0.6914640882378559\n",
            "Epoch 19511  \tTraining Loss: 0.6914716449332614\tValidation Loss: 0.6914640249423301\n",
            "Epoch 19512  \tTraining Loss: 0.6914715821486371\tValidation Loss: 0.6914639616459596\n",
            "Epoch 19513  \tTraining Loss: 0.6914715193631679\tValidation Loss: 0.6914638983487446\n",
            "Epoch 19514  \tTraining Loss: 0.6914714565768538\tValidation Loss: 0.6914638350506851\n",
            "Epoch 19515  \tTraining Loss: 0.6914713937896946\tValidation Loss: 0.6914637717517809\n",
            "Epoch 19516  \tTraining Loss: 0.6914713310016906\tValidation Loss: 0.6914637084520322\n",
            "Epoch 19517  \tTraining Loss: 0.6914712682128414\tValidation Loss: 0.6914636451514387\n",
            "Epoch 19518  \tTraining Loss: 0.6914712054231473\tValidation Loss: 0.6914635818500005\n",
            "Epoch 19519  \tTraining Loss: 0.691471142632608\tValidation Loss: 0.6914635185477175\n",
            "Epoch 19520  \tTraining Loss: 0.6914710798412236\tValidation Loss: 0.6914634552445899\n",
            "Epoch 19521  \tTraining Loss: 0.6914710170489938\tValidation Loss: 0.6914633919406172\n",
            "Epoch 19522  \tTraining Loss: 0.6914709542559191\tValidation Loss: 0.6914633286357998\n",
            "Epoch 19523  \tTraining Loss: 0.691470891461999\tValidation Loss: 0.6914632653301375\n",
            "Epoch 19524  \tTraining Loss: 0.6914708286672336\tValidation Loss: 0.6914632020236303\n",
            "Epoch 19525  \tTraining Loss: 0.6914707658716229\tValidation Loss: 0.6914631387162782\n",
            "Epoch 19526  \tTraining Loss: 0.691470703075167\tValidation Loss: 0.691463075408081\n",
            "Epoch 19527  \tTraining Loss: 0.6914706402778655\tValidation Loss: 0.6914630120990388\n",
            "Epoch 19528  \tTraining Loss: 0.6914705774797189\tValidation Loss: 0.6914629487891516\n",
            "Epoch 19529  \tTraining Loss: 0.6914705146807264\tValidation Loss: 0.6914628854784192\n",
            "Epoch 19530  \tTraining Loss: 0.6914704518808886\tValidation Loss: 0.6914628221668417\n",
            "Epoch 19531  \tTraining Loss: 0.6914703890802055\tValidation Loss: 0.6914627588544191\n",
            "Epoch 19532  \tTraining Loss: 0.6914703262786767\tValidation Loss: 0.6914626955411511\n",
            "Epoch 19533  \tTraining Loss: 0.6914702634763022\tValidation Loss: 0.6914626322270381\n",
            "Epoch 19534  \tTraining Loss: 0.6914702006730821\tValidation Loss: 0.6914625689120798\n",
            "Epoch 19535  \tTraining Loss: 0.6914701378690163\tValidation Loss: 0.6914625055962762\n",
            "Epoch 19536  \tTraining Loss: 0.6914700750641051\tValidation Loss: 0.691462442279627\n",
            "Epoch 19537  \tTraining Loss: 0.6914700122583479\tValidation Loss: 0.6914623789621328\n",
            "Epoch 19538  \tTraining Loss: 0.6914699494517451\tValidation Loss: 0.691462315643793\n",
            "Epoch 19539  \tTraining Loss: 0.6914698866442964\tValidation Loss: 0.6914622523246079\n",
            "Epoch 19540  \tTraining Loss: 0.6914698238360019\tValidation Loss: 0.6914621890045772\n",
            "Epoch 19541  \tTraining Loss: 0.6914697610268615\tValidation Loss: 0.691462125683701\n",
            "Epoch 19542  \tTraining Loss: 0.6914696982168752\tValidation Loss: 0.6914620623619794\n",
            "Epoch 19543  \tTraining Loss: 0.6914696354060431\tValidation Loss: 0.6914619990394121\n",
            "Epoch 19544  \tTraining Loss: 0.6914695725943649\tValidation Loss: 0.6914619357159991\n",
            "Epoch 19545  \tTraining Loss: 0.6914695097818407\tValidation Loss: 0.6914618723917407\n",
            "Epoch 19546  \tTraining Loss: 0.6914694469684705\tValidation Loss: 0.6914618090666365\n",
            "Epoch 19547  \tTraining Loss: 0.6914693841542542\tValidation Loss: 0.6914617457406864\n",
            "Epoch 19548  \tTraining Loss: 0.6914693213391918\tValidation Loss: 0.6914616824138907\n",
            "Epoch 19549  \tTraining Loss: 0.6914692585232833\tValidation Loss: 0.6914616190862494\n",
            "Epoch 19550  \tTraining Loss: 0.6914691957065285\tValidation Loss: 0.691461555757762\n",
            "Epoch 19551  \tTraining Loss: 0.6914691328889276\tValidation Loss: 0.6914614924284289\n",
            "Epoch 19552  \tTraining Loss: 0.6914690700704805\tValidation Loss: 0.6914614290982499\n",
            "Epoch 19553  \tTraining Loss: 0.6914690072511871\tValidation Loss: 0.6914613657672251\n",
            "Epoch 19554  \tTraining Loss: 0.6914689444310473\tValidation Loss: 0.691461302435354\n",
            "Epoch 19555  \tTraining Loss: 0.6914688816100613\tValidation Loss: 0.6914612391026371\n",
            "Epoch 19556  \tTraining Loss: 0.6914688187882289\tValidation Loss: 0.6914611757690742\n",
            "Epoch 19557  \tTraining Loss: 0.6914687559655499\tValidation Loss: 0.6914611124346653\n",
            "Epoch 19558  \tTraining Loss: 0.6914686931420245\tValidation Loss: 0.6914610490994102\n",
            "Epoch 19559  \tTraining Loss: 0.6914686303176527\tValidation Loss: 0.691460985763309\n",
            "Epoch 19560  \tTraining Loss: 0.6914685674924343\tValidation Loss: 0.6914609224263618\n",
            "Epoch 19561  \tTraining Loss: 0.6914685046663694\tValidation Loss: 0.6914608590885681\n",
            "Epoch 19562  \tTraining Loss: 0.6914684418394581\tValidation Loss: 0.6914607957499284\n",
            "Epoch 19563  \tTraining Loss: 0.6914683790116999\tValidation Loss: 0.6914607324104424\n",
            "Epoch 19564  \tTraining Loss: 0.6914683161830951\tValidation Loss: 0.6914606690701101\n",
            "Epoch 19565  \tTraining Loss: 0.6914682533536436\tValidation Loss: 0.6914606057289315\n",
            "Epoch 19566  \tTraining Loss: 0.6914681905233455\tValidation Loss: 0.6914605423869065\n",
            "Epoch 19567  \tTraining Loss: 0.6914681276922006\tValidation Loss: 0.691460479044035\n",
            "Epoch 19568  \tTraining Loss: 0.6914680648602088\tValidation Loss: 0.6914604157003172\n",
            "Epoch 19569  \tTraining Loss: 0.6914680020273704\tValidation Loss: 0.6914603523557529\n",
            "Epoch 19570  \tTraining Loss: 0.6914679391936849\tValidation Loss: 0.6914602890103421\n",
            "Epoch 19571  \tTraining Loss: 0.6914678763591525\tValidation Loss: 0.6914602256640847\n",
            "Epoch 19572  \tTraining Loss: 0.6914678135237732\tValidation Loss: 0.6914601623169807\n",
            "Epoch 19573  \tTraining Loss: 0.691467750687547\tValidation Loss: 0.6914600989690302\n",
            "Epoch 19574  \tTraining Loss: 0.6914676878504739\tValidation Loss: 0.6914600356202331\n",
            "Epoch 19575  \tTraining Loss: 0.6914676250125535\tValidation Loss: 0.6914599722705891\n",
            "Epoch 19576  \tTraining Loss: 0.6914675621737861\tValidation Loss: 0.6914599089200986\n",
            "Epoch 19577  \tTraining Loss: 0.6914674993341717\tValidation Loss: 0.6914598455687613\n",
            "Epoch 19578  \tTraining Loss: 0.69146743649371\tValidation Loss: 0.6914597822165772\n",
            "Epoch 19579  \tTraining Loss: 0.6914673736524013\tValidation Loss: 0.6914597188635463\n",
            "Epoch 19580  \tTraining Loss: 0.6914673108102454\tValidation Loss: 0.6914596555096686\n",
            "Epoch 19581  \tTraining Loss: 0.6914672479672421\tValidation Loss: 0.691459592154944\n",
            "Epoch 19582  \tTraining Loss: 0.6914671851233917\tValidation Loss: 0.6914595287993724\n",
            "Epoch 19583  \tTraining Loss: 0.6914671222786939\tValidation Loss: 0.6914594654429539\n",
            "Epoch 19584  \tTraining Loss: 0.6914670594331487\tValidation Loss: 0.6914594020856883\n",
            "Epoch 19585  \tTraining Loss: 0.6914669965867563\tValidation Loss: 0.6914593387275759\n",
            "Epoch 19586  \tTraining Loss: 0.6914669337395162\tValidation Loss: 0.6914592753686163\n",
            "Epoch 19587  \tTraining Loss: 0.6914668708914288\tValidation Loss: 0.6914592120088097\n",
            "Epoch 19588  \tTraining Loss: 0.6914668080424939\tValidation Loss: 0.6914591486481559\n",
            "Epoch 19589  \tTraining Loss: 0.6914667451927116\tValidation Loss: 0.6914590852866551\n",
            "Epoch 19590  \tTraining Loss: 0.6914666823420816\tValidation Loss: 0.6914590219243069\n",
            "Epoch 19591  \tTraining Loss: 0.691466619490604\tValidation Loss: 0.6914589585611116\n",
            "Epoch 19592  \tTraining Loss: 0.6914665566382789\tValidation Loss: 0.691458895197069\n",
            "Epoch 19593  \tTraining Loss: 0.691466493785106\tValidation Loss: 0.691458831832179\n",
            "Epoch 19594  \tTraining Loss: 0.6914664309310855\tValidation Loss: 0.6914587684664418\n",
            "Epoch 19595  \tTraining Loss: 0.6914663680762173\tValidation Loss: 0.6914587050998573\n",
            "Epoch 19596  \tTraining Loss: 0.6914663052205012\tValidation Loss: 0.6914586417324253\n",
            "Epoch 19597  \tTraining Loss: 0.6914662423639375\tValidation Loss: 0.691458578364146\n",
            "Epoch 19598  \tTraining Loss: 0.6914661795065258\tValidation Loss: 0.6914585149950191\n",
            "Epoch 19599  \tTraining Loss: 0.6914661166482663\tValidation Loss: 0.6914584516250447\n",
            "Epoch 19600  \tTraining Loss: 0.691466053789159\tValidation Loss: 0.6914583882542227\n",
            "Epoch 19601  \tTraining Loss: 0.6914659909292036\tValidation Loss: 0.6914583248825533\n",
            "Epoch 19602  \tTraining Loss: 0.6914659280684003\tValidation Loss: 0.6914582615100362\n",
            "Epoch 19603  \tTraining Loss: 0.691465865206749\tValidation Loss: 0.6914581981366715\n",
            "Epoch 19604  \tTraining Loss: 0.6914658023442497\tValidation Loss: 0.6914581347624591\n",
            "Epoch 19605  \tTraining Loss: 0.6914657394809024\tValidation Loss: 0.6914580713873989\n",
            "Epoch 19606  \tTraining Loss: 0.6914656766167068\tValidation Loss: 0.6914580080114912\n",
            "Epoch 19607  \tTraining Loss: 0.6914656137516632\tValidation Loss: 0.6914579446347355\n",
            "Epoch 19608  \tTraining Loss: 0.6914655508857714\tValidation Loss: 0.6914578812571321\n",
            "Epoch 19609  \tTraining Loss: 0.6914654880190313\tValidation Loss: 0.6914578178786808\n",
            "Epoch 19610  \tTraining Loss: 0.691465425151443\tValidation Loss: 0.6914577544993817\n",
            "Epoch 19611  \tTraining Loss: 0.6914653622830065\tValidation Loss: 0.6914576911192346\n",
            "Epoch 19612  \tTraining Loss: 0.6914652994137216\tValidation Loss: 0.6914576277382396\n",
            "Epoch 19613  \tTraining Loss: 0.6914652365435884\tValidation Loss: 0.6914575643563965\n",
            "Epoch 19614  \tTraining Loss: 0.6914651736726067\tValidation Loss: 0.6914575009737055\n",
            "Epoch 19615  \tTraining Loss: 0.6914651108007768\tValidation Loss: 0.6914574375901665\n",
            "Epoch 19616  \tTraining Loss: 0.6914650479280983\tValidation Loss: 0.6914573742057794\n",
            "Epoch 19617  \tTraining Loss: 0.6914649850545713\tValidation Loss: 0.6914573108205442\n",
            "Epoch 19618  \tTraining Loss: 0.691464922180196\tValidation Loss: 0.6914572474344607\n",
            "Epoch 19619  \tTraining Loss: 0.6914648593049719\tValidation Loss: 0.6914571840475291\n",
            "Epoch 19620  \tTraining Loss: 0.6914647964288992\tValidation Loss: 0.6914571206597493\n",
            "Epoch 19621  \tTraining Loss: 0.6914647335519779\tValidation Loss: 0.6914570572711211\n",
            "Epoch 19622  \tTraining Loss: 0.691464670674208\tValidation Loss: 0.6914569938816447\n",
            "Epoch 19623  \tTraining Loss: 0.6914646077955896\tValidation Loss: 0.6914569304913202\n",
            "Epoch 19624  \tTraining Loss: 0.6914645449161222\tValidation Loss: 0.691456867100147\n",
            "Epoch 19625  \tTraining Loss: 0.6914644820358062\tValidation Loss: 0.6914568037081255\n",
            "Epoch 19626  \tTraining Loss: 0.6914644191546412\tValidation Loss: 0.6914567403152556\n",
            "Epoch 19627  \tTraining Loss: 0.6914643562726275\tValidation Loss: 0.6914566769215372\n",
            "Epoch 19628  \tTraining Loss: 0.691464293389765\tValidation Loss: 0.6914566135269704\n",
            "Epoch 19629  \tTraining Loss: 0.6914642305060537\tValidation Loss: 0.691456550131555\n",
            "Epoch 19630  \tTraining Loss: 0.6914641676214932\tValidation Loss: 0.6914564867352911\n",
            "Epoch 19631  \tTraining Loss: 0.6914641047360837\tValidation Loss: 0.6914564233381785\n",
            "Epoch 19632  \tTraining Loss: 0.6914640418498254\tValidation Loss: 0.6914563599402174\n",
            "Epoch 19633  \tTraining Loss: 0.6914639789627179\tValidation Loss: 0.6914562965414075\n",
            "Epoch 19634  \tTraining Loss: 0.6914639160747614\tValidation Loss: 0.691456233141749\n",
            "Epoch 19635  \tTraining Loss: 0.6914638531859559\tValidation Loss: 0.6914561697412416\n",
            "Epoch 19636  \tTraining Loss: 0.6914637902963012\tValidation Loss: 0.6914561063398856\n",
            "Epoch 19637  \tTraining Loss: 0.6914637274057973\tValidation Loss: 0.6914560429376807\n",
            "Epoch 19638  \tTraining Loss: 0.6914636645144442\tValidation Loss: 0.691455979534627\n",
            "Epoch 19639  \tTraining Loss: 0.6914636016222417\tValidation Loss: 0.6914559161307243\n",
            "Epoch 19640  \tTraining Loss: 0.6914635387291901\tValidation Loss: 0.6914558527259728\n",
            "Epoch 19641  \tTraining Loss: 0.6914634758352892\tValidation Loss: 0.6914557893203724\n",
            "Epoch 19642  \tTraining Loss: 0.6914634129405389\tValidation Loss: 0.691455725913923\n",
            "Epoch 19643  \tTraining Loss: 0.6914633500449392\tValidation Loss: 0.6914556625066245\n",
            "Epoch 19644  \tTraining Loss: 0.6914632871484901\tValidation Loss: 0.691455599098477\n",
            "Epoch 19645  \tTraining Loss: 0.6914632242511916\tValidation Loss: 0.6914555356894805\n",
            "Epoch 19646  \tTraining Loss: 0.6914631613530435\tValidation Loss: 0.6914554722796348\n",
            "Epoch 19647  \tTraining Loss: 0.691463098454046\tValidation Loss: 0.6914554088689399\n",
            "Epoch 19648  \tTraining Loss: 0.691463035554199\tValidation Loss: 0.6914553454573957\n",
            "Epoch 19649  \tTraining Loss: 0.6914629726535022\tValidation Loss: 0.6914552820450025\n",
            "Epoch 19650  \tTraining Loss: 0.6914629097519559\tValidation Loss: 0.6914552186317601\n",
            "Epoch 19651  \tTraining Loss: 0.6914628468495598\tValidation Loss: 0.6914551552176683\n",
            "Epoch 19652  \tTraining Loss: 0.6914627839463142\tValidation Loss: 0.6914550918027272\n",
            "Epoch 19653  \tTraining Loss: 0.6914627210422187\tValidation Loss: 0.6914550283869367\n",
            "Epoch 19654  \tTraining Loss: 0.6914626581372736\tValidation Loss: 0.6914549649702968\n",
            "Epoch 19655  \tTraining Loss: 0.6914625952314786\tValidation Loss: 0.6914549015528075\n",
            "Epoch 19656  \tTraining Loss: 0.6914625323248338\tValidation Loss: 0.6914548381344687\n",
            "Epoch 19657  \tTraining Loss: 0.6914624694173391\tValidation Loss: 0.6914547747152805\n",
            "Epoch 19658  \tTraining Loss: 0.6914624065089946\tValidation Loss: 0.6914547112952427\n",
            "Epoch 19659  \tTraining Loss: 0.6914623435998001\tValidation Loss: 0.6914546478743553\n",
            "Epoch 19660  \tTraining Loss: 0.6914622806897556\tValidation Loss: 0.6914545844526184\n",
            "Epoch 19661  \tTraining Loss: 0.6914622177788612\tValidation Loss: 0.6914545210300318\n",
            "Epoch 19662  \tTraining Loss: 0.6914621548671167\tValidation Loss: 0.6914544576065955\n",
            "Epoch 19663  \tTraining Loss: 0.6914620919545219\tValidation Loss: 0.6914543941823095\n",
            "Epoch 19664  \tTraining Loss: 0.6914620290410773\tValidation Loss: 0.6914543307571739\n",
            "Epoch 19665  \tTraining Loss: 0.6914619661267825\tValidation Loss: 0.6914542673311884\n",
            "Epoch 19666  \tTraining Loss: 0.6914619032116375\tValidation Loss: 0.6914542039043532\n",
            "Epoch 19667  \tTraining Loss: 0.6914618402956423\tValidation Loss: 0.691454140476668\n",
            "Epoch 19668  \tTraining Loss: 0.6914617773787968\tValidation Loss: 0.691454077048133\n",
            "Epoch 19669  \tTraining Loss: 0.691461714461101\tValidation Loss: 0.6914540136187481\n",
            "Epoch 19670  \tTraining Loss: 0.691461651542555\tValidation Loss: 0.6914539501885133\n",
            "Epoch 19671  \tTraining Loss: 0.6914615886231585\tValidation Loss: 0.6914538867574284\n",
            "Epoch 19672  \tTraining Loss: 0.6914615257029118\tValidation Loss: 0.6914538233254937\n",
            "Epoch 19673  \tTraining Loss: 0.6914614627818145\tValidation Loss: 0.6914537598927087\n",
            "Epoch 19674  \tTraining Loss: 0.6914613998598669\tValidation Loss: 0.6914536964590738\n",
            "Epoch 19675  \tTraining Loss: 0.6914613369370688\tValidation Loss: 0.6914536330245887\n",
            "Epoch 19676  \tTraining Loss: 0.6914612740134201\tValidation Loss: 0.6914535695892535\n",
            "Epoch 19677  \tTraining Loss: 0.6914612110889208\tValidation Loss: 0.691453506153068\n",
            "Epoch 19678  \tTraining Loss: 0.691461148163571\tValidation Loss: 0.6914534427160324\n",
            "Epoch 19679  \tTraining Loss: 0.6914610852373705\tValidation Loss: 0.6914533792781465\n",
            "Epoch 19680  \tTraining Loss: 0.6914610223103194\tValidation Loss: 0.6914533158394104\n",
            "Epoch 19681  \tTraining Loss: 0.6914609593824175\tValidation Loss: 0.6914532523998238\n",
            "Epoch 19682  \tTraining Loss: 0.6914608964536649\tValidation Loss: 0.691453188959387\n",
            "Epoch 19683  \tTraining Loss: 0.6914608335240616\tValidation Loss: 0.6914531255180998\n",
            "Epoch 19684  \tTraining Loss: 0.6914607705936074\tValidation Loss: 0.691453062075962\n",
            "Epoch 19685  \tTraining Loss: 0.6914607076623025\tValidation Loss: 0.6914529986329739\n",
            "Epoch 19686  \tTraining Loss: 0.6914606447301468\tValidation Loss: 0.6914529351891353\n",
            "Epoch 19687  \tTraining Loss: 0.6914605817971401\tValidation Loss: 0.6914528717444463\n",
            "Epoch 19688  \tTraining Loss: 0.6914605188632824\tValidation Loss: 0.6914528082989065\n",
            "Epoch 19689  \tTraining Loss: 0.6914604559285737\tValidation Loss: 0.6914527448525163\n",
            "Epoch 19690  \tTraining Loss: 0.6914603929930141\tValidation Loss: 0.6914526814052752\n",
            "Epoch 19691  \tTraining Loss: 0.6914603300566035\tValidation Loss: 0.6914526179571836\n",
            "Epoch 19692  \tTraining Loss: 0.6914602671193417\tValidation Loss: 0.6914525545082413\n",
            "Epoch 19693  \tTraining Loss: 0.691460204181229\tValidation Loss: 0.6914524910584482\n",
            "Epoch 19694  \tTraining Loss: 0.691460141242265\tValidation Loss: 0.6914524276078045\n",
            "Epoch 19695  \tTraining Loss: 0.6914600783024497\tValidation Loss: 0.6914523641563098\n",
            "Epoch 19696  \tTraining Loss: 0.6914600153617834\tValidation Loss: 0.6914523007039644\n",
            "Epoch 19697  \tTraining Loss: 0.6914599524202658\tValidation Loss: 0.691452237250768\n",
            "Epoch 19698  \tTraining Loss: 0.6914598894778968\tValidation Loss: 0.6914521737967207\n",
            "Epoch 19699  \tTraining Loss: 0.6914598265346765\tValidation Loss: 0.6914521103418225\n",
            "Epoch 19700  \tTraining Loss: 0.691459763590605\tValidation Loss: 0.6914520468860734\n",
            "Epoch 19701  \tTraining Loss: 0.6914597006456821\tValidation Loss: 0.6914519834294732\n",
            "Epoch 19702  \tTraining Loss: 0.6914596376999076\tValidation Loss: 0.691451919972022\n",
            "Epoch 19703  \tTraining Loss: 0.6914595747532817\tValidation Loss: 0.6914518565137198\n",
            "Epoch 19704  \tTraining Loss: 0.6914595118058043\tValidation Loss: 0.6914517930545663\n",
            "Epoch 19705  \tTraining Loss: 0.6914594488574756\tValidation Loss: 0.6914517295945618\n",
            "Epoch 19706  \tTraining Loss: 0.6914593859082951\tValidation Loss: 0.691451666133706\n",
            "Epoch 19707  \tTraining Loss: 0.6914593229582631\tValidation Loss: 0.691451602671999\n",
            "Epoch 19708  \tTraining Loss: 0.6914592600073795\tValidation Loss: 0.6914515392094407\n",
            "Epoch 19709  \tTraining Loss: 0.6914591970556442\tValidation Loss: 0.6914514757460313\n",
            "Epoch 19710  \tTraining Loss: 0.6914591341030571\tValidation Loss: 0.6914514122817704\n",
            "Epoch 19711  \tTraining Loss: 0.6914590711496184\tValidation Loss: 0.6914513488166583\n",
            "Epoch 19712  \tTraining Loss: 0.6914590081953279\tValidation Loss: 0.6914512853506947\n",
            "Epoch 19713  \tTraining Loss: 0.6914589452401856\tValidation Loss: 0.6914512218838796\n",
            "Epoch 19714  \tTraining Loss: 0.6914588822841915\tValidation Loss: 0.6914511584162132\n",
            "Epoch 19715  \tTraining Loss: 0.6914588193273454\tValidation Loss: 0.6914510949476953\n",
            "Epoch 19716  \tTraining Loss: 0.6914587563696475\tValidation Loss: 0.6914510314783259\n",
            "Epoch 19717  \tTraining Loss: 0.6914586934110977\tValidation Loss: 0.6914509680081047\n",
            "Epoch 19718  \tTraining Loss: 0.6914586304516958\tValidation Loss: 0.6914509045370321\n",
            "Epoch 19719  \tTraining Loss: 0.691458567491442\tValidation Loss: 0.6914508410651078\n",
            "Epoch 19720  \tTraining Loss: 0.6914585045303361\tValidation Loss: 0.6914507775923319\n",
            "Epoch 19721  \tTraining Loss: 0.6914584415683781\tValidation Loss: 0.6914507141187043\n",
            "Epoch 19722  \tTraining Loss: 0.691458378605568\tValidation Loss: 0.6914506506442248\n",
            "Epoch 19723  \tTraining Loss: 0.6914583156419059\tValidation Loss: 0.6914505871688937\n",
            "Epoch 19724  \tTraining Loss: 0.6914582526773915\tValidation Loss: 0.6914505236927108\n",
            "Epoch 19725  \tTraining Loss: 0.6914581897120249\tValidation Loss: 0.6914504602156759\n",
            "Epoch 19726  \tTraining Loss: 0.691458126745806\tValidation Loss: 0.6914503967377894\n",
            "Epoch 19727  \tTraining Loss: 0.6914580637787348\tValidation Loss: 0.6914503332590508\n",
            "Epoch 19728  \tTraining Loss: 0.6914580008108112\tValidation Loss: 0.6914502697794602\n",
            "Epoch 19729  \tTraining Loss: 0.6914579378420355\tValidation Loss: 0.6914502062990178\n",
            "Epoch 19730  \tTraining Loss: 0.6914578748724073\tValidation Loss: 0.6914501428177232\n",
            "Epoch 19731  \tTraining Loss: 0.6914578119019267\tValidation Loss: 0.6914500793355767\n",
            "Epoch 19732  \tTraining Loss: 0.6914577489305935\tValidation Loss: 0.691450015852578\n",
            "Epoch 19733  \tTraining Loss: 0.691457685958408\tValidation Loss: 0.6914499523687274\n",
            "Epoch 19734  \tTraining Loss: 0.69145762298537\tValidation Loss: 0.6914498888840245\n",
            "Epoch 19735  \tTraining Loss: 0.6914575600114792\tValidation Loss: 0.6914498253984694\n",
            "Epoch 19736  \tTraining Loss: 0.691457497036736\tValidation Loss: 0.6914497619120621\n",
            "Epoch 19737  \tTraining Loss: 0.69145743406114\tValidation Loss: 0.6914496984248026\n",
            "Epoch 19738  \tTraining Loss: 0.6914573710846914\tValidation Loss: 0.6914496349366908\n",
            "Epoch 19739  \tTraining Loss: 0.6914573081073901\tValidation Loss: 0.6914495714477266\n",
            "Epoch 19740  \tTraining Loss: 0.6914572451292362\tValidation Loss: 0.6914495079579102\n",
            "Epoch 19741  \tTraining Loss: 0.6914571821502294\tValidation Loss: 0.6914494444672412\n",
            "Epoch 19742  \tTraining Loss: 0.6914571191703698\tValidation Loss: 0.69144938097572\n",
            "Epoch 19743  \tTraining Loss: 0.6914570561896574\tValidation Loss: 0.6914493174833461\n",
            "Epoch 19744  \tTraining Loss: 0.6914569932080921\tValidation Loss: 0.6914492539901198\n",
            "Epoch 19745  \tTraining Loss: 0.6914569302256738\tValidation Loss: 0.691449190496041\n",
            "Epoch 19746  \tTraining Loss: 0.6914568672424027\tValidation Loss: 0.6914491270011097\n",
            "Epoch 19747  \tTraining Loss: 0.6914568042582786\tValidation Loss: 0.6914490635053258\n",
            "Epoch 19748  \tTraining Loss: 0.6914567412733015\tValidation Loss: 0.6914490000086891\n",
            "Epoch 19749  \tTraining Loss: 0.6914566782874714\tValidation Loss: 0.6914489365111999\n",
            "Epoch 19750  \tTraining Loss: 0.6914566153007881\tValidation Loss: 0.691448873012858\n",
            "Epoch 19751  \tTraining Loss: 0.6914565523132518\tValidation Loss: 0.6914488095136634\n",
            "Epoch 19752  \tTraining Loss: 0.6914564893248621\tValidation Loss: 0.6914487460136157\n",
            "Epoch 19753  \tTraining Loss: 0.6914564263356194\tValidation Loss: 0.6914486825127155\n",
            "Epoch 19754  \tTraining Loss: 0.6914563633455235\tValidation Loss: 0.6914486190109624\n",
            "Epoch 19755  \tTraining Loss: 0.6914563003545744\tValidation Loss: 0.6914485555083566\n",
            "Epoch 19756  \tTraining Loss: 0.691456237362772\tValidation Loss: 0.6914484920048977\n",
            "Epoch 19757  \tTraining Loss: 0.6914561743701162\tValidation Loss: 0.6914484285005859\n",
            "Epoch 19758  \tTraining Loss: 0.691456111376607\tValidation Loss: 0.6914483649954211\n",
            "Epoch 19759  \tTraining Loss: 0.6914560483822445\tValidation Loss: 0.6914483014894032\n",
            "Epoch 19760  \tTraining Loss: 0.6914559853870286\tValidation Loss: 0.6914482379825324\n",
            "Epoch 19761  \tTraining Loss: 0.6914559223909591\tValidation Loss: 0.6914481744748083\n",
            "Epoch 19762  \tTraining Loss: 0.6914558593940362\tValidation Loss: 0.6914481109662314\n",
            "Epoch 19763  \tTraining Loss: 0.6914557963962596\tValidation Loss: 0.6914480474568012\n",
            "Epoch 19764  \tTraining Loss: 0.6914557333976297\tValidation Loss: 0.6914479839465177\n",
            "Epoch 19765  \tTraining Loss: 0.691455670398146\tValidation Loss: 0.6914479204353812\n",
            "Epoch 19766  \tTraining Loss: 0.6914556073978088\tValidation Loss: 0.6914478569233914\n",
            "Epoch 19767  \tTraining Loss: 0.6914555443966179\tValidation Loss: 0.6914477934105482\n",
            "Epoch 19768  \tTraining Loss: 0.6914554813945732\tValidation Loss: 0.6914477298968517\n",
            "Epoch 19769  \tTraining Loss: 0.6914554183916749\tValidation Loss: 0.6914476663823019\n",
            "Epoch 19770  \tTraining Loss: 0.6914553553879227\tValidation Loss: 0.6914476028668988\n",
            "Epoch 19771  \tTraining Loss: 0.6914552923833169\tValidation Loss: 0.6914475393506421\n",
            "Epoch 19772  \tTraining Loss: 0.691455229377857\tValidation Loss: 0.6914474758335319\n",
            "Epoch 19773  \tTraining Loss: 0.6914551663715434\tValidation Loss: 0.6914474123155684\n",
            "Epoch 19774  \tTraining Loss: 0.6914551033643758\tValidation Loss: 0.6914473487967513\n",
            "Epoch 19775  \tTraining Loss: 0.6914550403563543\tValidation Loss: 0.6914472852770807\n",
            "Epoch 19776  \tTraining Loss: 0.6914549773474788\tValidation Loss: 0.6914472217565563\n",
            "Epoch 19777  \tTraining Loss: 0.6914549143377493\tValidation Loss: 0.6914471582351784\n",
            "Epoch 19778  \tTraining Loss: 0.6914548513271658\tValidation Loss: 0.691447094712947\n",
            "Epoch 19779  \tTraining Loss: 0.691454788315728\tValidation Loss: 0.6914470311898616\n",
            "Epoch 19780  \tTraining Loss: 0.6914547253034363\tValidation Loss: 0.6914469676659226\n",
            "Epoch 19781  \tTraining Loss: 0.6914546622902904\tValidation Loss: 0.6914469041411299\n",
            "Epoch 19782  \tTraining Loss: 0.6914545992762903\tValidation Loss: 0.6914468406154832\n",
            "Epoch 19783  \tTraining Loss: 0.6914545362614359\tValidation Loss: 0.691446777088983\n",
            "Epoch 19784  \tTraining Loss: 0.6914544732457274\tValidation Loss: 0.6914467135616286\n",
            "Epoch 19785  \tTraining Loss: 0.6914544102291645\tValidation Loss: 0.6914466500334203\n",
            "Epoch 19786  \tTraining Loss: 0.6914543472117474\tValidation Loss: 0.6914465865043582\n",
            "Epoch 19787  \tTraining Loss: 0.6914542841934758\tValidation Loss: 0.6914465229744421\n",
            "Epoch 19788  \tTraining Loss: 0.6914542211743498\tValidation Loss: 0.691446459443672\n",
            "Epoch 19789  \tTraining Loss: 0.6914541581543694\tValidation Loss: 0.6914463959120478\n",
            "Epoch 19790  \tTraining Loss: 0.6914540951335345\tValidation Loss: 0.6914463323795695\n",
            "Epoch 19791  \tTraining Loss: 0.6914540321118452\tValidation Loss: 0.6914462688462372\n",
            "Epoch 19792  \tTraining Loss: 0.6914539690893012\tValidation Loss: 0.6914462053120506\n",
            "Epoch 19793  \tTraining Loss: 0.6914539060659028\tValidation Loss: 0.6914461417770098\n",
            "Epoch 19794  \tTraining Loss: 0.6914538430416498\tValidation Loss: 0.6914460782411149\n",
            "Epoch 19795  \tTraining Loss: 0.6914537800165421\tValidation Loss: 0.6914460147043658\n",
            "Epoch 19796  \tTraining Loss: 0.6914537169905798\tValidation Loss: 0.6914459511667623\n",
            "Epoch 19797  \tTraining Loss: 0.6914536539637626\tValidation Loss: 0.6914458876283046\n",
            "Epoch 19798  \tTraining Loss: 0.6914535909360908\tValidation Loss: 0.6914458240889925\n",
            "Epoch 19799  \tTraining Loss: 0.6914535279075642\tValidation Loss: 0.6914457605488259\n",
            "Epoch 19800  \tTraining Loss: 0.6914534648781828\tValidation Loss: 0.6914456970078049\n",
            "Epoch 19801  \tTraining Loss: 0.6914534018479466\tValidation Loss: 0.6914456334659295\n",
            "Epoch 19802  \tTraining Loss: 0.6914533388168554\tValidation Loss: 0.6914455699231995\n",
            "Epoch 19803  \tTraining Loss: 0.6914532757849093\tValidation Loss: 0.6914455063796151\n",
            "Epoch 19804  \tTraining Loss: 0.6914532127521084\tValidation Loss: 0.6914454428351762\n",
            "Epoch 19805  \tTraining Loss: 0.6914531497184524\tValidation Loss: 0.6914453792898825\n",
            "Epoch 19806  \tTraining Loss: 0.6914530866839413\tValidation Loss: 0.6914453157437344\n",
            "Epoch 19807  \tTraining Loss: 0.6914530236485753\tValidation Loss: 0.6914452521967314\n",
            "Epoch 19808  \tTraining Loss: 0.6914529606123542\tValidation Loss: 0.6914451886488737\n",
            "Epoch 19809  \tTraining Loss: 0.691452897575278\tValidation Loss: 0.6914451251001613\n",
            "Epoch 19810  \tTraining Loss: 0.6914528345373466\tValidation Loss: 0.6914450615505943\n",
            "Epoch 19811  \tTraining Loss: 0.6914527714985599\tValidation Loss: 0.6914449980001723\n",
            "Epoch 19812  \tTraining Loss: 0.691452708458918\tValidation Loss: 0.6914449344488957\n",
            "Epoch 19813  \tTraining Loss: 0.6914526454184209\tValidation Loss: 0.691444870896764\n",
            "Epoch 19814  \tTraining Loss: 0.6914525823770686\tValidation Loss: 0.6914448073437774\n",
            "Epoch 19815  \tTraining Loss: 0.6914525193348608\tValidation Loss: 0.6914447437899359\n",
            "Epoch 19816  \tTraining Loss: 0.6914524562917977\tValidation Loss: 0.6914446802352394\n",
            "Epoch 19817  \tTraining Loss: 0.6914523932478793\tValidation Loss: 0.691444616679688\n",
            "Epoch 19818  \tTraining Loss: 0.6914523302031054\tValidation Loss: 0.6914445531232815\n",
            "Epoch 19819  \tTraining Loss: 0.6914522671574759\tValidation Loss: 0.69144448956602\n",
            "Epoch 19820  \tTraining Loss: 0.6914522041109911\tValidation Loss: 0.6914444260079032\n",
            "Epoch 19821  \tTraining Loss: 0.6914521410636506\tValidation Loss: 0.6914443624489313\n",
            "Epoch 19822  \tTraining Loss: 0.6914520780154545\tValidation Loss: 0.6914442988891042\n",
            "Epoch 19823  \tTraining Loss: 0.6914520149664029\tValidation Loss: 0.6914442353284219\n",
            "Epoch 19824  \tTraining Loss: 0.6914519519164957\tValidation Loss: 0.6914441717668844\n",
            "Epoch 19825  \tTraining Loss: 0.6914518888657326\tValidation Loss: 0.6914441082044915\n",
            "Epoch 19826  \tTraining Loss: 0.6914518258141141\tValidation Loss: 0.6914440446412434\n",
            "Epoch 19827  \tTraining Loss: 0.6914517627616397\tValidation Loss: 0.6914439810771399\n",
            "Epoch 19828  \tTraining Loss: 0.6914516997083096\tValidation Loss: 0.691443917512181\n",
            "Epoch 19829  \tTraining Loss: 0.6914516366541236\tValidation Loss: 0.6914438539463666\n",
            "Epoch 19830  \tTraining Loss: 0.6914515735990817\tValidation Loss: 0.6914437903796968\n",
            "Epoch 19831  \tTraining Loss: 0.6914515105431839\tValidation Loss: 0.6914437268121717\n",
            "Epoch 19832  \tTraining Loss: 0.6914514474864302\tValidation Loss: 0.6914436632437909\n",
            "Epoch 19833  \tTraining Loss: 0.6914513844288207\tValidation Loss: 0.6914435996745545\n",
            "Epoch 19834  \tTraining Loss: 0.691451321370355\tValidation Loss: 0.6914435361044625\n",
            "Epoch 19835  \tTraining Loss: 0.6914512583110335\tValidation Loss: 0.6914434725335149\n",
            "Epoch 19836  \tTraining Loss: 0.6914511952508557\tValidation Loss: 0.6914434089617116\n",
            "Epoch 19837  \tTraining Loss: 0.691451132189822\tValidation Loss: 0.6914433453890527\n",
            "Epoch 19838  \tTraining Loss: 0.6914510691279321\tValidation Loss: 0.6914432818155379\n",
            "Epoch 19839  \tTraining Loss: 0.6914510060651861\tValidation Loss: 0.6914432182411674\n",
            "Epoch 19840  \tTraining Loss: 0.6914509430015838\tValidation Loss: 0.6914431546659412\n",
            "Epoch 19841  \tTraining Loss: 0.6914508799371253\tValidation Loss: 0.691443091089859\n",
            "Epoch 19842  \tTraining Loss: 0.6914508168718106\tValidation Loss: 0.691443027512921\n",
            "Epoch 19843  \tTraining Loss: 0.6914507538056395\tValidation Loss: 0.691442963935127\n",
            "Epoch 19844  \tTraining Loss: 0.6914506907386121\tValidation Loss: 0.6914429003564772\n",
            "Epoch 19845  \tTraining Loss: 0.6914506276707284\tValidation Loss: 0.6914428367769714\n",
            "Epoch 19846  \tTraining Loss: 0.6914505646019883\tValidation Loss: 0.6914427731966094\n",
            "Epoch 19847  \tTraining Loss: 0.6914505015323916\tValidation Loss: 0.6914427096153917\n",
            "Epoch 19848  \tTraining Loss: 0.6914504384619385\tValidation Loss: 0.6914426460333176\n",
            "Epoch 19849  \tTraining Loss: 0.6914503753906289\tValidation Loss: 0.6914425824503874\n",
            "Epoch 19850  \tTraining Loss: 0.6914503123184628\tValidation Loss: 0.6914425188666012\n",
            "Epoch 19851  \tTraining Loss: 0.6914502492454401\tValidation Loss: 0.6914424552819587\n",
            "Epoch 19852  \tTraining Loss: 0.6914501861715608\tValidation Loss: 0.69144239169646\n",
            "Epoch 19853  \tTraining Loss: 0.6914501230968249\tValidation Loss: 0.6914423281101052\n",
            "Epoch 19854  \tTraining Loss: 0.6914500600212322\tValidation Loss: 0.6914422645228938\n",
            "Epoch 19855  \tTraining Loss: 0.6914499969447828\tValidation Loss: 0.6914422009348264\n",
            "Epoch 19856  \tTraining Loss: 0.6914499338674767\tValidation Loss: 0.6914421373459024\n",
            "Epoch 19857  \tTraining Loss: 0.6914498707893137\tValidation Loss: 0.6914420737561221\n",
            "Epoch 19858  \tTraining Loss: 0.6914498077102941\tValidation Loss: 0.6914420101654855\n",
            "Epoch 19859  \tTraining Loss: 0.6914497446304175\tValidation Loss: 0.6914419465739922\n",
            "Epoch 19860  \tTraining Loss: 0.691449681549684\tValidation Loss: 0.6914418829816426\n",
            "Epoch 19861  \tTraining Loss: 0.6914496184680937\tValidation Loss: 0.6914418193884362\n",
            "Epoch 19862  \tTraining Loss: 0.6914495553856463\tValidation Loss: 0.6914417557943735\n",
            "Epoch 19863  \tTraining Loss: 0.6914494923023419\tValidation Loss: 0.6914416921994541\n",
            "Epoch 19864  \tTraining Loss: 0.6914494292181805\tValidation Loss: 0.6914416286036781\n",
            "Epoch 19865  \tTraining Loss: 0.6914493661331621\tValidation Loss: 0.6914415650070453\n",
            "Epoch 19866  \tTraining Loss: 0.6914493030472866\tValidation Loss: 0.6914415014095558\n",
            "Epoch 19867  \tTraining Loss: 0.691449239960554\tValidation Loss: 0.6914414378112098\n",
            "Epoch 19868  \tTraining Loss: 0.691449176872964\tValidation Loss: 0.6914413742120069\n",
            "Epoch 19869  \tTraining Loss: 0.691449113784517\tValidation Loss: 0.6914413106119471\n",
            "Epoch 19870  \tTraining Loss: 0.6914490506952128\tValidation Loss: 0.6914412470110305\n",
            "Epoch 19871  \tTraining Loss: 0.6914489876050511\tValidation Loss: 0.6914411834092572\n",
            "Epoch 19872  \tTraining Loss: 0.6914489245140324\tValidation Loss: 0.6914411198066267\n",
            "Epoch 19873  \tTraining Loss: 0.6914488614221562\tValidation Loss: 0.6914410562031393\n",
            "Epoch 19874  \tTraining Loss: 0.6914487983294226\tValidation Loss: 0.6914409925987952\n",
            "Epoch 19875  \tTraining Loss: 0.6914487352358316\tValidation Loss: 0.6914409289935938\n",
            "Epoch 19876  \tTraining Loss: 0.6914486721413833\tValidation Loss: 0.6914408653875355\n",
            "Epoch 19877  \tTraining Loss: 0.6914486090460773\tValidation Loss: 0.6914408017806201\n",
            "Epoch 19878  \tTraining Loss: 0.691448545949914\tValidation Loss: 0.6914407381728475\n",
            "Epoch 19879  \tTraining Loss: 0.6914484828528931\tValidation Loss: 0.6914406745642178\n",
            "Epoch 19880  \tTraining Loss: 0.6914484197550145\tValidation Loss: 0.6914406109547307\n",
            "Epoch 19881  \tTraining Loss: 0.6914483566562784\tValidation Loss: 0.6914405473443866\n",
            "Epoch 19882  \tTraining Loss: 0.6914482935566845\tValidation Loss: 0.6914404837331853\n",
            "Epoch 19883  \tTraining Loss: 0.6914482304562329\tValidation Loss: 0.6914404201211266\n",
            "Epoch 19884  \tTraining Loss: 0.6914481673549238\tValidation Loss: 0.6914403565082106\n",
            "Epoch 19885  \tTraining Loss: 0.6914481042527567\tValidation Loss: 0.6914402928944372\n",
            "Epoch 19886  \tTraining Loss: 0.6914480411497319\tValidation Loss: 0.6914402292798064\n",
            "Epoch 19887  \tTraining Loss: 0.6914479780458495\tValidation Loss: 0.6914401656643182\n",
            "Epoch 19888  \tTraining Loss: 0.6914479149411089\tValidation Loss: 0.6914401020479726\n",
            "Epoch 19889  \tTraining Loss: 0.6914478518355106\tValidation Loss: 0.6914400384307693\n",
            "Epoch 19890  \tTraining Loss: 0.6914477887290542\tValidation Loss: 0.6914399748127087\n",
            "Epoch 19891  \tTraining Loss: 0.69144772562174\tValidation Loss: 0.6914399111937903\n",
            "Epoch 19892  \tTraining Loss: 0.6914476625135678\tValidation Loss: 0.6914398475740146\n",
            "Epoch 19893  \tTraining Loss: 0.6914475994045374\tValidation Loss: 0.6914397839533809\n",
            "Epoch 19894  \tTraining Loss: 0.6914475362946492\tValidation Loss: 0.6914397203318898\n",
            "Epoch 19895  \tTraining Loss: 0.6914474731839027\tValidation Loss: 0.6914396567095409\n",
            "Epoch 19896  \tTraining Loss: 0.6914474100722982\tValidation Loss: 0.6914395930863343\n",
            "Epoch 19897  \tTraining Loss: 0.6914473469598352\tValidation Loss: 0.6914395294622698\n",
            "Epoch 19898  \tTraining Loss: 0.6914472838465143\tValidation Loss: 0.6914394658373476\n",
            "Epoch 19899  \tTraining Loss: 0.6914472207323351\tValidation Loss: 0.6914394022115676\n",
            "Epoch 19900  \tTraining Loss: 0.6914471576172976\tValidation Loss: 0.6914393385849296\n",
            "Epoch 19901  \tTraining Loss: 0.6914470945014017\tValidation Loss: 0.6914392749574337\n",
            "Epoch 19902  \tTraining Loss: 0.6914470313846476\tValidation Loss: 0.6914392113290798\n",
            "Epoch 19903  \tTraining Loss: 0.6914469682670351\tValidation Loss: 0.691439147699868\n",
            "Epoch 19904  \tTraining Loss: 0.6914469051485642\tValidation Loss: 0.6914390840697981\n",
            "Epoch 19905  \tTraining Loss: 0.6914468420292348\tValidation Loss: 0.6914390204388703\n",
            "Epoch 19906  \tTraining Loss: 0.6914467789090468\tValidation Loss: 0.6914389568070843\n",
            "Epoch 19907  \tTraining Loss: 0.6914467157880004\tValidation Loss: 0.6914388931744402\n",
            "Epoch 19908  \tTraining Loss: 0.6914466526660955\tValidation Loss: 0.6914388295409379\n",
            "Epoch 19909  \tTraining Loss: 0.6914465895433318\tValidation Loss: 0.6914387659065775\n",
            "Epoch 19910  \tTraining Loss: 0.6914465264197097\tValidation Loss: 0.6914387022713588\n",
            "Epoch 19911  \tTraining Loss: 0.691446463295229\tValidation Loss: 0.691438638635282\n",
            "Epoch 19912  \tTraining Loss: 0.6914464001698895\tValidation Loss: 0.6914385749983466\n",
            "Epoch 19913  \tTraining Loss: 0.6914463370436912\tValidation Loss: 0.6914385113605531\n",
            "Epoch 19914  \tTraining Loss: 0.6914462739166342\tValidation Loss: 0.6914384477219012\n",
            "Epoch 19915  \tTraining Loss: 0.6914462107887184\tValidation Loss: 0.6914383840823908\n",
            "Epoch 19916  \tTraining Loss: 0.6914461476599437\tValidation Loss: 0.6914383204420221\n",
            "Epoch 19917  \tTraining Loss: 0.6914460845303102\tValidation Loss: 0.6914382568007947\n",
            "Epoch 19918  \tTraining Loss: 0.6914460213998178\tValidation Loss: 0.691438193158709\n",
            "Epoch 19919  \tTraining Loss: 0.6914459582684663\tValidation Loss: 0.6914381295157649\n",
            "Epoch 19920  \tTraining Loss: 0.6914458951362561\tValidation Loss: 0.6914380658719621\n",
            "Epoch 19921  \tTraining Loss: 0.6914458320031867\tValidation Loss: 0.6914380022273006\n",
            "Epoch 19922  \tTraining Loss: 0.6914457688692582\tValidation Loss: 0.6914379385817806\n",
            "Epoch 19923  \tTraining Loss: 0.691445705734471\tValidation Loss: 0.6914378749354017\n",
            "Epoch 19924  \tTraining Loss: 0.6914456425988244\tValidation Loss: 0.6914378112881642\n",
            "Epoch 19925  \tTraining Loss: 0.6914455794623187\tValidation Loss: 0.691437747640068\n",
            "Epoch 19926  \tTraining Loss: 0.6914455163249538\tValidation Loss: 0.6914376839911129\n",
            "Epoch 19927  \tTraining Loss: 0.6914454531867298\tValidation Loss: 0.6914376203412992\n",
            "Epoch 19928  \tTraining Loss: 0.6914453900476463\tValidation Loss: 0.6914375566906265\n",
            "Epoch 19929  \tTraining Loss: 0.6914453269077038\tValidation Loss: 0.6914374930390949\n",
            "Epoch 19930  \tTraining Loss: 0.6914452637669017\tValidation Loss: 0.6914374293867045\n",
            "Epoch 19931  \tTraining Loss: 0.6914452006252405\tValidation Loss: 0.6914373657334549\n",
            "Epoch 19932  \tTraining Loss: 0.6914451374827197\tValidation Loss: 0.6914373020793466\n",
            "Epoch 19933  \tTraining Loss: 0.6914450743393397\tValidation Loss: 0.6914372384243791\n",
            "Epoch 19934  \tTraining Loss: 0.6914450111951002\tValidation Loss: 0.6914371747685527\n",
            "Epoch 19935  \tTraining Loss: 0.691444948050001\tValidation Loss: 0.691437111111867\n",
            "Epoch 19936  \tTraining Loss: 0.6914448849040424\tValidation Loss: 0.6914370474543223\n",
            "Epoch 19937  \tTraining Loss: 0.6914448217572243\tValidation Loss: 0.6914369837959184\n",
            "Epoch 19938  \tTraining Loss: 0.6914447586095466\tValidation Loss: 0.6914369201366554\n",
            "Epoch 19939  \tTraining Loss: 0.6914446954610092\tValidation Loss: 0.6914368564765331\n",
            "Epoch 19940  \tTraining Loss: 0.6914446323116121\tValidation Loss: 0.6914367928155514\n",
            "Epoch 19941  \tTraining Loss: 0.6914445691613554\tValidation Loss: 0.6914367291537107\n",
            "Epoch 19942  \tTraining Loss: 0.691444506010239\tValidation Loss: 0.6914366654910105\n",
            "Epoch 19943  \tTraining Loss: 0.6914444428582628\tValidation Loss: 0.6914366018274509\n",
            "Epoch 19944  \tTraining Loss: 0.6914443797054267\tValidation Loss: 0.6914365381630319\n",
            "Epoch 19945  \tTraining Loss: 0.6914443165517309\tValidation Loss: 0.6914364744977535\n",
            "Epoch 19946  \tTraining Loss: 0.6914442533971751\tValidation Loss: 0.6914364108316157\n",
            "Epoch 19947  \tTraining Loss: 0.6914441902417594\tValidation Loss: 0.6914363471646181\n",
            "Epoch 19948  \tTraining Loss: 0.6914441270854838\tValidation Loss: 0.6914362834967612\n",
            "Epoch 19949  \tTraining Loss: 0.6914440639283481\tValidation Loss: 0.6914362198280447\n",
            "Epoch 19950  \tTraining Loss: 0.6914440007703527\tValidation Loss: 0.6914361561584687\n",
            "Epoch 19951  \tTraining Loss: 0.6914439376114969\tValidation Loss: 0.6914360924880327\n",
            "Epoch 19952  \tTraining Loss: 0.6914438744517812\tValidation Loss: 0.6914360288167374\n",
            "Epoch 19953  \tTraining Loss: 0.6914438112912054\tValidation Loss: 0.6914359651445823\n",
            "Epoch 19954  \tTraining Loss: 0.6914437481297695\tValidation Loss: 0.6914359014715673\n",
            "Epoch 19955  \tTraining Loss: 0.6914436849674732\tValidation Loss: 0.6914358377976926\n",
            "Epoch 19956  \tTraining Loss: 0.6914436218043168\tValidation Loss: 0.6914357741229581\n",
            "Epoch 19957  \tTraining Loss: 0.6914435586403002\tValidation Loss: 0.6914357104473637\n",
            "Epoch 19958  \tTraining Loss: 0.6914434954754232\tValidation Loss: 0.6914356467709095\n",
            "Epoch 19959  \tTraining Loss: 0.691443432309686\tValidation Loss: 0.6914355830935953\n",
            "Epoch 19960  \tTraining Loss: 0.6914433691430882\tValidation Loss: 0.6914355194154211\n",
            "Epoch 19961  \tTraining Loss: 0.6914433059756303\tValidation Loss: 0.691435455736387\n",
            "Epoch 19962  \tTraining Loss: 0.6914432428073118\tValidation Loss: 0.6914353920564928\n",
            "Epoch 19963  \tTraining Loss: 0.691443179638133\tValidation Loss: 0.6914353283757386\n",
            "Epoch 19964  \tTraining Loss: 0.6914431164680935\tValidation Loss: 0.6914352646941244\n",
            "Epoch 19965  \tTraining Loss: 0.6914430532971936\tValidation Loss: 0.6914352010116499\n",
            "Epoch 19966  \tTraining Loss: 0.691442990125433\tValidation Loss: 0.6914351373283152\n",
            "Epoch 19967  \tTraining Loss: 0.691442926952812\tValidation Loss: 0.6914350736441205\n",
            "Epoch 19968  \tTraining Loss: 0.6914428637793301\tValidation Loss: 0.6914350099590654\n",
            "Epoch 19969  \tTraining Loss: 0.6914428006049879\tValidation Loss: 0.69143494627315\n",
            "Epoch 19970  \tTraining Loss: 0.6914427374297847\tValidation Loss: 0.6914348825863745\n",
            "Epoch 19971  \tTraining Loss: 0.6914426742537207\tValidation Loss: 0.6914348188987386\n",
            "Epoch 19972  \tTraining Loss: 0.691442611076796\tValidation Loss: 0.6914347552102421\n",
            "Epoch 19973  \tTraining Loss: 0.6914425478990106\tValidation Loss: 0.6914346915208854\n",
            "Epoch 19974  \tTraining Loss: 0.6914424847203643\tValidation Loss: 0.6914346278306682\n",
            "Epoch 19975  \tTraining Loss: 0.691442421540857\tValidation Loss: 0.6914345641395906\n",
            "Epoch 19976  \tTraining Loss: 0.691442358360489\tValidation Loss: 0.6914345004476523\n",
            "Epoch 19977  \tTraining Loss: 0.6914422951792599\tValidation Loss: 0.6914344367548537\n",
            "Epoch 19978  \tTraining Loss: 0.6914422319971698\tValidation Loss: 0.6914343730611944\n",
            "Epoch 19979  \tTraining Loss: 0.6914421688142188\tValidation Loss: 0.6914343093666746\n",
            "Epoch 19980  \tTraining Loss: 0.6914421056304066\tValidation Loss: 0.6914342456712939\n",
            "Epoch 19981  \tTraining Loss: 0.6914420424457335\tValidation Loss: 0.6914341819750527\n",
            "Epoch 19982  \tTraining Loss: 0.6914419792601991\tValidation Loss: 0.6914341182779508\n",
            "Epoch 19983  \tTraining Loss: 0.6914419160738037\tValidation Loss: 0.691434054579988\n",
            "Epoch 19984  \tTraining Loss: 0.691441852886547\tValidation Loss: 0.6914339908811644\n",
            "Epoch 19985  \tTraining Loss: 0.691441789698429\tValidation Loss: 0.6914339271814802\n",
            "Epoch 19986  \tTraining Loss: 0.6914417265094498\tValidation Loss: 0.691433863480935\n",
            "Epoch 19987  \tTraining Loss: 0.6914416633196093\tValidation Loss: 0.6914337997795288\n",
            "Epoch 19988  \tTraining Loss: 0.6914416001289075\tValidation Loss: 0.6914337360772619\n",
            "Epoch 19989  \tTraining Loss: 0.6914415369373444\tValidation Loss: 0.6914336723741339\n",
            "Epoch 19990  \tTraining Loss: 0.6914414737449199\tValidation Loss: 0.6914336086701449\n",
            "Epoch 19991  \tTraining Loss: 0.6914414105516339\tValidation Loss: 0.691433544965295\n",
            "Epoch 19992  \tTraining Loss: 0.6914413473574864\tValidation Loss: 0.6914334812595838\n",
            "Epoch 19993  \tTraining Loss: 0.6914412841624773\tValidation Loss: 0.6914334175530117\n",
            "Epoch 19994  \tTraining Loss: 0.6914412209666069\tValidation Loss: 0.6914333538455784\n",
            "Epoch 19995  \tTraining Loss: 0.6914411577698746\tValidation Loss: 0.6914332901372839\n",
            "Epoch 19996  \tTraining Loss: 0.6914410945722809\tValidation Loss: 0.6914332264281282\n",
            "Epoch 19997  \tTraining Loss: 0.6914410313738257\tValidation Loss: 0.6914331627181113\n",
            "Epoch 19998  \tTraining Loss: 0.6914409681745086\tValidation Loss: 0.6914330990072332\n",
            "Epoch 19999  \tTraining Loss: 0.6914409049743298\tValidation Loss: 0.6914330352954937\n",
            "Epoch 20000  \tTraining Loss: 0.6914408417732893\tValidation Loss: 0.6914329715828929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your training and validation loss by epoch"
      ],
      "metadata": {
        "id": "7Vyhwa8k71v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.plot(per_epoch_loss_train, label = 'Training')\n",
        "plt.plot(per_epoch_loss_val, label = 'Validation')\n",
        "\n",
        "\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "aJ7QQ_Vf8GPq",
        "outputId": "7e8e951b-17b5-47f9-845b-1b414e636e3b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report the accuracy of your classifier on your validation dataset"
      ],
      "metadata": {
        "id": "C_I22ybD73EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set using the trained model\n",
        "predictions = final_classifier.predict(X_val.T)\n",
        "\n",
        "# Convert predictions to binary labels (assuming a sigmoid activation function for binary classification)\n",
        "prediction_label = (predictions > 0.5).astype(int).T\n",
        "\n",
        "# Calculate accuracy by comparing predicted labels with ground truth labels\n",
        "accuracy = accuracy_score(y_val, prediction_label)\n",
        "\n",
        "# Print the validation accuracy as a percentage\n",
        "print(f'Validation accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BapoWoVe8HOc",
        "outputId": "3c6ae733-1bcc-41d6-d4b9-cae1b5cbbd5c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 79.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your choice of loss function and hyperparameters:\n",
        "\n",
        "Accuracy is approximately 80%.\n",
        "For the loss function, binary cross entropy was chosen as it is typically used for binary classification, similar to what we are doing here. Multiple layers were considered, as well as different activations. Parameters were chosen such to end with 1 dimension (classification) and to optimize accuracy. Moreover, several parameters were considered in combination, as seen above, before ultimately selecting the final parameters.  "
      ],
      "metadata": {
        "id": "KUTkZ_mQ74mt"
      }
    }
  ]
}